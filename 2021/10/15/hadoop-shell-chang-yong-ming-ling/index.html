<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script><link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet"><script></script><meta name="theme-color" content="#222"><style>.pace .pace-progress{background:#1e92fb;height:3px}.pace .pace-progress-inner{box-shadow:0 0 10px #1e92fb,0 0 5px #1e92fb}.pace .pace-activity{border-top-color:#1e92fb;border-left-color:#1e92fb}</style><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="HGM141IpbHrSmnAmR6W_zE4bo9Z3f-yXLeHYT3bg1fk"><meta name="baidu-site-verification" content="code-5Ai1DA8e6T"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="Hadoop,Hadoop Shell,"><link rel="alternate" href="/atom.xml" title="凡间的精灵" type="application/atom+xml"><meta name="description" content="一、HDFS Shell大多数HDFS Shell命令的行为和对应的Unix Shell命令类似，主要不同之处是HDFS Shell命令操作的是远程Hadoop服务器的文件，而Unix Shell命令操作的是本地文件，其他不同之处将在介绍各命令使用详情时指出。常用的HDFS Shell命令如下表所示1.1 cat说明：将路径指定文件的内容输出到stdout。用法：hadoop fs -cat UR"><meta name="keywords" content="Hadoop,Hadoop Shell"><meta property="og:type" content="article"><meta property="og:title" content="Hadoop Shell常用命令"><meta property="og:url" content="http:&#x2F;&#x2F;chenzhonzhou.github.io&#x2F;2021&#x2F;10&#x2F;15&#x2F;hadoop-shell-chang-yong-ming-ling&#x2F;index.html"><meta property="og:site_name" content="凡间的精灵"><meta property="og:description" content="一、HDFS Shell大多数HDFS Shell命令的行为和对应的Unix Shell命令类似，主要不同之处是HDFS Shell命令操作的是远程Hadoop服务器的文件，而Unix Shell命令操作的是本地文件，其他不同之处将在介绍各命令使用详情时指出。常用的HDFS Shell命令如下表所示1.1 cat说明：将路径指定文件的内容输出到stdout。用法：hadoop fs -cat UR"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2021-10-15T11:00:13.119Z"><meta name="twitter:card" content="summary"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"always",offset:12,b2t:!0,scrollpercent:!0,onmobile:!0},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://chenzhonzhou.github.io/2021/10/15/hadoop-shell-chang-yong-ming-ling/"><title>Hadoop Shell常用命令 | 凡间的精灵</title><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">凡间的精灵</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">凡尘落素一精灵</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>站点地图</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://chenzhonzhou.github.io/2021/10/15/hadoop-shell-chang-yong-ming-ling/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Zhongzhou Chen"><meta itemprop="description" content=""><meta itemprop="image" content="/images/chen.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="凡间的精灵"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Hadoop Shell常用命令</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-10-15T17:04:35+08:00">2021-10-15</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">2.3k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">9</span></div></div></header><div class="post-body" itemprop="articleBody"><h3 id="一、HDFS-Shell"><a href="#一、HDFS-Shell" class="headerlink" title="一、HDFS Shell"></a>一、HDFS Shell</h3><p>大多数HDFS Shell命令的行为和对应的Unix Shell命令类似，主要不同之处是HDFS Shell命令操作的是远程Hadoop服务器的文件，而Unix Shell命令操作的是本地文件，其他不同之处将在介绍各命令使用详情时指出。</p><p>常用的HDFS Shell命令如下表所示</p><h4 id="1-1-cat"><a href="#1-1-cat" class="headerlink" title="1.1 cat"></a>1.1 cat</h4><p>说明：将路径指定文件的内容输出到stdout。<br>用法：hadoop fs -cat URI [URI …]<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2  </span><br><span class="line">$ hadoop fs -cat file:///file3/user/hadoop/file4</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h4 id="1-2-chgrp"><a href="#1-2-chgrp" class="headerlink" title="1.2 chgrp"></a>1.2 chgrp</h4><p>说明：改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。<br>用法：hadoop fs -chgrp [-R] GROUP URI [URI …]<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop dfs -chgrp -R hadoop /user/hadoop/</span><br></pre></td></tr></table></figure><h4 id="1-3-chmod"><a href="#1-3-chmod" class="headerlink" title="1.3 chmod"></a>1.3 chmod</h4><p>说明：改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。<br>用法：hadoop fs -chmod [-R] URI [URI …]<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -chmod -R 744 /user/hadoop/</span><br></pre></td></tr></table></figure><h4 id="1-4-chown"><a href="#1-4-chown" class="headerlink" title="1.4 chown"></a>1.4 chown</h4><p>说明：改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。<br>用法：hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -chmod -R hadoop /user/hadoop/</span><br></pre></td></tr></table></figure><h4 id="1-5-copyFromLocal-上传文件"><a href="#1-5-copyFromLocal-上传文件" class="headerlink" title="1.5 copyFromLocal 上传文件"></a>1.5 copyFromLocal 上传文件</h4><p>说明：除了限定源路径是一个本地文件外，和put命令相似。<br>用法：hadoop fs -copyFromLocal &lt;localsrc&gt; URI</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -put words.txt /aa</span><br><span class="line">$ hadoop fs -copyFromLocal words.txt /aa/bb</span><br></pre></td></tr></table></figure><h4 id="1-6-copyToLocal-下载文件"><a href="#1-6-copyToLocal-下载文件" class="headerlink" title="1.6 copyToLocal 下载文件"></a>1.6 copyToLocal 下载文件</h4><p>说明：除了限定目标路径是一个本地文件外，和get命令类似。<br>用法：hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -get /aa/words.txt ~/newwords.txt</span><br><span class="line">$ hadoop fs -copyToLocal /aa/words.txt ~/newwords1.txt</span><br></pre></td></tr></table></figure><h4 id="1-7-appendToFile-追加内容"><a href="#1-7-appendToFile-追加内容" class="headerlink" title="1.7 appendToFile 追加内容"></a>1.7 appendToFile 追加内容</h4><p>说明：将单个 src 或多个 src 从本地文件系统附加到目标文件系统。还从 stdin 读取输入并附加到目<br>标文件系统。<br>用法：hadoop fs -appendToFile &lt;localsrc&gt; … &lt;dst&gt;</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -appendToFile ~/hello.txt /aa/words.txt</span><br></pre></td></tr></table></figure><p>退出代码：成功返回 0，错误返回 1</p><h4 id="1-8-cp"><a href="#1-8-cp" class="headerlink" title="1.8 cp"></a>1.8 cp</h4><p>说明：将文件将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。<br>用法：hadoop fs -cp URI [URI …] &lt;dest&gt;<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2</span><br><span class="line">$ hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h4 id="1-9-du"><a href="#1-9-du" class="headerlink" title="1.9 du"></a>1.9 du</h4><p>说明：显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。<br>用法：hadoop fs -du URI [URI …]<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1</span><br></pre></td></tr></table></figure><p><strong>查看hbase所有文件的大小</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -du hdfs://master:54310/hbase</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h4 id="1-10-dus"><a href="#1-10-dus" class="headerlink" title="1.10 dus"></a>1.10 dus</h4><p>说明：显示文件的大小。<br>用法：hadoop fs -dus &lt;args&gt;</p><h4 id="1-11-expunge"><a href="#1-11-expunge" class="headerlink" title="1.11 expunge"></a>1.11 expunge</h4><p>说明：清空回收站。<br>用法：hadoop fs -expunge</p><h4 id="1-12-get-下载文件"><a href="#1-12-get-下载文件" class="headerlink" title="1.12 get 下载文件"></a>1.12 get 下载文件</h4><p>说明：复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。<br>用法：hadoop fs -get [-ignorecrc] [-crc]<src><localdst><br>示例：</localdst></src></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -get /user/hadoop/file localfile</span><br><span class="line">$ hadoop fs -get hdfs://host:port/user/hadoop/file localfile</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h4 id="1-13-getmerge-合并下载"><a href="#1-13-getmerge-合并下载" class="headerlink" title="1.13 getmerge 合并下载"></a>1.13 getmerge 合并下载</h4><p>说明：接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。<br>用法：hadoop fs -getmerge &lt;src&gt; &lt;localdst&gt; [addnl]</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -getmerge /aa/words.txt /aa/bb/words.txt ~/2words.txt</span><br></pre></td></tr></table></figure><h4 id="1-14-ls"><a href="#1-14-ls" class="headerlink" title="1.14 ls"></a>1.14 ls</h4><p>用法：hadoop fs -ls &lt;args&gt;<br>说明：<br>(1).如果是文件，则按照如下格式返回文件信息：<br>文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID<br>(2).如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：<br>目录名 &lt;dir&gt; 修改日期 修改时间 权限 用户ID 组ID<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h4 id="1-15-lsr"><a href="#1-15-lsr" class="headerlink" title="1.15 lsr"></a>1.15 lsr</h4><p>用法：hadoop fs -lsr &lt;args&gt;<br>说明：ls命令的递归版本。类似于Unix中的ls -R。</p><h4 id="1-16-mkdir"><a href="#1-16-mkdir" class="headerlink" title="1.16 mkdir"></a>1.16 mkdir</h4><p>说明：接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。<br>用法：hadoop fs -mkdir &lt;paths&gt;<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2</span><br><span class="line">$ hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h4 id="1-17-movefromLocal"><a href="#1-17-movefromLocal" class="headerlink" title="1.17 movefromLocal"></a>1.17 movefromLocal</h4><p>说明：从本地剪切文件到HDFS上，输出一个not implemented信息。<br>用法：dfs -moveFromLocal &lt;src&gt; &lt;dst&gt;<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ls</span><br><span class="line">hello.txt</span><br><span class="line">$ hadoop fs -moveFromLocal hello.txt /<span class="built_in">test</span></span><br><span class="line">$ ls</span><br></pre></td></tr></table></figure><h4 id="1-18-mv"><a href="#1-18-mv" class="headerlink" title="1.18 mv"></a>1.18 mv</h4><p>说明：在HDFS目录中移动文件。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。<br>用法：hadoop fs -mv URI [URI …] &lt;dest&gt;<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2</span><br><span class="line">$ hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h4 id="1-19-put"><a href="#1-19-put" class="headerlink" title="1.19 put"></a>1.19 put</h4><p>说明：从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。<br>用法：hadoop fs -put &lt;localsrc&gt; … &lt;dst&gt;<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -put localfile /user/hadoop/hadoopfile  </span><br><span class="line">$ hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir  </span><br><span class="line">$ hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile  </span><br><span class="line">$ hadoop fs -put - hdfs://host:port/hadoop/hadoopfile</span><br></pre></td></tr></table></figure><p>从标准输入中读取输入。<br>返回值：成功返回0，失败返回-1</p><h4 id="1-20-rm"><a href="#1-20-rm" class="headerlink" title="1.20 rm"></a>1.20 rm</h4><p>说明：删除指定的文件。只删除非空目录和文件。请参考<strong><code>rmr</code></strong>命令了解递归删除。<br>使用方法：hadoop fs -rm URI [URI …]<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h4 id="1-21-rmr"><a href="#1-21-rmr" class="headerlink" title="1.21 rmr"></a>1.21 rmr</h4><p>说明：delete的递归版本。<br>使用方法：hadoop fs -rmr URI [URI …]<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -rmr /user/hadoop/dir</span><br><span class="line">$ hadoop fs -rmr hdfs://host:port/user/hadoop/dir</span><br></pre></td></tr></table></figure><p>返回值<br>成功返回0，失败返回-1</p><h4 id="1-22-setrep"><a href="#1-22-setrep" class="headerlink" title="1.22 setrep"></a>1.22 setrep</h4><p>说明：改变一个文件的副本系数。<strong>-R</strong>选项用于递归改变目录下所有文件的副本系数。<br>使用方法：hadoop fs -setrep [-R] &lt;path&gt;<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -setrep -w 3 -R /user/hadoop/dir1</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h4 id="1-23-stat"><a href="#1-23-stat" class="headerlink" title="1.23 stat"></a>1.23 stat</h4><p>说明：返回指定路径的统计信息。<br>用法：hadoop fs -stat URI [URI …]<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -<span class="built_in">stat</span> path</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h4 id="1-24-tail"><a href="#1-24-tail" class="headerlink" title="1.24 tail"></a>1.24 tail</h4><p>用法：将文件尾部1K字节的内容输出到stdout。支持 <strong>-f</strong> 选项，行为和Unix中一致。<br>用法：hadoop fs -tail [-f] URI<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -tail pathname</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h4 id="1-25-test"><a href="#1-25-test" class="headerlink" title="1.25 test"></a>1.25 test</h4><p>用法：hadoop fs -test -[ezd] URI<br>选项：<br>-e 检查文件是否存在。如果存在则返回0。<br>-z 检查文件是否是0字节。如果是则返回0。<br>-d 如果路径是个目录，则返回1，否则返回0。<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -<span class="built_in">test</span> -e filename</span><br></pre></td></tr></table></figure><h4 id="1-26-text"><a href="#1-26-text" class="headerlink" title="1.26 text"></a>1.26 text</h4><p>说明：将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。<br>用法：hadoop fs -text &lt;src&gt;</p><h4 id="1-27-touchz"><a href="#1-27-touchz" class="headerlink" title="1.27 touchz"></a>1.27 touchz</h4><p>说明：创建一个0字节的空文件。<br>用法：hadoop fs -touchz URI [URI …]<br>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop -touchz pathname</span><br></pre></td></tr></table></figure><p>返回值：成功返回0，失败返回-1</p><h3 id="二、Hadoop系统管理命令"><a href="#二、Hadoop系统管理命令" class="headerlink" title="二、Hadoop系统管理命令"></a>二、Hadoop系统管理命令</h3><p>除了操作文件系统外，Hadoop还提供一些系统管理命令，包括开启服务、关闭服务、格式化、安全模式设置等功能，常用的系统管理命令如下。</p><h4 id="2-1-查看-Hadoop-版本"><a href="#2-1-查看-Hadoop-版本" class="headerlink" title="2.1 查看 Hadoop 版本"></a>2.1 查看 Hadoop 版本</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop version</span><br></pre></td></tr></table></figure><h4 id="2-2-启动-Hadoop-所有进程"><a href="#2-2-启动-Hadoop-所有进程" class="headerlink" title="2.2 启动 Hadoop 所有进程"></a>2.2 启动 Hadoop 所有进程</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/start-all.sh</span><br></pre></td></tr></table></figure><h4 id="2-3-停止-Hadoop-所有进程"><a href="#2-3-停止-Hadoop-所有进程" class="headerlink" title="2.3 停止 Hadoop 所有进程"></a>2.3 停止 Hadoop 所有进程</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/stop-all.sh</span><br></pre></td></tr></table></figure><h4 id="2-4-格式化-HDFS"><a href="#2-4-格式化-HDFS" class="headerlink" title="2.4 格式化 HDFS"></a>2.4 格式化 HDFS</h4><p>格式化一个新的分布式文件系统</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop namenode -format</span><br></pre></td></tr></table></figure><h4 id="2-5-启动-HDFS"><a href="#2-5-启动-HDFS" class="headerlink" title="2.5 启动 HDFS"></a>2.5 启动 HDFS</h4><p>在分配的Namenode上，运行下面的命令启动HDFS</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-dfs.sh</span><br></pre></td></tr></table></figure><h4 id="2-6-停止-HDFS"><a href="#2-6-停止-HDFS" class="headerlink" title="2.6 停止 HDFS"></a>2.6 停止 HDFS</h4><p>在分配的Namenode上，执行下面的命令停止HDFS</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/stop-dfs.sh</span><br></pre></td></tr></table></figure><h4 id="2-7-启动-yarn"><a href="#2-7-启动-yarn" class="headerlink" title="2.7 启动 yarn"></a>2.7 启动 yarn</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>Namenode在启动时会自动进入安全模式。安全模式是Namenode的一种状态，在这个阶段，文件系统不允许有任何修改。安全模式的目的是在系统启动时检查各个Datanode上数据块的有效性，同时根据策略对数据块进行必要的复制或删除，当数据块副本数满足最小副本数条件时，会自动退出安全模式。需要注意的是，HDFS进入安全模式后会导致<strong>Hive</strong>和<strong>HBase</strong>的启动异常。</p><p>可以使用下面的命令手动进入安全模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop dfsadmin -safemode enter</span><br></pre></td></tr></table></figure><p>将集群退出安全模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop dfsadmin -safemode leave</span><br></pre></td></tr></table></figure><p>查看集群是否处于安全模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop dfsadmin -safemode get</span><br></pre></td></tr></table></figure><p>列出所有当前支持的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop dfsadmin -<span class="built_in">help</span></span><br></pre></td></tr></table></figure></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Hadoop/" rel="tag"><i class="fa fa-tag"></i> Hadoop</a><a href="/tags/Hadoop-Shell/" rel="tag"><i class="fa fa-tag"></i> Hadoop Shell</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2021/10/13/ambari-bu-shu-hadoop-da-shu-ju-ji-qun/" rel="next" title="Ambari 部署Hadoop 大数据集群"><i class="fa fa-chevron-left"></i> Ambari 部署Hadoop 大数据集群</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2021/10/18/hadoop-nosql-shu-ju-ku-hbase/" rel="prev" title="Hadoop NoSQL数据库HBase">Hadoop NoSQL数据库HBase<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"><div class="jiathis_style"><span class="jiathis_txt">分享到：</span> <a class="jiathis_button_fav">收藏夹</a> <a class="jiathis_button_copy">复制网址</a> <a class="jiathis_button_email">邮件</a> <a class="jiathis_button_weixin">微信</a> <a class="jiathis_button_qzone">QQ空间</a> <a class="jiathis_button_tqq">腾讯微博</a> <a class="jiathis_button_douban">豆瓣</a> <a class="jiathis_button_share">一键分享</a> <a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a><a class="jiathis_counter_style"></a></div><script type="text/javascript">var jiathis_config={data_track_clickback:!0,summary:"",shortUrl:!1,hideMore:!1}</script><script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=" charset="utf-8"></script></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/chen.png" alt="Zhongzhou Chen"><p class="site-author-name" itemprop="name">Zhongzhou Chen</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/%7C%7C%20archive"><span class="site-state-item-count">325</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">84</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">182</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、HDFS-Shell"><span class="nav-text">一、HDFS Shell</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-cat"><span class="nav-text">1.1 cat</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-chgrp"><span class="nav-text">1.2 chgrp</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-chmod"><span class="nav-text">1.3 chmod</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-chown"><span class="nav-text">1.4 chown</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-copyFromLocal-上传文件"><span class="nav-text">1.5 copyFromLocal 上传文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-6-copyToLocal-下载文件"><span class="nav-text">1.6 copyToLocal 下载文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-7-appendToFile-追加内容"><span class="nav-text">1.7 appendToFile 追加内容</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-8-cp"><span class="nav-text">1.8 cp</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-9-du"><span class="nav-text">1.9 du</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-10-dus"><span class="nav-text">1.10 dus</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-11-expunge"><span class="nav-text">1.11 expunge</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-12-get-下载文件"><span class="nav-text">1.12 get 下载文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-13-getmerge-合并下载"><span class="nav-text">1.13 getmerge 合并下载</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-14-ls"><span class="nav-text">1.14 ls</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-15-lsr"><span class="nav-text">1.15 lsr</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-16-mkdir"><span class="nav-text">1.16 mkdir</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-17-movefromLocal"><span class="nav-text">1.17 movefromLocal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-18-mv"><span class="nav-text">1.18 mv</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-19-put"><span class="nav-text">1.19 put</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-20-rm"><span class="nav-text">1.20 rm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-21-rmr"><span class="nav-text">1.21 rmr</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-22-setrep"><span class="nav-text">1.22 setrep</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-23-stat"><span class="nav-text">1.23 stat</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-24-tail"><span class="nav-text">1.24 tail</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-25-test"><span class="nav-text">1.25 test</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-26-text"><span class="nav-text">1.26 text</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-27-touchz"><span class="nav-text">1.27 touchz</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、Hadoop系统管理命令"><span class="nav-text">二、Hadoop系统管理命令</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-查看-Hadoop-版本"><span class="nav-text">2.1 查看 Hadoop 版本</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-启动-Hadoop-所有进程"><span class="nav-text">2.2 启动 Hadoop 所有进程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-停止-Hadoop-所有进程"><span class="nav-text">2.3 停止 Hadoop 所有进程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-格式化-HDFS"><span class="nav-text">2.4 格式化 HDFS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-启动-HDFS"><span class="nav-text">2.5 启动 HDFS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-6-停止-HDFS"><span class="nav-text">2.6 停止 HDFS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-启动-yarn"><span class="nav-text">2.7 启动 yarn</span></a></li></ol></li></ol></div></div></section><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">Zhongzhou Chen</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">765.4k</span></div><span class="post-meta-divider"></span></div></footer></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><style>.copy-btn{display:inline-block;padding:6px 12px;font-size:13px;font-weight:700;line-height:20px;color:#333;white-space:nowrap;vertical-align:middle;cursor:pointer;background-color:#eee;background-image:linear-gradient(#fcfcfc,#eee);border:1px solid #d5d5d5;border-radius:3px;user-select:none;outline:0}.highlight-wrap .copy-btn{transition:opacity .3s ease-in-out;opacity:0;padding:2px 6px;position:absolute;right:4px;top:8px}.highlight-wrap .copy-btn:focus,.highlight-wrap:hover .copy-btn{opacity:1}.highlight-wrap{position:relative}</style><script>$(".highlight").each(function(t,e){var n=$("<div>").addClass("highlight-wrap");$(e).after(n),n.append($("<button>").addClass("copy-btn").append("复制").on("click",function(t){var e=$(this).parent().find(".code").find(".line").map(function(t,e){return $(e).text()}).toArray().join("\n"),n=document.createElement("textarea");document.body.appendChild(n),n.style.position="absolute",n.style.top="0px",n.style.left="0px",n.value=e,n.select(),n.focus();var o=document.execCommand("copy");document.body.removeChild(n),o?$(this).text("复制成功"):$(this).text("复制失败"),$(this).blur()})).on("mouseleave",function(t){var e=$(this).find(".copy-btn");setTimeout(function(){e.text("复制")},300)}).append(e)})</script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,s){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var o=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,n=document.getElementById(e),r=document.getElementById(s);n.addEventListener("input",function(){var y=n.value.trim().toLowerCase(),T=y.split(/[\s\-]+/);1<T.length&&T.push(y);var b=[];if(0<y.length&&o.forEach(function(t){function e(t,e,o,n){for(var r=n[n.length-1],s=r.position,a=r.word,i=[],c=0;s+a.length<=o&&0!=n.length;){a===y&&c++,i.push({position:s,length:a.length});var l=s+a.length;for(n.pop();0!=n.length&&(s=(r=n[n.length-1]).position,a=r.word,s<l);)n.pop()}return h+=c,{hits:i,start:e,end:o,searchTextCount:c}}function o(o,t){var n="",r=t.start;return t.hits.forEach(function(t){n+=o.substring(r,t.position);var e=t.position+t.length;n+='<b class="search-keyword">'+o.substring(t.position,e)+"</b>",r=e}),n+=o.substring(r,t.end)}var n=!1,r=0,h=0,s=t.title.trim(),a=s.toLowerCase(),i=t.content.trim().replace(/<[^>]+>/g,""),c=i.toLowerCase(),l=decodeURIComponent(t.url),p=[],u=[];if(""!=s&&(T.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());-1<(s=e.indexOf(t,r));)a.push({position:s,word:t}),r=s+n;return a}p=p.concat(e(t,a,!1)),u=u.concat(e(t,c,!1))}),(0<p.length||0<u.length)&&(n=!0,r=p.length+u.length)),n){[p,u].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var f=[];0!=p.length&&f.push(e(0,0,s.length,p));for(var d=[];0!=u.length;){var g=u[u.length-1],v=g.position,$=g.word,C=v-20,m=v+80;C<0&&(C=0),m<v+$.length&&(m=v+$.length),m>i.length&&(m=i.length),d.push(e(0,C,m,u))}d.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var x=parseInt("1");0<=x&&(d=d.slice(0,x));var w="";w+=0!=f.length?"<li><a href='"+l+"' class='search-result-title'>"+o(s,f[0])+"</a>":"<li><a href='"+l+"' class='search-result-title'>"+s+"</a>",d.forEach(function(t){w+="<a href='"+l+'\'><p class="search-result">'+o(i,t)+"...</p></a>"}),w+="</li>",b.push({item:w,searchTextCount:h,hitCount:r,id:b.length})}}),1===T.length&&""===T[0])r.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===b.length)r.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{b.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var e='<ul class="search-result-list">';b.forEach(function(t){e+=t.item}),e+="</ul>",r.innerHTML=e}}),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),!1===isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){27===t.which&&$(".search-popup").is(":visible")&&onPopupClose()})</script><script type="text/javascript" src="/js/src/love.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,log:!1,model:{jsonPath:"/live2dw/assets/shizuku.model.json"},display:{position:"right"},mobile:{show:!0,scale:.2},react:{opacityDefault:.7,opacityOnHover:.2,opacity:.4}})</script></body></html>