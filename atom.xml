<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>凡间的精灵</title>
  
  <subtitle>凡尘落素一精灵</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://chenzhonzhou.github.io/"/>
  <updated>2022-08-22T02:57:51.708Z</updated>
  <id>http://chenzhonzhou.github.io/</id>
  
  <author>
    <name>Zhongzhou Chen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Grafana 使用SSL加密集成 OpenLDAP</title>
    <link href="http://chenzhonzhou.github.io/2022/08/22/grafana-shi-yong-ssl-jia-mi-ji-cheng-openldap/"/>
    <id>http://chenzhonzhou.github.io/2022/08/22/grafana-shi-yong-ssl-jia-mi-ji-cheng-openldap/</id>
    <published>2022-08-22T01:58:28.000Z</published>
    <updated>2022-08-22T02:57:51.708Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><h3 id="准备grafana相应的权限用户组"><a href="#准备grafana相应的权限用户组" class="headerlink" title="准备grafana相应的权限用户组"></a>准备grafana相应的权限用户组</h3><p><img src="/2022/08/22/grafana-shi-yong-ssl-jia-mi-ji-cheng-openldap/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><h3 id="设置ldap配置文件"><a href="#设置ldap配置文件" class="headerlink" title="设置ldap配置文件"></a>设置ldap配置文件</h3><p>ldap使用的是自签发证书</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost grafana]# vim ldap.toml</span><br><span class="line">[log]</span><br><span class="line">mode = 'console'</span><br><span class="line"><span class="meta">#</span><span class="bash">filters = <span class="string">'ldap:debug'</span></span></span><br><span class="line"></span><br><span class="line">[[servers]]</span><br><span class="line">host = "ldap.sys.com"</span><br><span class="line">port = 10636</span><br><span class="line">use_ssl = true</span><br><span class="line">start_tls = false</span><br><span class="line">ssl_skip_verify = true</span><br><span class="line"></span><br><span class="line">bind_dn = "cn=admin,dc=sys,dc=com"</span><br><span class="line">bind_password = 'abc.abc'</span><br><span class="line">timeout = 10</span><br><span class="line"></span><br><span class="line">search_filter = "(cn=%s)"</span><br><span class="line">search_base_dns = ["ou=users,dc=sys,dc=com"]</span><br><span class="line"></span><br><span class="line">group_search_filter = "(&amp;(objectClass=posixGroup)(memberUid=%s))"</span><br><span class="line">group_search_base_dns = ["ou=AWS-UAT,ou=grafana,ou=groups,dc=sys,dc=com"]</span><br><span class="line">group_search_filter_user_attribute = "uid"</span><br><span class="line"></span><br><span class="line">[servers.attributes]</span><br><span class="line">name = "givenName"</span><br><span class="line">surname = "sn"</span><br><span class="line">username = "cn"</span><br><span class="line">member_of = "cn"</span><br><span class="line">email =  "mail"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">admins组的用户分配到Admin权限组</span></span><br><span class="line">[[servers.group_mappings]]</span><br><span class="line">group_dn = "AWS-UAT-Admins"</span><br><span class="line">org_role = "Admin"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">editors组的用户分配到Editor权限组</span></span><br><span class="line">[[servers.group_mappings]]</span><br><span class="line">group_dn = "grafana-editors"</span><br><span class="line">org_role = "Editor"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">viewers组的用户分配到viewer权限组</span></span><br><span class="line">[[servers.group_mappings]]</span><br><span class="line">group_dn = "grafana-viewers"</span><br><span class="line">org_role = "Viewer"</span><br></pre></td></tr></table></figure><p>配置完后重启grafana服务</p><h3 id="测试ldap账户登录"><a href="#测试ldap账户登录" class="headerlink" title="测试ldap账户登录"></a>测试ldap账户登录</h3><p><img src="/2022/08/22/grafana-shi-yong-ssl-jia-mi-ji-cheng-openldap/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;准备grafana相应的权限用户组&quot;&gt;&lt;a href=&quot;#准备grafana相应的权限用户组&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="Grafana" scheme="http://chenzhonzhou.github.io/categories/Grafana/"/>
    
    
      <category term="OpenLDAP" scheme="http://chenzhonzhou.github.io/tags/OpenLDAP/"/>
    
      <category term="Grafana" scheme="http://chenzhonzhou.github.io/tags/Grafana/"/>
    
  </entry>
  
  <entry>
    <title>OpenLDAP 开启SSL/TLS加密通信</title>
    <link href="http://chenzhonzhou.github.io/2022/08/17/openldap-kai-qi-ssl-tls-jia-mi-tong-xin/"/>
    <id>http://chenzhonzhou.github.io/2022/08/17/openldap-kai-qi-ssl-tls-jia-mi-tong-xin/</id>
    <published>2022-08-17T03:34:27.000Z</published>
    <updated>2022-08-18T07:16:34.940Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><h3 id="Openldap为啥要加密"><a href="#Openldap为啥要加密" class="headerlink" title="Openldap为啥要加密"></a>Openldap为啥要加密</h3><p>​ Openldap默认使用简单验证，对slapd的所有访问都使用明文密码通过未加密通道进行。为了确保信息安全，需要对信息进行加密传输，SSL(Secure Sockets Layer)是一个可靠的解决方案。</p><p>​ 它使用X.509证书，由可信任第三方（Certificate Authority(CA)）进行数字签名的一个标准格式的数据。有效的数字签名意味着已签名的数据没有被篡改。如果签名的数据被更改，将不会通过验证。</p><h3 id="SSL-TLS加密原理简介"><a href="#SSL-TLS加密原理简介" class="headerlink" title="SSL/TLS加密原理简介"></a>SSL/TLS加密原理简介</h3><p>​ SSL/TLS是基于PKI机制的加密方式，包括证书认证、密钥交换、非对称加密、对称加密。SSL/TLS采用CA作为服务端核客户端都信赖的具有权威性的组织，证书的颁发和认证都依赖于CA，并假定CA颁发的证书是可靠的、可信赖的，证书里面的内容是真实的、有效的，并可用于客户机和服务器进行安全的可靠的通信加密。</p><p>​ SSL/TLS证书用来认证服务器和客户机双方的身份，并用于密钥交换的非对称加密。密钥交换完毕之后，就可以用这个密钥做通信数据的对称加密了，具体的加密算法是由客户机和服务器相互协商得来的。服务器和客户机由于SSL/TLS库的不同以及用户的配置不同，双方支持的算法列表不完全相同，当双方做SSL/TLS握手的时候，就需要将自己支持的算法列表以及优先顺序告知对方，一旦对方按照优先顺序找到了第一个支持的算法，那么协商完成，否则双方协商失败，SSL/TLS连接断开。</p><h2 id="二、自生成证书"><a href="#二、自生成证书" class="headerlink" title="二、自生成证书"></a>二、自生成证书</h2><h2 id="2-1-自建-CA-中心"><a href="#2-1-自建-CA-中心" class="headerlink" title="2.1 自建 CA 中心"></a>2.1 自建 CA 中心</h2><h3 id="2-1-1-CA中心生成自身私钥"><a href="#2-1-1-CA中心生成自身私钥" class="headerlink" title="2.1.1 CA中心生成自身私钥"></a>2.1.1 CA中心生成自身私钥</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost openldap]# cd /etc/pki/CA</span><br><span class="line">[root@localhost CA]# openssl genrsa -out private/cakey.pem 2048</span><br><span class="line">Generating RSA private key, 2048 bit long modulus</span><br><span class="line">............+++</span><br><span class="line">....................+++</span><br><span class="line">e is 65537 (0x10001)</span><br></pre></td></tr></table></figure><h3 id="2-1-2-CA签发自身公钥"><a href="#2-1-2-CA签发自身公钥" class="headerlink" title="2.1.2 CA签发自身公钥"></a>2.1.2 CA签发自身公钥</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost CA]# openssl  req -new -x509 -key private/cakey.pem -out cacert.pem -days 3650</span><br><span class="line">You are about to be asked to enter information that will be incorporated</span><br><span class="line">into your certificate request.</span><br><span class="line">What you are about to enter is what is called a Distinguished Name or a DN.</span><br><span class="line">There are quite a few fields but you can leave some blank</span><br><span class="line">For some fields there will be a default value,</span><br><span class="line">If you enter '.', the field will be left blank.</span><br><span class="line">-----</span><br><span class="line">Country Name (2 letter code) [XX]:CN</span><br><span class="line">State or Province Name (full name) []:BeiJing</span><br><span class="line">Locality Name (eg, city) [Default City]:BeiJing</span><br><span class="line">Organization Name (eg, company) [Default Company Ltd]:sys.com</span><br><span class="line">Organizational Unit Name (eg, section) []:Devops</span><br><span class="line">Common Name (eg, your name or your server's hostname) []:ldap.sys.com</span><br><span class="line">Email Address []:ldap@sys.com</span><br></pre></td></tr></table></figure><p>信息可以随便填写，但后面生成LDAP证书的时候需要和这里的信息保持一致</p><h3 id="2-1-3-创建index-txt和serial文件"><a href="#2-1-3-创建index-txt和serial文件" class="headerlink" title="2.1.3 创建index.txt和serial文件"></a>2.1.3 创建index.txt和serial文件</h3><p>index.txt 文件用于存放客户端证书信息，serial 文件用于存放客户端证书编号，可以自定义，用于识别客户端证书</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost CA]# touch serial index.txt</span><br><span class="line">[root@localhost CA]# echo "01" &gt; serial</span><br></pre></td></tr></table></figure><h3 id="2-1-4-使用openssl命令获取证书信息"><a href="#2-1-4-使用openssl命令获取证书信息" class="headerlink" title="2.1.4 使用openssl命令获取证书信息"></a>2.1.4 使用openssl命令获取证书信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost CA]# openssl x509 -noout -text -in /etc/pki/CA/cacert.pem</span><br><span class="line">Certificate:</span><br><span class="line">    Data:</span><br><span class="line">        Version: 3 (0x2)</span><br><span class="line">        Serial Number:</span><br><span class="line">            99:9d:0e:5e:77:51:2c:38</span><br><span class="line">    Signature Algorithm: sha256WithRSAEncryption</span><br><span class="line">        Issuer: C=CN, ST=BeiJing, L=BeiJing, O=sys.com, OU=Devops, CN=ldap.sys.com/emailAddress=ldap@sys.com</span><br><span class="line">        Validity</span><br><span class="line">            Not Before: Aug 17 09:07:28 2022 GMT</span><br><span class="line">            Not After : Aug 14 09:07:28 2032 GMT</span><br><span class="line">        Subject: C=CN, ST=BeiJing, L=BeiJing, O=sys.com, OU=Devops, CN=ldap.sys.com/emailAddress=ldap@sys.com</span><br><span class="line">        Subject Public Key Info:</span><br><span class="line">            Public Key Algorithm: rsaEncryption</span><br><span class="line">                Public-Key: (2048 bit)</span><br><span class="line">                Modulus:</span><br><span class="line">                    00:a6:08:87:0e:0b:cb:85:8b:ef:03:1e:c9:5c:ed:</span><br><span class="line">                    eb:b3:02:77:dd:4d:ad:4b:b5:ca:b3:7c:8c:03:12:</span><br><span class="line">                    63:c5:8c:89:1e:a4:15:c9:4c:c1:68:e0:8c:74:3d:</span><br><span class="line">                    9b:2b:a2:8e:cf:ad:3c:40:42:e7:ff:e8:27:b7:98:</span><br><span class="line">                    73:99:2d:33:b6:c9:39:ce:62:07:cd:ae:65:ea:c2:</span><br><span class="line">                    7a:0a:eb:84:ff:42:db:56:da:e1:6a:ef:fb:fc:29:</span><br><span class="line">                    75:73:1d:00:15:e5:04:f2:fe:d4:4e:f5:00:08:29:</span><br><span class="line">                    b8:f9:89:41:7d:c8:a5:61:ef:10:8f:5d:29:ce:d3:</span><br><span class="line">                    d6:c2:d9:33:4c:ab:e1:d5:49:90:51:b7:3f:a4:6f:</span><br><span class="line">                    7b:6c:2d:1a:8e:8f:73:a6:af:c7:7d:c4:58:7d:36:</span><br><span class="line">                    d4:e7:eb:4c:1a:ba:23:9d:ac:6b:30:54:ba:0a:fb:</span><br><span class="line">                    13:1b:27:7a:a7:f5:ad:3f:e6:be:8b:f7:a3:52:a5:</span><br><span class="line">                    05:23:42:24:56:ba:7d:80:ce:81:fb:00:05:89:19:</span><br><span class="line">                    31:f1:19:66:a7:a8:57:98:5b:5d:b6:9e:4c:bf:a3:</span><br><span class="line">                    15:25:1c:e9:76:cd:84:48:50:0b:e8:f8:cf:df:cb:</span><br><span class="line">                    1e:69:aa:7e:51:73:f6:e8:59:3c:bb:d4:0d:a1:a7:</span><br><span class="line">                    22:3f:54:b2:ae:7c:ea:33:d3:75:64:94:52:aa:2e:</span><br><span class="line">                    b5:33</span><br><span class="line">                Exponent: 65537 (0x10001)</span><br><span class="line">        X509v3 extensions:</span><br><span class="line">            X509v3 Subject Key Identifier:</span><br><span class="line">                15:E1:DF:F5:24:B8:F2:AD:C2:93:0B:92:48:E6:EC:A8:D5:25:88:B8</span><br><span class="line">            X509v3 Authority Key Identifier:</span><br><span class="line">                keyid:15:E1:DF:F5:24:B8:F2:AD:C2:93:0B:92:48:E6:EC:A8:D5:25:88:B8</span><br><span class="line"></span><br><span class="line">            X509v3 Basic Constraints:</span><br><span class="line">                CA:TRUE</span><br><span class="line">    Signature Algorithm: sha256WithRSAEncryption</span><br><span class="line">         01:2e:84:f1:ee:ce:99:b1:77:1b:f3:b4:ab:21:80:8a:8a:04:</span><br><span class="line">         16:23:f9:48:85:d5:db:bf:d1:00:d8:2f:7f:2c:b8:e5:1e:6b:</span><br><span class="line">         0f:f7:10:41:b2:a4:75:84:bf:0b:b6:eb:97:3e:06:07:30:f6:</span><br><span class="line">         c7:f2:6f:9c:ed:9a:0e:70:fd:14:cc:a4:34:b9:ef:a8:69:a7:</span><br><span class="line">         c4:f3:ff:00:b2:2d:c6:ac:3a:35:86:58:25:2a:be:0c:4f:20:</span><br><span class="line">         52:91:98:f3:06:33:79:ce:c7:cb:8c:a2:a3:ca:6d:2a:60:94:</span><br><span class="line">         1d:97:38:d1:f5:55:f6:db:30:ff:67:85:c7:0e:7f:08:eb:88:</span><br><span class="line">         e0:30:b1:f9:6e:01:a8:fa:16:53:53:12:62:af:ca:35:cf:85:</span><br><span class="line">         e2:be:7c:39:70:57:7b:06:19:4a:aa:8a:12:8a:e7:3f:a9:dd:</span><br><span class="line">         11:f4:45:96:6f:1c:82:90:62:bb:24:57:a5:cc:a8:99:96:80:</span><br><span class="line">         8c:48:75:34:94:05:e2:42:9c:64:81:11:d9:f2:1c:c7:c2:4c:</span><br><span class="line">         fa:ad:16:23:7d:ba:a0:26:fc:b5:df:df:5d:34:6d:1c:39:61:</span><br><span class="line">         e2:45:e2:0d:00:22:a2:89:72:d2:25:e0:b0:c0:25:70:8f:bf:</span><br><span class="line">         e3:4c:a9:bd:a5:60:67:d6:d3:77:a2:aa:6e:92:2f:cb:17:fb:</span><br><span class="line">         a4:ef:b2:d3</span><br></pre></td></tr></table></figure><h2 id="2-2-生成-LDAP-证书"><a href="#2-2-生成-LDAP-证书" class="headerlink" title="2.2 生成 LDAP 证书"></a>2.2 生成 LDAP 证书</h2><h3 id="2-2-1-获取LDAP证书"><a href="#2-2-1-获取LDAP证书" class="headerlink" title="2.2.1 获取LDAP证书"></a>2.2.1 获取LDAP证书</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost CA]# mkdir /etc/openldap/ssl</span><br><span class="line">[root@localhost CA]# cd /etc/openldap/ssl</span><br></pre></td></tr></table></figure><p>生成服务端密钥</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ssl]# openssl genrsa -out ldapkey.pem 2048</span><br><span class="line">Generating RSA private key, 2048 bit long modulus</span><br><span class="line">....................................+++</span><br><span class="line">......................................................+++</span><br><span class="line">e is 65537 (0x10001)</span><br></pre></td></tr></table></figure><p>服务端向CA中心申请证书签署请求，相关信息必须和CA所填写一致才可以正常签发</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ssl]# openssl req -new -key ldapkey.pem -out ldap.csr -days 3650</span><br></pre></td></tr></table></figure><h3 id="2-2-2-生成LDAP证书"><a href="#2-2-2-生成LDAP证书" class="headerlink" title="2.2.2 生成LDAP证书"></a>2.2.2 生成LDAP证书</h3><p>CA检测用户请求，通过后生成证书</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ssl]# openssl ca -in ldap.csr -out ldapcert.pem -days 3650</span><br><span class="line">Using configuration from /etc/pki/tls/openssl.cnf</span><br><span class="line">Check that the request matches the signature</span><br><span class="line">Signature ok</span><br><span class="line">Certificate Details:</span><br><span class="line">        Serial Number: 1 (0x1)</span><br><span class="line">        Validity</span><br><span class="line">            Not Before: Aug 17 09:26:34 2022 GMT</span><br><span class="line">            Not After : Aug 14 09:26:34 2032 GMT</span><br><span class="line">        Subject:</span><br><span class="line">            countryName               = CN</span><br><span class="line">            stateOrProvinceName       = BeiJing</span><br><span class="line">            organizationName          = sys.com</span><br><span class="line">            organizationalUnitName    = Devops</span><br><span class="line">            commonName                = ldap.sys.com</span><br><span class="line">            emailAddress              = ldap@sys.com</span><br><span class="line">        X509v3 extensions:</span><br><span class="line">            X509v3 Basic Constraints:</span><br><span class="line">                CA:FALSE</span><br><span class="line">            Netscape Comment:</span><br><span class="line">                OpenSSL Generated Certificate</span><br><span class="line">            X509v3 Subject Key Identifier:</span><br><span class="line">                AD:14:68:D1:B3:1D:4E:34:5A:EA:B0:F5:78:74:C8:51:0B:D8:83:E7</span><br><span class="line">            X509v3 Authority Key Identifier:</span><br><span class="line">                keyid:15:E1:DF:F5:24:B8:F2:AD:C2:93:0B:92:48:E6:EC:A8:D5:25:88:B8</span><br><span class="line"></span><br><span class="line">Certificate is to be certified until Aug 14 09:26:34 2032 GMT (3650 days)</span><br><span class="line">Sign the certificate? [y/n]:y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1 out of 1 certificate requests certified, commit? [y/n]y</span><br><span class="line">Write out database with 1 new entries</span><br><span class="line">Data Base Updated</span><br></pre></td></tr></table></figure><h3 id="2-2-3-验证证书"><a href="#2-2-3-验证证书" class="headerlink" title="2.2.3 验证证书"></a>2.2.3 验证证书</h3><p>成功生成证书后，可以 openssl 验证服务端证书的合法性</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ssl]# openssl verify -CAfile /etc/pki/CA/cacert.pem /etc/openldap/ssl/ldapcert.pem</span><br><span class="line">/etc/openldap/ssl/ldapcert.pem: OK</span><br></pre></td></tr></table></figure><h3 id="2-2-4-准备证书文件"><a href="#2-2-4-准备证书文件" class="headerlink" title="2.2.4 准备证书文件"></a>2.2.4 准备证书文件</h3><p>后面ldap开启SSL需要使用到<code>cacert.pem</code>,<code>ldapcert.pem</code>,<code>ldapkey.pem</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ssl]# cp /etc/pki/CA/cacert.pem /etc/openldap/ssl/</span><br><span class="line">[root@localhost ssl]# ls /etc/openldap/ssl/</span><br><span class="line">cacert.pem  ldapcert.pem  ldap.csr  ldapkey.pem</span><br></pre></td></tr></table></figure><h2 id="三、OpenLDAP-开启-SSL"><a href="#三、OpenLDAP-开启-SSL" class="headerlink" title="三、OpenLDAP 开启 SSL"></a>三、OpenLDAP 开启 SSL</h2><h3 id="3-1-自建-OpenLDAP"><a href="#3-1-自建-OpenLDAP" class="headerlink" title="3.1 自建 OpenLDAP"></a>3.1 自建 OpenLDAP</h3><p>这里以我之前手动部署了一个openldap 2.4.44为例，开启SSL，不需要重新生成数据直接开启SSL就行</p><p>OpenLDAP 2.5版本开始支持很多新功能，像<code>多重身份认证</code>,<code>密码过期提醒</code>等，更多信息可前往<a href="https://www.openldap.org/software/roadmap.html" target="_blank" rel="noopener">官网</a>查看</p><h4 id="3-1-1-创建ssl的ldif配置文件"><a href="#3-1-1-创建ssl的ldif配置文件" class="headerlink" title="3.1.1 创建ssl的ldif配置文件"></a>3.1.1 创建ssl的ldif配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost schema]# vim enable_ssl.ldif</span><br><span class="line">dn: cn=config</span><br><span class="line">changetype: modify</span><br><span class="line">replace: olcTLSCACertificateFile</span><br><span class="line">olcTLSCACertificateFile: /etc/openldap/ssl/cacert.pem</span><br><span class="line">-</span><br><span class="line">replace: olcTLSCertificateFile</span><br><span class="line">olcTLSCertificateFile: /etc/openldap/ssl/ldapcert.pem</span><br><span class="line">-</span><br><span class="line">replace: olcTLSCertificateKeyFile</span><br><span class="line">olcTLSCertificateKeyFile: /etc/openldap/ssl/ldapkey.pem</span><br><span class="line">-</span><br><span class="line">add: olcTLSVerifyClient</span><br><span class="line">olcTLSVerifyClient: never</span><br></pre></td></tr></table></figure><h4 id="3-1-2-导入ssl配置"><a href="#3-1-2-导入ssl配置" class="headerlink" title="3.1.2 导入ssl配置"></a>3.1.2 导入ssl配置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost schema]# ldapmodify -Y EXTERNAL -H ldapi:/// -D "cn=admin,dc=sys,dc=com" -w Abc.123456 -f enable_ssl.ldif</span><br><span class="line">SASL/EXTERNAL authentication started</span><br><span class="line">SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth</span><br><span class="line">SASL SSF: 0</span><br><span class="line">modifying entry "cn=config"</span><br></pre></td></tr></table></figure><h4 id="3-1-3-配置只开启ldaps"><a href="#3-1-3-配置只开启ldaps" class="headerlink" title="3.1.3 配置只开启ldaps"></a>3.1.3 配置只开启ldaps</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost schema]# vim /etc/sysconfig/slapd</span><br><span class="line">SLAPD_URLS="ldapi:/// ldaps:///"</span><br></pre></td></tr></table></figure><h4 id="3-1-4-重启slapd服务"><a href="#3-1-4-重启slapd服务" class="headerlink" title="3.1.4 重启slapd服务"></a>3.1.4 重启slapd服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost schema]# systemctl restart slapd</span><br><span class="line">[root@localhost schema]# netstat -ntpl</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      957/sshd</span><br><span class="line">tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1152/master</span><br><span class="line">tcp        0      0 0.0.0.0:636             0.0.0.0:*               LISTEN      5650/slapd</span><br></pre></td></tr></table></figure><p>已经开启了636端口</p><h4 id="3-1-5-测试连接"><a href="#3-1-5-测试连接" class="headerlink" title="3.1.5 测试连接"></a>3.1.5 测试连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#命令行连接需要开启</span></span></span><br><span class="line">[root@localhost schema]# echo "TLS_REQCERT allow" &gt;&gt; /etc/openldap/ldap.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#测试是否可以匿名访问，关闭匿名访问可以参考我的另一篇OpenLDAP 禁止匿名访问</span></span></span><br><span class="line">[root@localhost schema]# ldapwhoami -v -x -Z</span><br><span class="line">ldap_initialize( &lt;DEFAULT&gt; )</span><br><span class="line">anonymous</span><br><span class="line">Result: Success (0)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#测试连接</span></span></span><br><span class="line">[root@localhost schema]# ldapwhoami -D "cn=admin,dc=sys,dc=com" -w Abc.123456 -H ldaps://192.168.126.145:636 -v</span><br><span class="line">ldap_initialize( ldaps://192.168.126.145:636/??base )</span><br><span class="line">dn:cn=admin,dc=sys,dc=com</span><br><span class="line">Result: Success (0)</span><br><span class="line">[root@localhost schema]#</span><br></pre></td></tr></table></figure><p>可以正常连接ldap，说明配置成功了<br>同时也可以测试当前套接字是否能通过CA的验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost schema]# openssl s_client -connect 192.168.126.145:636 -showcerts -state -CAfile /etc/openldap/ssl/cacert.pem</span><br><span class="line">CONNECTED(00000003)</span><br><span class="line">SSL_connect:before/connect initialization</span><br><span class="line">SSL_connect:SSLv2/v3 write client hello A</span><br><span class="line">SSL_connect:SSLv3 read server hello A</span><br><span class="line">depth=1 C = CN, ST = BeiJing, L = BeiJing, O = sys.com, OU = Devops, CN = ldap.sys.com, emailAddress = ldap@sys.com</span><br><span class="line">verify return:1</span><br><span class="line">depth=0 C = CN, ST = BeiJing, O = sys.com, OU = Devops, CN = ldap.sys.com, emailAddress = ldap@sys.com</span><br><span class="line">verify return:1</span><br><span class="line">SSL_connect:SSLv3 read server certificate A</span><br><span class="line">SSL_connect:SSLv3 read server done A</span><br><span class="line">SSL_connect:SSLv3 write client key exchange A</span><br><span class="line">SSL_connect:SSLv3 write change cipher spec A</span><br><span class="line">SSL_connect:SSLv3 write finished A</span><br><span class="line">SSL_connect:SSLv3 flush data</span><br><span class="line">SSL_connect:SSLv3 read server session ticket A</span><br><span class="line">SSL_connect:SSLv3 read finished A</span><br><span class="line">---</span><br><span class="line">Certificate chain</span><br><span class="line"> 0 s:/C=CN/ST=BeiJing/O=sys.com/OU=Devops/CN=ldap.sys.com/emailAddress=ldap@sys.com</span><br><span class="line">   i:/C=CN/ST=BeiJing/L=BeiJing/O=sys.com/OU=Devops/CN=ldap.sys.com/emailAddress=ldap@sys.com</span><br><span class="line">-----BEGIN CERTIFICATE-----</span><br><span class="line">MIID9TCCAt2gAwIBAgIBATANBgkqhkiG9w0BAQsFADCBiDELMAkGA1UEBhMCQ04x</span><br><span class="line">EDAOBgNVBAgMB0JlaUppbmcxEDAOBgNVBAcMB0JlaUppbmcxEDAOBgNVBAoMB3N5</span><br><span class="line">cy5jb20xDzANBgNVBAsMBkRldm9wczEVMBMGA1UEAwwMbGRhcC5zeXMuY29tMRsw</span><br><span class="line">GQYJKoZIhvcNAQkBFgxsZGFwQHN5cy5jb20wHhcNMjIwODE3MDkyNjM0WhcNMzIw</span><br><span class="line">ODE0MDkyNjM0WjB2MQswCQYDVQQGEwJDTjEQMA4GA1UECAwHQmVpSmluZzEQMA4G</span><br><span class="line">A1UECgwHc3lzLmNvbTEPMA0GA1UECwwGRGV2b3BzMRUwEwYDVQQDDAxsZGFwLnN5</span><br><span class="line">cy5jb20xGzAZBgkqhkiG9w0BCQEWDGxkYXBAc3lzLmNvbTCCASIwDQYJKoZIhvcN</span><br><span class="line">AQEBBQADggEPADCCAQoCggEBANLIIW1esDMAfv/WVFMvHWjeOfkzHxj7U3Npi5o6</span><br><span class="line">m/TYY7tklQYVLl+qdXhVyqOFIxPqISDaIov/33ubgI+2QS6YAGUtaXiNLjrd4G5P</span><br><span class="line">yZ1eOMBR567OojiroF6ZX+ukwp/YK1qD/FjY1lhd2PXnqKmKRfD/6JXmx8fPFsXf</span><br><span class="line">OI2dlMzwE7R+A8O02QUkVtm3fPDKOV67wld2dlrr9F0pf7i3CVW8nQr7ys+487a8</span><br><span class="line">jXEI/AwIsLX1tmJxMZBuswJn+Q3If0n4R9agzfTOm+PuTCCOHbDvBQtSnhogLSRy</span><br><span class="line">OZnkEfAZpSVoo8qZ6rTkZdXbkfN10XJvT2E5f7dhZe+Kj9sCAwEAAaN7MHkwCQYD</span><br><span class="line">VR0TBAIwADAsBglghkgBhvhCAQ0EHxYdT3BlblNTTCBHZW5lcmF0ZWQgQ2VydGlm</span><br><span class="line">aWNhdGUwHQYDVR0OBBYEFK0UaNGzHU40Wuqw9Xh0yFEL2IPnMB8GA1UdIwQYMBaA</span><br><span class="line">FBXh3/UkuPKtwpMLkkjm7KjVJYi4MA0GCSqGSIb3DQEBCwUAA4IBAQB1jIRE0C4c</span><br><span class="line">RAsQTkGEr1p+xkhnkIcBm+ryIyLDRDSPBa6Knd00L//VhM0IcIQCWVRG2LwbfVAD</span><br><span class="line">b8EszOVXB5UHPNyOWUO5aO1jzfeCmMw+IIurYW4iLftMRAwKakwpUobdPmgfuDy9</span><br><span class="line">4SrGAbLEAS3nYt/scM6sGJnQ4cCM9E7kGwyM7a2HqB/XKl6bOESTZ2yhtjaOHT32</span><br><span class="line">vzsr5iQAzSf0Kytp63Y3nYewLO/nE5FlBT84c32nMsliBwN7Tl3CSu1844cnJAPE</span><br><span class="line">pZlBZoGKZGWRGDY4Dw1Tk6PxfrmpmGzVoU7NiyshyThBMx/89Mxx7xQgP4zWtWkA</span><br><span class="line">l0Fyzr3jF3z0</span><br><span class="line">-----END CERTIFICATE-----</span><br><span class="line"> 1 s:/C=CN/ST=BeiJing/L=BeiJing/O=sys.com/OU=Devops/CN=ldap.sys.com/emailAddress=ldap@sys.com</span><br><span class="line">   i:/C=CN/ST=BeiJing/L=BeiJing/O=sys.com/OU=Devops/CN=ldap.sys.com/emailAddress=ldap@sys.com</span><br><span class="line">-----BEGIN CERTIFICATE-----</span><br><span class="line">MIID5TCCAs2gAwIBAgIJAJmdDl53USw4MA0GCSqGSIb3DQEBCwUAMIGIMQswCQYD</span><br><span class="line">VQQGEwJDTjEQMA4GA1UECAwHQmVpSmluZzEQMA4GA1UEBwwHQmVpSmluZzEQMA4G</span><br><span class="line">A1UECgwHc3lzLmNvbTEPMA0GA1UECwwGRGV2b3BzMRUwEwYDVQQDDAxsZGFwLnN5</span><br><span class="line">cy5jb20xGzAZBgkqhkiG9w0BCQEWDGxkYXBAc3lzLmNvbTAeFw0yMjA4MTcwOTA3</span><br><span class="line">MjhaFw0zMjA4MTQwOTA3MjhaMIGIMQswCQYDVQQGEwJDTjEQMA4GA1UECAwHQmVp</span><br><span class="line">SmluZzEQMA4GA1UEBwwHQmVpSmluZzEQMA4GA1UECgwHc3lzLmNvbTEPMA0GA1UE</span><br><span class="line">CwwGRGV2b3BzMRUwEwYDVQQDDAxsZGFwLnN5cy5jb20xGzAZBgkqhkiG9w0BCQEW</span><br><span class="line">DGxkYXBAc3lzLmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKYI</span><br><span class="line">hw4Ly4WL7wMeyVzt67MCd91NrUu1yrN8jAMSY8WMiR6kFclMwWjgjHQ9myuijs+t</span><br><span class="line">PEBC5//oJ7eYc5ktM7bJOc5iB82uZerCegrrhP9C21ba4Wrv+/wpdXMdABXlBPL+</span><br><span class="line">1E71AAgpuPmJQX3IpWHvEI9dKc7T1sLZM0yr4dVJkFG3P6Rve2wtGo6Pc6avx33E</span><br><span class="line">WH021OfrTBq6I52sazBUugr7Exsneqf1rT/mvov3o1KlBSNCJFa6fYDOgfsABYkZ</span><br><span class="line">MfEZZqeoV5hbXbaeTL+jFSUc6XbNhEhQC+j4z9/LHmmqflFz9uhZPLvUDaGnIj9U</span><br><span class="line">sq586jPTdWSUUqoutTMCAwEAAaNQME4wHQYDVR0OBBYEFBXh3/UkuPKtwpMLkkjm</span><br><span class="line">7KjVJYi4MB8GA1UdIwQYMBaAFBXh3/UkuPKtwpMLkkjm7KjVJYi4MAwGA1UdEwQF</span><br><span class="line">MAMBAf8wDQYJKoZIhvcNAQELBQADggEBAAEuhPHuzpmxdxvztKshgIqKBBYj+UiF</span><br><span class="line">1du/0QDYL38suOUeaw/3EEGypHWEvwu265c+Bgcw9sfyb5ztmg5w/RTMpDS576hp</span><br><span class="line">p8Tz/wCyLcasOjWGWCUqvgxPIFKRmPMGM3nOx8uMoqPKbSpglB2XONH1VfbbMP9n</span><br><span class="line">hccOfwjriOAwsfluAaj6FlNTEmKvyjXPheK+fDlwV3sGGUqqihKK5z+p3RH0RZZv</span><br><span class="line">HIKQYrskV6XMqJmWgIxIdTSUBeJCnGSBEdnyHMfCTPqtFiN9uqAm/LXf3100bRw5</span><br><span class="line">YeJF4g0AIqKJctIl4LDAJXCPv+NMqb2lYGfW03eiqm6SL8sX+6TvstM=</span><br><span class="line">-----END CERTIFICATE-----</span><br><span class="line">---</span><br><span class="line">Server certificate</span><br><span class="line">subject=/C=CN/ST=BeiJing/O=sys.com/OU=Devops/CN=ldap.sys.com/emailAddress=ldap@sys.com</span><br><span class="line">issuer=/C=CN/ST=BeiJing/L=BeiJing/O=sys.com/OU=Devops/CN=ldap.sys.com/emailAddress=ldap@sys.com</span><br><span class="line">---</span><br><span class="line">No client certificate CA names sent</span><br><span class="line">---</span><br><span class="line">SSL handshake has read 2334 bytes and written 607 bytes</span><br><span class="line">---</span><br><span class="line">New, TLSv1/SSLv3, Cipher is AES256-GCM-SHA384</span><br><span class="line">Server public key is 2048 bit</span><br><span class="line">Secure Renegotiation IS supported</span><br><span class="line">Compression: NONE</span><br><span class="line">Expansion: NONE</span><br><span class="line">No ALPN negotiated</span><br><span class="line">SSL-Session:</span><br><span class="line">    Protocol  : TLSv1.2</span><br><span class="line">    Cipher    : AES256-GCM-SHA384</span><br><span class="line">    Session-ID: 47754753CC89AABE4A33D4E89C6C382C3557F074D1F20EE69C5E07CE3284F7B2</span><br><span class="line">    Session-ID-ctx:</span><br><span class="line">    Master-Key: F115E963EF46F2C6608D0DF61CD676404B54B400E081A56E3C49CC2C8C73D74F59F6D524F5B96AD94F8497737848C71E</span><br><span class="line">    Key-Arg   : None</span><br><span class="line">    Krb5 Principal: None</span><br><span class="line">    PSK identity: None</span><br><span class="line">    PSK identity hint: None</span><br><span class="line">    TLS session ticket lifetime hint: 300 (seconds)</span><br><span class="line">    TLS session ticket:</span><br><span class="line">    0000 - e8 74 39 03 64 ab 5b 66-f8 bc 5c e6 1e cc 55 de   .t9.d.[f..\...U.</span><br><span class="line">    0010 - 99 de 08 3c da ef 99 3f-a9 52 13 e6 34 57 ec a5   ...&lt;...?.R..4W..</span><br><span class="line">    0020 - 4f 49 54 77 df 89 66 93-b1 d6 9f f0 76 39 1b 15   OITw..f.....v9..</span><br><span class="line">    0030 - fc 16 a3 fd 23 ad 7f 6d-a7 dc b4 01 89 3c 9e f4   ....#..m.....&lt;..</span><br><span class="line">    0040 - 0a cd d0 80 fe 90 b5 ff-42 03 31 f2 93 5a f7 af   ........B.1..Z..</span><br><span class="line">    0050 - 92 be 04 5e 22 16 3a 0d-36 46 7c 53 c1 6a f3 71   ...^".:.6F|S.j.q</span><br><span class="line">    0060 - e8 00 3c 01 0a d4 0f 23-6d 27 23 fd fc 91 25 a1   ..&lt;....#m'#...%.</span><br><span class="line">    0070 - 15 d5 da ce 95 f3 bb 74-e1 60 6a 3d 7e a9 81 e3   .......t.`j=~...</span><br><span class="line">    0080 - 5d 6c 2e 5d bb 9e 89 26-23 ab 08 99 66 ee 82 f7   ]l.]...&amp;#...f...</span><br><span class="line">    0090 - 37 2f 1c 0b b1 88 47 6f-45 2e ac ca 11 c2 7e 98   7/....GoE.....~.</span><br><span class="line"></span><br><span class="line">    Start Time: 1660732009</span><br><span class="line">    Timeout   : 300 (sec)</span><br><span class="line">    Verify return code: 0 (ok)</span><br><span class="line">---</span><br></pre></td></tr></table></figure><h3 id="3-2-容器-OpenLDAP"><a href="#3-2-容器-OpenLDAP" class="headerlink" title="3.2 容器 OpenLDAP"></a>3.2 容器 OpenLDAP</h3><p>为了节省资源现在很多服务都采用窗口方式部署，OpenLDAP也不例可以使用容器运行，这里新版本2.6.3为例，镜像使用的<code>bitnami/openldap</code> 目前只有这个OpenLDAP镜像在长期更新</p><h4 id="3-2-1-创建-OpenLDAP-pvc"><a href="#3-2-1-创建-OpenLDAP-pvc" class="headerlink" title="3.2.1 创建 OpenLDAP pvc"></a>3.2.1 创建 OpenLDAP pvc</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[ec2-user@localhost</span> <span class="string">ldap2.6]$</span> <span class="string">vim</span> <span class="string">openldap-pvc.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">openldap-data-pvc</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">monitor</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">10</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">openldap-config-pvc</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">monitor</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">10</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"></span><br><span class="line"><span class="string">[ec2-user@localhost</span> <span class="string">ldap2.6]$</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="bullet">-f</span> <span class="string">openldap-pvc.yaml</span></span><br></pre></td></tr></table></figure><p>注意：根据自己的环境修改<code>storageClassName:</code>存储类名称</p><h4 id="3-2-2-导入-OpenLDAP-证书"><a href="#3-2-2-导入-OpenLDAP-证书" class="headerlink" title="3.2.2 导入 OpenLDAP 证书"></a>3.2.2 导入 OpenLDAP 证书</h4><p>将生成的OpenLDAP证书导入到k8s的secret中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ec2-user@localhost ssl]$ kubectl create secret generic openldap-certs -nmonitor --from-file=./cacert.pem --from-file=./ldapcert.pem --from-file=./ldapkey.pem</span><br><span class="line">secret/openldap-certs created</span><br><span class="line">[ec2-user@localhost ssl]$ kubectl get secret -nmonitor |grep openldap</span><br><span class="line">openldap-certs                      Opaque                                3      12s</span><br></pre></td></tr></table></figure><h4 id="3-2-3-创建-OpenLDAP-deployment"><a href="#3-2-3-创建-OpenLDAP-deployment" class="headerlink" title="3.2.3 创建 OpenLDAP deployment"></a>3.2.3 创建 OpenLDAP deployment</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[ec2-user@localhost</span> <span class="string">ldap2.6]$</span> <span class="string">vim</span> <span class="string">openldap-deployment.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">app.kubernetes.io/alias-name:</span> <span class="string">LDAP</span></span><br><span class="line">    <span class="string">app.kubernetes.io/description:</span> <span class="string">认证中心</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">openldap</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">openldap</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">monitor</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  progressDeadlineSeconds:</span> <span class="number">600</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">openldap</span></span><br><span class="line"><span class="attr">  strategy:</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">Recreate</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">openldap</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">LDAP_ROOT</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">dc=sys,dc=com</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">LDAP_ADMIN_USERNAME</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">LDAP_ADMIN_PASSWORD</span></span><br><span class="line"><span class="attr">          value:</span> <span class="number">6</span><span class="meta">&amp;g0hbSRZJovaqjsA</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">LDAP_TLS_CERT_FILE</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">/opt/bitnami/openldap/certs/ldapcert.pem</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">LDAP_TLS_KEY_FILE</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">/opt/bitnami/openldap/certs/ldapkey.pem</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">LDAP_TLS_CA_FILE</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">/opt/bitnami/openldap/certs/cacert.pem</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">LDAP_ENABLE_TLS</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">"yes"</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">BITNAMI_DEBUG</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">bitnami/openldap:2.6.3</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">openldap</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">1389</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">tcp-389</span></span><br><span class="line"><span class="attr">          protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">1636</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">tcp-636</span></span><br><span class="line"><span class="attr">          protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">500</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">500</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">64</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">        terminationMessagePath:</span> <span class="string">/dev/termination-log</span></span><br><span class="line"><span class="attr">        terminationMessagePolicy:</span> <span class="string">File</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/bitnami/openldap/</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">ldap-data-pvc</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/opt/bitnami/openldap/certs</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">ldap-certs</span></span><br><span class="line"><span class="attr">      dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br><span class="line"><span class="attr">      restartPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">      schedulerName:</span> <span class="string">default-scheduler</span></span><br><span class="line"><span class="attr">      securityContext:</span> <span class="string">&#123;&#125;</span></span><br><span class="line"><span class="attr">      terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">ldap-data-pvc</span></span><br><span class="line"><span class="attr">        persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">          claimName:</span> <span class="string">openldap-data-pvc</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">ldap-certs</span></span><br><span class="line"><span class="attr">        secret:</span></span><br><span class="line"><span class="attr">          defaultMode:</span> <span class="number">420</span></span><br><span class="line"><span class="attr">          secretName:</span> <span class="string">openldap-certs</span></span><br><span class="line"></span><br><span class="line"><span class="string">[ec2-user@localhost</span> <span class="string">ldap2.6]$</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="bullet">-f</span> <span class="string">openldap-deployment.yaml</span></span><br></pre></td></tr></table></figure><h4 id="3-2-4-创建-OpenLDAP-service"><a href="#3-2-4-创建-OpenLDAP-service" class="headerlink" title="3.2.4 创建 OpenLDAP service"></a>3.2.4 创建 OpenLDAP service</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[ec2-user@localhost ldap2.6]$ cat openldap-svc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: openldap-svc</span><br><span class="line">  namespace: monitor</span><br><span class="line">  labels:</span><br><span class="line">    app: openldap-svc</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: tcp-389</span><br><span class="line">    nodePort: 30402</span><br><span class="line">    port: 1389</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 1389</span><br><span class="line">  - name: tcp-636</span><br><span class="line">    nodePort: 30381</span><br><span class="line">    port: 1636</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 1636</span><br><span class="line">  selector:</span><br><span class="line">    app: openldap</span><br><span class="line">  type: LoadBalancer</span><br><span class="line"></span><br><span class="line">[ec2-user@localhost ldap2.6]$ kubectl apply -f openldap-svc.yaml</span><br></pre></td></tr></table></figure><h4 id="3-2-5-测试连接"><a href="#3-2-5-测试连接" class="headerlink" title="3.2.5 测试连接"></a>3.2.5 测试连接</h4><p>这里使用ldapadmin连接</p><p><img src="/2022/08/17/openldap-kai-qi-ssl-tls-jia-mi-tong-xin/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><p><img src="/2022/08/17/openldap-kai-qi-ssl-tls-jia-mi-tong-xin/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;
      
    
    </summary>
    
    
      <category term="LDAP" scheme="http://chenzhonzhou.github.io/categories/LDAP/"/>
    
      <category term="OpenLDAP" scheme="http://chenzhonzhou.github.io/categories/LDAP/OpenLDAP/"/>
    
    
      <category term="OpenLDAP" scheme="http://chenzhonzhou.github.io/tags/OpenLDAP/"/>
    
      <category term="LDAP" scheme="http://chenzhonzhou.github.io/tags/LDAP/"/>
    
  </entry>
  
  <entry>
    <title>K8s 中运行OpenLDAP</title>
    <link href="http://chenzhonzhou.github.io/2022/08/03/k8s-zhong-yun-xing-openldap/"/>
    <id>http://chenzhonzhou.github.io/2022/08/03/k8s-zhong-yun-xing-openldap/</id>
    <published>2022-08-03T02:31:12.000Z</published>
    <updated>2022-08-04T02:43:03.453Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>通常我们会遵循专机专用的原则，单独使用一台服务器运行OpenLDAP，但实际使用到的资源只有几百兆甚至更少，存在一定的资源浪费，而且多出一台服务器也增加了一定的管理成本。这时如果在k8s容器中运行OpenLDAP的话，就能很好的解决这一问题。</p><h2 id="部署-OpenLDAP-容器服务"><a href="#部署-OpenLDAP-容器服务" class="headerlink" title="部署 OpenLDAP 容器服务"></a>部署 OpenLDAP 容器服务</h2><h3 id="创建-PVC"><a href="#创建-PVC" class="headerlink" title="创建 PVC"></a>创建 PVC</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[ec2-user@ip-</span> <span class="string">ldap]$</span> <span class="string">cat</span> <span class="string">openldap-pvc.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">openldap-data-pvc</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ops</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">10</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">openldap-config-pvc</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ops</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">10</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"></span><br><span class="line"><span class="string">[ec2-user@ip-</span> <span class="string">ldap]$</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="bullet">-f</span> <span class="string">openldap-pvc.yaml</span></span><br></pre></td></tr></table></figure><p>这里使用到了 <code>storageClassName</code> 储存类，可以参照各云厂商创建或自建存储类</p><h3 id="创建-deployment"><a href="#创建-deployment" class="headerlink" title="创建 deployment"></a>创建 deployment</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[ec2-user@ip-</span> <span class="string">ldap]$</span> <span class="string">cat</span> <span class="string">openldap-deployment.yaml</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">openldap</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ops</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">openldap</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">app.kubernetes.io/alias-name:</span> <span class="string">LDAP</span></span><br><span class="line">    <span class="string">app.kubernetes.io/description:</span> <span class="string">认证中心</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">openldap</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">openldap</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">openldap</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">'osixia/openldap:1.5.0'</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">tcp-389</span></span><br><span class="line"><span class="attr">              containerPort:</span> <span class="number">389</span></span><br><span class="line"><span class="attr">              protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">tcp-636</span></span><br><span class="line"><span class="attr">              containerPort:</span> <span class="number">636</span></span><br><span class="line"><span class="attr">              protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">LDAP_ORGANISATION</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">LDAP_DOMAIN</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">default.com</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">LDAP_ADMIN_PASSWORD</span></span><br><span class="line"><span class="attr">              value:</span> <span class="number">6</span><span class="meta">&amp;g0hbSRZJovaqjsA</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">LDAP_CONFIG_PASSWORD</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">C!DUwyUFZqqQj2&amp;!</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">LDAP_BACKEND</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">mdb</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line"><span class="attr">            limits:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="number">500</span><span class="string">m</span></span><br><span class="line"><span class="attr">              memory:</span> <span class="number">500</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">            requests:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">              memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ldap-config-pvc</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/etc/ldap/slapd.d</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ldap-data-pvc</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/var/lib/ldap</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">ldap-config-pvc</span></span><br><span class="line"><span class="attr">          persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">            claimName:</span> <span class="string">openldap-config-pvc</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">ldap-data-pvc</span></span><br><span class="line"><span class="attr">          persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">            claimName:</span> <span class="string">openldap-data-pvc</span></span><br><span class="line"></span><br><span class="line"><span class="string">[ec2-user@ip-</span> <span class="string">ldap]$</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="bullet">-f</span> <span class="string">openldap-deployment.yaml</span></span><br></pre></td></tr></table></figure><blockquote><p>这里使用的镜像<code>osixia/openldap:1.5.0</code>对应 OpenLDAP: slapd 2.4.57 版本<br>另外需要注意<code>env</code> 里面配置的参数记得做修改</p></blockquote><h3 id="创建-svc"><a href="#创建-svc" class="headerlink" title="创建 svc"></a>创建 svc</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[ec2-user@ip-</span> <span class="string">ldap]$</span> <span class="string">cat</span> <span class="string">openldap-svc.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">openldap-svc</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ops</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">openldap-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">tcp-389</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">389</span></span><br><span class="line"><span class="attr">    protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">389</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">tcp-636</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">636</span></span><br><span class="line"><span class="attr">    protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">636</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">openldap</span></span><br><span class="line">    </span><br><span class="line"><span class="string">[ec2-user@ip-</span> <span class="string">ldap]$</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="bullet">-f</span> <span class="string">openldap-svc.yaml</span></span><br></pre></td></tr></table></figure><p>至此，openldap服务就已经部署好了；接下来部署 phpldapadmin 页面用来管理openldap，使用客户端ldap管理工具的话可以跳过下面步骤</p><h2 id="部署-phpldap-管理-OpenLDAP"><a href="#部署-phpldap-管理-OpenLDAP" class="headerlink" title="部署 phpldap 管理 OpenLDAP"></a>部署 phpldap 管理 OpenLDAP</h2><h3 id="创建-phpldap-deploy-svc"><a href="#创建-phpldap-deploy-svc" class="headerlink" title="创建 phpldap deploy,svc"></a>创建 phpldap deploy,svc</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[ec2-user@ip-</span> <span class="string">ldap]$</span> <span class="string">cat</span> <span class="string">openldap-phpldapadmin.yaml</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ldap-phpldapadmin</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ops</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">ldap-phpldapadmin</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">app.kubernetes.io/alias-name:</span> <span class="string">LDAP</span></span><br><span class="line">    <span class="string">app.kubernetes.io/description:</span> <span class="string">LDAP在线工具</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">ldap-phpldapadmin</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">ldap-phpldapadmin</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">phpldapadmin</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">'osixia/phpldapadmin:stable'</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">tcp-80</span></span><br><span class="line"><span class="attr">              containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">              protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">PHPLDAPADMIN_HTTPS</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">'false'</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">PHPLDAPADMIN_LDAP_HOSTS</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">openldap-svc</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line"><span class="attr">            limits:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="number">500</span><span class="string">m</span></span><br><span class="line"><span class="attr">              memory:</span> <span class="number">500</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">            requests:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="number">10</span><span class="string">m</span></span><br><span class="line"><span class="attr">              memory:</span> <span class="number">10</span><span class="string">Mi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ldap-phpldapadmin-svc</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ops</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">ldap-phpldapadmin-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">tcp-80</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">ldap-phpldapadmin</span></span><br><span class="line">    </span><br><span class="line"><span class="string">[ec2-user@ip-</span> <span class="string">ldap]$</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="bullet">-f</span> <span class="string">openldap-phpldapadmin.yaml</span></span><br></pre></td></tr></table></figure><h3 id="创建-ingress-代理"><a href="#创建-ingress-代理" class="headerlink" title="创建 ingress 代理"></a>创建 ingress 代理</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[ec2-user@ip-</span> <span class="string">ldap]$</span> <span class="string">cat</span> <span class="string">phpldapadmin-ingress.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/whitelist-source-range:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span><span class="string">/0</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">phpldap</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ops</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">phpldap.xxx.xxx</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">ldap-phpldapadmin-svc</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">        pathType:</span> <span class="string">Prefix</span></span><br><span class="line"><span class="attr">  tls:</span></span><br><span class="line"><span class="attr">  - hosts:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">phpldap.xxx.xxx</span></span><br><span class="line"><span class="attr">    secretName:</span> <span class="string">xxx.xxx-20220322-20230322-https</span></span><br><span class="line"></span><br><span class="line"><span class="string">[ec2-user@ip-</span> <span class="string">ldap]$</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="bullet">-f</span> <span class="string">phpldapadmin-ingress.yaml</span></span><br></pre></td></tr></table></figure><h3 id="访问-phpldapadmin"><a href="#访问-phpldapadmin" class="headerlink" title="访问 phpldapadmin"></a>访问 phpldapadmin</h3><p><strong>login DN:</strong> cn=admin,dc=xxx,dc=com<br><strong>Password:</strong> 系统变量中的：LDAP_ADMIN_PASSWORD</p><p><img src="/2022/08/03/k8s-zhong-yun-xing-openldap/%E5%9B%BE%E7%89%872.png" alt="图片1"></p><p>顺利登录后就可以通过web页面对openldap进行管理了</p><p><img src="/2022/08/03/k8s-zhong-yun-xing-openldap/%E5%9B%BE%E7%89%873.png" alt="图片2"></p><p>这里在登录时还有个小插曲，上面我设置的ldap密码使用了特殊字符在web登录时认证失败，但我通过ldap客户端工具是可以登录的，后来把ldap密码中的特殊字符<code>&amp;</code>换掉就能顺利登录了</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;通常我们会
      
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://chenzhonzhou.github.io/categories/kubernetes/"/>
    
    
      <category term="OpenLDAP" scheme="http://chenzhonzhou.github.io/tags/OpenLDAP/"/>
    
      <category term="kubernetes" scheme="http://chenzhonzhou.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>K8s 容器PodDisruptionBudget 主动驱逐保护</title>
    <link href="http://chenzhonzhou.github.io/2022/06/28/k8s-rong-qi-poddisruptionbudget-zhu-dong-qu-zhu-bao-hu/"/>
    <id>http://chenzhonzhou.github.io/2022/06/28/k8s-rong-qi-poddisruptionbudget-zhu-dong-qu-zhu-bao-hu/</id>
    <published>2022-06-28T07:41:37.000Z</published>
    <updated>2022-06-29T03:14:02.289Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在 Kubernetes 中，为了保证业务不中断或业务SLA不降级，需要将应用进行集群化部署。通过PodDisruptionBudget 控制器可以设置应用POD集群处于运行状态最低个数，也可以设置应用POD集群处于运行状态的最低百分比，这样可以保证在主动销毁应用POD的时候，不会一次性销毁太多的应用POD，从而保证业务不中断或业务SLA不降级。</p><h2 id="PodDisruptionBudget-简介"><a href="#PodDisruptionBudget-简介" class="headerlink" title="PodDisruptionBudget 简介"></a>PodDisruptionBudget 简介</h2><p><code>Pod Disruption Budget</code> (pod 中断 预算) 简称<code>PDB</code>，含义其实是终止pod前通过 <code>labelSelector</code> 机制获取正常运行的<code>pod</code>数目的限制，目的是对<strong><code>自愿中断</code></strong>的保护措施。</p><blockquote><p>Kubernetes version &gt;= 1.7 才支持 <code>PodDisruptionBudget</code></p></blockquote><h2 id="PDB-应用场景"><a href="#PDB-应用场景" class="headerlink" title="PDB 应用场景"></a>PDB 应用场景</h2><p>节点维护或升级时 ( kubectl drain )</p><blockquote><p>注意：如果 Node 状态处于 <code>not ready</code>，PDB 是不会生效，因为 PDB 只能针对<code>自愿中断</code>生效，什么叫 <strong><code>自愿中断</code></strong> 下文介绍。</p></blockquote><h2 id="自愿中断和非自愿中断"><a href="#自愿中断和非自愿中断" class="headerlink" title="自愿中断和非自愿中断"></a>自愿中断和非自愿中断</h2><h3 id="非自愿中断"><a href="#非自愿中断" class="headerlink" title="非自愿中断"></a>非自愿中断</h3><p>Pod 不会消失，直到有人（人类或控制器）将其销毁，或者当出现不可避免的硬件或系统软件错误。</p><p>我们把这些不可避免的情况称为应用的<code>非自愿性中断</code>，例如：</p><ul><li>后端节点物理机的硬件故障</li><li>集群管理员错误地删除虚拟机（实例）</li><li>云提供商或管理程序故障使虚拟机消失</li><li>内核恐慌（kernel panic）</li><li>节点由于集群网络分区而从集群中消失</li><li>由于节点资源不足而将容器逐出</li></ul><p>除资源不足的情况外，大多数用户应该都熟悉以下这些情况；它们不是特定于 Kubernetes 的。</p><h3 id="自愿中断"><a href="#自愿中断" class="headerlink" title="自愿中断"></a>自愿中断</h3><p>包括由应用程序所有者发起的操作和由集群管理员发起的操作，我们称这些情况为<code>自愿中断</code>。</p><p><strong>典型的应用程序所有者操作包括</strong>：</p><ul><li>删除管理该 pod 的 Deployment 或其他控制器</li><li>更新了 Deployment 的 pod 模板导致 pod 重启</li><li>直接删除 pod（意外删除）</li></ul><p><strong>集群管理员操作包括</strong>：</p><ul><li>排空（drain）节点进行修复或升级。</li><li>从集群中排空节点以缩小集群（了解集群自动调节）。</li><li>从节点中移除一个 pod，以允许其他 pod 使用该节点。</li></ul><p>这些操作可能由集群管理员直接执行，也可能由集群管理员或集群托管提供商自动执行。</p><h2 id="PDB-关键参数与注意事项"><a href="#PDB-关键参数与注意事项" class="headerlink" title="PDB 关键参数与注意事项"></a>PDB 关键参数与注意事项</h2><blockquote><p>spec.minAvailable：表示发生自愿中断的过程中，要保证至少可用的Pods数或者百分比<br>spec.maxUnavailable：表示发生自愿中断的过程中，要保证最大不可用的Pods数或者百分比</p></blockquote><p>上面配置只能用来对应 Deployment，RS，RC，StatefulSet的Pods，推荐优先使用 .spec.maxUnavailable。</p><p>在极端的情况下，比如将maxUnavailable设置成0，或者设置成100%，那么就表示不能进行kubectl drain操作。同理将minAvailable设置成100%，或者设置成应用POD集群最大副本数，也表示不能进行kubectl drain操作。</p><p><strong>注意：</strong></p><ul><li>同一个 PDB Object 中不能同时定义 .spec.minAvailable 和 .spec.maxUnavailable。</li><li>前面提到，应用滚动更新时Pod的delete和unavailable虽然也属于自愿中断，但是实际上滚动更新有自己的策略控制（marSurge 和 maxUnavailable），因此PDB不会干预这个过程。</li><li>PDB 只能保证自愿中断时的副本数，比如 evict pod过程中刚好满足 .spec.minAvailable 或 .spec.maxUnavailable，这时某个本来正常的Pod突然因为Node Down(非自愿中断)挂了，那么这个时候实际Pods数就比PDB中要求的少了，因此PDB不是万能的！</li></ul><p>在使用上，如果设置 .spec.minAvailable 为 100% 或者 .spec.maxUnavailable 为 0%，意味着会完全阻止 evict pods 的过程（ Deployment和StatefulSet的滚动更新除外 ）。</p><h2 id="PDB-例子"><a href="#PDB-例子" class="headerlink" title="PDB 例子"></a>PDB 例子</h2><h3 id="定义-minAvailable"><a href="#定义-minAvailable" class="headerlink" title="定义 minAvailable"></a>定义 minAvailable</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodDisruptionBudget</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr"> name:</span> <span class="string">zk-pdb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr"> minAvailable:</span> <span class="number">2</span>  <span class="comment">#定义存活的zookeeper的pod最少为2个</span></span><br><span class="line"><span class="attr"> selector:</span></span><br><span class="line"><span class="attr">   matchLabels:</span></span><br><span class="line"><span class="attr">     app:</span> <span class="string">zookeeper</span></span><br></pre></td></tr></table></figure><h3 id="定义-maxUnavailable"><a href="#定义-maxUnavailable" class="headerlink" title="定义 maxUnavailable"></a>定义 maxUnavailable</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodDisruptionBudget</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr"> name:</span> <span class="string">zk-pdb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr"> maxUnavailable:</span> <span class="number">1</span>  <span class="comment">#定义非存活的zookeeper的pod最多为1个</span></span><br><span class="line"><span class="attr"> selector:</span></span><br><span class="line"><span class="attr">   matchLabels:</span></span><br><span class="line"><span class="attr">     app:</span> <span class="string">zookeeper</span></span><br></pre></td></tr></table></figure><p>创建资源清单</p><p>对于PodDisruptionBudget对象，无法直接进行更新操作，只能通过删除和重新创建来完成对PodDisruptionBudget对象的更新。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f zk-pdb.yaml</span></span><br></pre></td></tr></table></figure><p>查看状态</p><p>对于PodDisruptionBudget对象，无法直接进行更新操作，只能通过删除和重新创建来完成对PodDisruptionBudget对象的更新。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pdb</span></span><br><span class="line">NAME     MIN-AVAILABLE   ALLOWED-DISRUPTIONS   AGE</span><br><span class="line">zk-pdb   2               1                     7s</span><br></pre></td></tr></table></figure><p>查看详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl describe pdb pdb-demo</span></span><br><span class="line">Name:           zk-pdb</span><br><span class="line">Namespace:      default</span><br><span class="line">Min available:  2</span><br><span class="line">Selector:       app=zookeeper</span><br><span class="line">Status:</span><br><span class="line">    Allowed disruptions:  0</span><br><span class="line">    Current:              0</span><br><span class="line">    Desired:              2</span><br><span class="line">    Total:                0</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason  Age                   From               Message</span><br><span class="line">  ----    ------  ----                  ----               -------</span><br><span class="line">  Normal  NoPods  2m58s (x53 over 28m)  controllermanager  No matching pods found</span><br></pre></td></tr></table></figure><p>通过下面命令查看PodDisruptionBudget对象的详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get poddisruptionbudgets zk-pdb-o yaml</span></span><br><span class="line"></span><br><span class="line">apiVersion: policy/v1beta1</span><br><span class="line">kind: PodDisruptionBudget</span><br><span class="line">metadata:</span><br><span class="line"> name: zk-pdb</span><br><span class="line">...</span><br><span class="line">status:</span><br><span class="line"> currentHealthy: 3</span><br><span class="line"> desiredHealthy: 3</span><br><span class="line"> disruptedPods: null</span><br><span class="line"> disruptionsAllowed: 1</span><br><span class="line"> expectedPods: 3</span><br><span class="line"> observedGeneration: 1</span><br></pre></td></tr></table></figure><p><strong>官方参考文档</strong>：<a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/run-application/configure-pdb/</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在 Kub
      
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://chenzhonzhou.github.io/categories/kubernetes/"/>
    
    
      <category term="kubernetes" scheme="http://chenzhonzhou.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>K8s 中运行Zookeeper</title>
    <link href="http://chenzhonzhou.github.io/2022/06/28/k8s-zhong-yun-xing-zookeeper/"/>
    <id>http://chenzhonzhou.github.io/2022/06/28/k8s-zhong-yun-xing-zookeeper/</id>
    <published>2022-06-28T07:38:05.000Z</published>
    <updated>2022-06-28T09:11:09.705Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><h3 id="1、zk-的yaml资源文件"><a href="#1、zk-的yaml资源文件" class="headerlink" title="1、zk 的yaml资源文件"></a>1、zk 的yaml资源文件</h3><p>这里使用statefulSet的方式来部署 Zookeeper，它会创建一个ConfigMap，一个Service，一个Headless Service，一个PodDisruptionBudget，一个StatefulSet</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">zk-scripts</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    <span class="string">app.kubernetes.io/name:</span> <span class="string">zookeeper</span></span><br><span class="line">    <span class="string">app.kubernetes.io/component:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="string">setup.sh:</span> <span class="string">|-</span></span><br><span class="line">    <span class="comment">#!/bin/bash</span></span><br><span class="line">    <span class="string">if</span> <span class="string">[[</span> <span class="bullet">-f</span> <span class="string">"/bitnami/zookeeper/data/myid"</span> <span class="string">]];</span> <span class="string">then</span></span><br><span class="line">        <span class="string">export</span> <span class="string">ZOO_SERVER_ID="$(cat</span> <span class="string">/bitnami/zookeeper/data/myid)"</span></span><br><span class="line">    <span class="string">else</span></span><br><span class="line">        <span class="string">HOSTNAME="$(hostname</span> <span class="bullet">-s)"</span></span><br><span class="line">        <span class="string">if</span> <span class="string">[[</span> <span class="string">$HOSTNAME</span> <span class="string">=~</span> <span class="string">(.*)-([0-9]+)$</span> <span class="string">]];</span> <span class="string">then</span></span><br><span class="line">            <span class="string">ORD=$&#123;BASH_REMATCH[2]&#125;</span></span><br><span class="line">            <span class="string">export</span> <span class="string">ZOO_SERVER_ID="$((ORD</span> <span class="string">+</span> <span class="number">1</span> <span class="string">))"</span></span><br><span class="line">        <span class="string">else</span></span><br><span class="line">            <span class="string">echo</span> <span class="string">"Failed to get index from hostname $HOST"</span></span><br><span class="line">            <span class="string">exit</span> <span class="number">1</span></span><br><span class="line">        <span class="string">fi</span></span><br><span class="line">    <span class="string">fi</span></span><br><span class="line">    <span class="string">exec</span> <span class="string">/entrypoint.sh</span> <span class="string">/run.sh</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># 创建headless服务</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">zk-headless</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    <span class="string">app.kubernetes.io/name:</span> <span class="string">zookeeper</span></span><br><span class="line">    <span class="string">app.kubernetes.io/component:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">ClusterIP</span></span><br><span class="line"><span class="attr">  clusterIP:</span> <span class="string">None</span></span><br><span class="line"><span class="attr">  publishNotReadyAddresses:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">tcp-client</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">2181</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="string">client</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">tcp-follower</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">2888</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="string">follower</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">tcp-election</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">3888</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="string">election</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line">    <span class="string">app.kubernetes.io/name:</span> <span class="string">zookeeper</span></span><br><span class="line">    <span class="string">app.kubernetes.io/component:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># 创建service服务</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">zk</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    <span class="string">app.kubernetes.io/name:</span> <span class="string">zookeeper</span></span><br><span class="line">    <span class="string">app.kubernetes.io/component:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">ClusterIP</span></span><br><span class="line"><span class="attr">  sessionAffinity:</span> <span class="string">None</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">tcp-client</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">2181</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="string">client</span></span><br><span class="line"><span class="attr">      nodePort:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">tcp-follower</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">2888</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="string">follower</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">tcp-election</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">3888</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="string">election</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line">    <span class="string">app.kubernetes.io/name:</span> <span class="string">zookeeper</span></span><br><span class="line">    <span class="string">app.kubernetes.io/component:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># 创建PodDisruptionBudget 控制器</span></span><br><span class="line"><span class="comment"># Pod Disruption Budget (pod 中断 预算) 简称PDB，含义其实是终止pod前通过 labelSelector 机制获取正常运行的pod数目的限制，目的是对自愿中断的保护措施</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodDisruptionBudget</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">zookeeper-pod-disruption-budget</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line">      <span class="string">app.kubernetes.io/name:</span> <span class="string">zookeeper</span></span><br><span class="line">      <span class="string">app.kubernetes.io/component:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">  maxUnavailable:</span> <span class="number">1</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">zk</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    <span class="string">app.kubernetes.io/name:</span> <span class="string">zookeeper</span></span><br><span class="line">    <span class="string">app.kubernetes.io/component:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">    role:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  podManagementPolicy:</span> <span class="string">Parallel</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line">      <span class="string">app.kubernetes.io/name:</span> <span class="string">zookeeper</span></span><br><span class="line">      <span class="string">app.kubernetes.io/component:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">  serviceName:</span> <span class="string">zk-headless</span></span><br><span class="line"><span class="attr">  updateStrategy:</span></span><br><span class="line"><span class="attr">    rollingUpdate:</span> <span class="string">&#123;&#125;</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">RollingUpdate</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      annotations:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line">        <span class="string">app.kubernetes.io/name:</span> <span class="string">zookeeper</span></span><br><span class="line">        <span class="string">app.kubernetes.io/component:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        podAntiAffinity:</span></span><br><span class="line"><span class="attr">          preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">            - podAffinityTerm:</span></span><br><span class="line"><span class="attr">                labelSelector:</span></span><br><span class="line"><span class="attr">                  matchLabels:</span></span><br><span class="line">                    <span class="string">app.kubernetes.io/name:</span> <span class="string">zookeeper</span></span><br><span class="line">                    <span class="string">app.kubernetes.io/component:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">                namespaces:</span></span><br><span class="line"><span class="bullet">                  -</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">                topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line"><span class="attr">              weight:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">      securityContext:</span></span><br><span class="line"><span class="attr">        fsGroup:</span> <span class="number">1001</span></span><br><span class="line"><span class="attr">      initContainers:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">bitnami/zookeeper:3.8.0</span></span><br><span class="line"><span class="attr">          imagePullPolicy:</span> <span class="string">"IfNotPresent"</span></span><br><span class="line"><span class="attr">          securityContext:</span></span><br><span class="line"><span class="attr">            runAsNonRoot:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">            runAsUser:</span> <span class="number">1001</span></span><br><span class="line"><span class="attr">          command:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">/scripts/setup.sh</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line"><span class="attr">            limits:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">              memory:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">            requests:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="number">0.1</span></span><br><span class="line"><span class="attr">              memory:</span> <span class="number">128</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">BITNAMI_DEBUG</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"false"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_DATA_LOG_DIR</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_PORT_NUMBER</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"2181"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_TICK_TIME</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"2000"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_INIT_LIMIT</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"10"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_SYNC_LIMIT</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"5"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_PRE_ALLOC_SIZE</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"65536"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_SNAPCOUNT</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"100000"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_MAX_CLIENT_CNXNS</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"60"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_4LW_COMMANDS_WHITELIST</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"srvr, mntr, ruok"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_LISTEN_ALLIPS_ENABLED</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"no"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_AUTOPURGE_INTERVAL</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"0"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_AUTOPURGE_RETAIN_COUNT</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"3"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_MAX_SESSION_TIMEOUT</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"40000"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_SERVERS</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">zk-0.zk-headless.default.svc.cluster.local:2888:3888::1</span> <span class="string">zk-1.zk-headless.default.svc.cluster.local:2888:3888::2</span> <span class="string">zk-2.zk-headless.default.svc.cluster.local:2888:3888::3</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_ENABLE_AUTH</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"no"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_HEAP_SIZE</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"1024"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ZOO_LOG_LEVEL</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"ERROR"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">ALLOW_ANONYMOUS_LOGIN</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">"yes"</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">POD_NAME</span></span><br><span class="line"><span class="attr">              valueFrom:</span></span><br><span class="line"><span class="attr">                fieldRef:</span></span><br><span class="line"><span class="attr">                  apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">                  fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">client</span></span><br><span class="line"><span class="attr">              containerPort:</span> <span class="number">2181</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">follower</span></span><br><span class="line"><span class="attr">              containerPort:</span> <span class="number">2888</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">election</span></span><br><span class="line"><span class="attr">              containerPort:</span> <span class="number">3888</span></span><br><span class="line"><span class="attr">          livenessProbe:</span></span><br><span class="line"><span class="attr">            failureThreshold:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">            initialDelaySeconds:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">            periodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">            successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">            exec:</span></span><br><span class="line"><span class="attr">              command:</span> <span class="string">['/bin/bash',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok'</span><span class="string">]</span></span><br><span class="line"><span class="attr">          readinessProbe:</span></span><br><span class="line"><span class="attr">            failureThreshold:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">            initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">            periodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">            successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">            exec:</span></span><br><span class="line"><span class="attr">              command:</span> <span class="string">['/bin/bash',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok'</span><span class="string">]</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">scripts</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/scripts/setup.sh</span></span><br><span class="line"><span class="attr">              subPath:</span> <span class="string">setup.sh</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">zookeeper-data</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/bitnami/zookeeper</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">scripts</span></span><br><span class="line"><span class="attr">          configMap:</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">zk-scripts</span></span><br><span class="line"><span class="attr">            defaultMode:</span> <span class="number">0755</span></span><br><span class="line"><span class="attr">  volumeClaimTemplates:</span></span><br><span class="line"><span class="attr">  - metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">zookeeper-data</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">      accessModes:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">      resources:</span></span><br><span class="line"><span class="attr">        requests:</span></span><br><span class="line"><span class="attr">          storage:</span> <span class="number">2</span><span class="string">Gi</span></span><br></pre></td></tr></table></figure><blockquote><p>因为这里使用的镜像<code>bitnami/zookeeper</code>是别人经过调整的，介意的话可以使用k8s 官网的zk yaml部署 <a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/" target="_blank" rel="noopener">https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/</a></p></blockquote><h3 id="2、部署-zk"><a href="#2、部署-zk" class="headerlink" title="2、部署 zk"></a>2、部署 zk</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@001-new ~]$ kubectl apply -f zookeeper.yaml</span><br></pre></td></tr></table></figure><h3 id="3、查看-zk-服务状态"><a href="#3、查看-zk-服务状态" class="headerlink" title="3、查看 zk 服务状态"></a>3、查看 zk 服务状态</h3><p>查看资源清单</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@001-new ~]$ kubectl get po,svc,pdb,pvc|grep zk</span><br><span class="line">pod/zk-0                                      1/1     Running   0          72m</span><br><span class="line">pod/zk-1                                      1/1     Running   0          72m</span><br><span class="line">pod/zk-2                                      1/1     Running   1          72m</span><br><span class="line">service/zk                      ClusterIP   172.21.6.1      &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP   72m</span><br><span class="line">service/zk-headless             ClusterIP   None            &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP   72m</span><br><span class="line">persistentvolumeclaim/zookeeper-data-zk-0   Bound    pvc-beb9b6e9-f6b0-11ec-a9e9-d27996f947d5   2Gi        RWO            nfs   72m</span><br><span class="line">persistentvolumeclaim/zookeeper-data-zk-1   Bound    pvc-bebb5f58-f6b0-11ec-a9e9-d27996f947d5   2Gi        RWO            nfs   72m</span><br><span class="line">persistentvolumeclaim/zookeeper-data-zk-2   Bound    pvc-bebd09ee-f6b0-11ec-a9e9-d27996f947d5   2Gi        RWO            nfs   72m</span><br></pre></td></tr></table></figure><p>查看zk节点状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@001-new ~]$ <span class="keyword">for</span> i <span class="keyword">in</span> 0 1 2;<span class="keyword">do</span> kubectl <span class="built_in">exec</span> zk-<span class="variable">$i</span> -- /opt/bitnami/zookeeper/bin/zkServer.sh status;<span class="keyword">done</span></span><br><span class="line">/opt/bitnami/java/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/bitnami/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost. Client SSL: <span class="literal">false</span>.</span><br><span class="line">Mode: follower</span><br><span class="line">/opt/bitnami/java/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/bitnami/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost. Client SSL: <span class="literal">false</span>.</span><br><span class="line">Mode: leader</span><br><span class="line">/opt/bitnami/java/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/bitnami/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost. Client SSL: <span class="literal">false</span>.</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><p>让zk节点重新选举</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模拟当前的leader节点意外下线</span></span><br><span class="line">[root@001-new ~]$ kubectl <span class="built_in">exec</span> -it zk-1 bash</span><br><span class="line">kubectl <span class="built_in">exec</span> [POD] [COMMAND] is DEPRECATED and will be removed <span class="keyword">in</span> a future version. Use kubectl <span class="built_in">exec</span> [POD] -- [COMMAND] instead.</span><br><span class="line">I have no name!@zk-1:/$ /opt/bitnami/zookeeper/bin/zkServer.sh status</span><br><span class="line">/opt/bitnami/java/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/bitnami/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost. Client SSL: <span class="literal">false</span>.</span><br><span class="line">Mode: leader</span><br><span class="line">I have no name!@zk-1:/$ ps -ef</span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">1001         1     0  0 07:06 ?        00:00:06 java -Dzookeeper.log.dir=/opt/bitnami/zookeeper/logs -Dzookeeper.log.file=/opt/bitnami</span><br><span class="line">1001      8877     0  0 08:24 pts/0    00:00:00 bash</span><br><span class="line">1001      8944  8877  0 08:25 pts/0    00:00:00 ps -ef</span><br><span class="line">I have no name!@zk-1:/$ <span class="built_in">kill</span> 1</span><br><span class="line">I have no name!@zk-1:/$ <span class="built_in">command</span> terminated with <span class="built_in">exit</span> code 137</span><br><span class="line">[root@001-new ~]$</span><br><span class="line">[root@001-new ~]$ kubectl get po |grep zk</span><br><span class="line">zk-0                                      1/1     Running   0          79m</span><br><span class="line">zk-1                                      1/1     Running   1          79m</span><br><span class="line">zk-2                                      1/1     Running   1          79m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次查看zk节点状态</span></span><br><span class="line">[root@001-new ~]$ <span class="keyword">for</span> i <span class="keyword">in</span> 0 1 2;<span class="keyword">do</span> kubectl <span class="built_in">exec</span> zk-<span class="variable">$i</span> -- /opt/bitnami/zookeeper/bin/zkServer.sh status;<span class="keyword">done</span></span><br><span class="line">/opt/bitnami/java/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/bitnami/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost. Client SSL: <span class="literal">false</span>.</span><br><span class="line">Mode: follower</span><br><span class="line">/opt/bitnami/java/bin/java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/bitnami/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost. Client SSL: <span class="literal">false</span>.</span><br><span class="line">Mode: follower</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/bitnami/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">/opt/bitnami/java/bin/java</span><br><span class="line">Client port found: 2181. Client address: localhost. Client SSL: <span class="literal">false</span>.</span><br><span class="line">Mode: leader</span><br></pre></td></tr></table></figure><p>获取zk StatefulSet 中每个 Pod 的FQDN域名</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@001-new ~]$ <span class="keyword">for</span> i <span class="keyword">in</span> 0 1 2; <span class="keyword">do</span> kubectl <span class="built_in">exec</span> zk-<span class="variable">$i</span> -- hostname -f; <span class="keyword">done</span></span><br><span class="line">zk-0.zk-headless.default.svc.cluster.local</span><br><span class="line">zk-1.zk-headless.default.svc.cluster.local</span><br><span class="line">zk-2.zk-headless.default.svc.cluster.local</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;1、zk-的yaml资源文件&quot;&gt;&lt;a href=&quot;#1、zk-的yaml资源文件&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://chenzhonzhou.github.io/categories/kubernetes/"/>
    
    
      <category term="kubernetes" scheme="http://chenzhonzhou.github.io/tags/kubernetes/"/>
    
      <category term="Zookeeper" scheme="http://chenzhonzhou.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Gitlab 版本升级</title>
    <link href="http://chenzhonzhou.github.io/2022/04/08/gitlab-ban-ben-sheng-ji/"/>
    <id>http://chenzhonzhou.github.io/2022/04/08/gitlab-ban-ben-sheng-ji/</id>
    <published>2022-04-08T06:19:18.000Z</published>
    <updated>2022-06-22T09:38:26.081Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>由于gitlab使用的 13.5.3版本有 CVE-2021-22205 漏洞，需要升级到稳定版本13.8.8</p><h2 id="二、升级流程"><a href="#二、升级流程" class="headerlink" title="二、升级流程"></a>二、升级流程</h2><p>先查看<a href="https://docs.gitlab.com/ee/update/#upgrade-paths" target="_blank" rel="noopener">官网</a>的版本升级路线，13.5.3可以直接升级13.8.8</p><h3 id="2-1-备份原gitlab数据"><a href="#2-1-备份原gitlab数据" class="headerlink" title="2.1 备份原gitlab数据"></a>2.1 备份原gitlab数据</h3><p>升级前备份原gitlab文件</p><blockquote><p>/etc/gitlab/gitlab.rb 该文件配置了gitlab的域名、邮件发送信息、白名单等相关信息。<br>/etc/gitlab/gitlab-secrets.json 该文件存储了gitlab的db secret信息。如果丢失，那么使用双重身份验证的GitLab用户将无法访问GitLab服务器，而存储在GitLab中的 <strong><code>安全变量</code></strong>将被丢失。</p></blockquote><p>使用 gitlab-rake 备份gitlab数据，可忽略因为升级过程中会自动进行备份</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gitlab-rake gitlab:backup:create</span></span><br><span class="line"></span><br><span class="line">默认备份文件存储在/var/opt/gitlab/backups，若该目录下不存在，则需要查看/etc/gitlab/gitlab.rb配置中对应的gitlab_rails[<span class="string">'backup_path'</span>]选项所指定的目录。</span><br></pre></td></tr></table></figure><h3 id="2-2-部署新版本的gitlab服务"><a href="#2-2-部署新版本的gitlab服务" class="headerlink" title="2.2 部署新版本的gitlab服务"></a>2.2 部署新版本的gitlab服务</h3><p>关闭部分gitlab服务（不要全关，因为升级中可能需要部分服务）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gitlab-ctl stop unicorn</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> gitlab-ctl stop sidekiq</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> gitlab-ctl stop nginx</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">升级</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yum install -y gitlab-ee-13.8.8</span></span><br></pre></td></tr></table></figure><p>按升级路线版本进行部署，安装源可以使用<a href="https://packages.gitlab.com/gitlab/" target="_blank" rel="noopener">gitlab官方源</a></p><h3 id="2-3-重启gitlab服务"><a href="#2-3-重启gitlab服务" class="headerlink" title="2.3 重启gitlab服务"></a>2.3 重启gitlab服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">刷新配置</span><br><span class="line"><span class="meta">#</span><span class="bash"> gitlab-ctl reconfigure</span></span><br><span class="line"></span><br><span class="line">重启gitlab服务</span><br><span class="line"><span class="meta">#</span><span class="bash"> gitlab-ctl restart</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>如果跨多个大版本需要按版本升级路线中的版本，重复2.2、2.3步骤，升级到目标版本</strong></p></blockquote><h3 id="2-4-设置gitlab定时备份"><a href="#2-4-设置gitlab定时备份" class="headerlink" title="2.4 设置gitlab定时备份"></a>2.4 设置gitlab定时备份</h3><p>修改 <code>/etc/gitlab/gitlab.rb</code> 文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置备份存储位置</span></span><br><span class="line"><span class="string">gitlab_rails['backup_path']</span> <span class="string">=</span> <span class="string">"/mnt/udisk/gitlab/backups"</span></span><br><span class="line"><span class="comment"># 备份最近七天的数据，即 7*24*60*60 秒</span></span><br><span class="line"><span class="string">gitlab_rails['backup_keep_time']</span> <span class="string">=</span> <span class="number">604800</span></span><br></pre></td></tr></table></figure><p>保存后，刷新配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gitlab-ctl reconfigure</span><br></pre></td></tr></table></figure><p>编辑定时任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br></pre></td></tr></table></figure><p>加入以下内容后保存(每天中午 12 点和傍晚 18 点自动执行备份操作)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0 12 * * * /opt/gitlab/bin/gitlab-rake gitlab:backup:create SKIP=uploads,builds,artifacts,lfs,registry,pages</span><br></pre></td></tr></table></figure><p>保存后需要重新启动定时器，执行如下语句</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重启定时任务</span></span><br><span class="line">systemctl restart crond.service</span><br><span class="line"><span class="comment"># 重装定时任务</span></span><br><span class="line">systemctl reload crond.service</span><br><span class="line"><span class="comment"># 查看定时任务是否开启（可选）</span></span><br><span class="line">systemctl is-enabled crond.service</span><br><span class="line"><span class="comment"># 查看所有用户下的定时任务（可选）</span></span><br><span class="line">cat /etc/passwd | cut -f 1 -d : |xargs -I &#123;&#125; crontab -l -u &#123;&#125;</span><br></pre></td></tr></table></figure><h2 id="三、迁移流程"><a href="#三、迁移流程" class="headerlink" title="三、迁移流程"></a>三、迁移流程</h2><p>迁移并非是升级gitlab服务的版本，而是将gitlab从一台服务器迁移到另一台服务器的过程，并且要保持迁移前后gitlab的版本一致否则在还原数据的时候会报错。</p><h3 id="3-1-备份原gitlab数据"><a href="#3-1-备份原gitlab数据" class="headerlink" title="3.1 备份原gitlab数据"></a>3.1 备份原gitlab数据</h3><p>迁移前备份原gitlab文件</p><blockquote><p>/etc/gitlab/gitlab.rb 该文件配置了gitlab的域名、邮件发送信息、白名单等相关信息。<br>/etc/gitlab/gitlab-secrets.json 该文件存储了gitlab的db secret信息。如果丢失，那么使用双重身份验证的GitLab用户将无法访问GitLab服务器，而存储在GitLab中的 <strong><code>安全变量</code></strong>将被丢失。</p></blockquote><p>使用 gitlab-rake 备份gitlab数据</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gitlab-rake gitlab:backup:create</span></span><br></pre></td></tr></table></figure><h3 id="3-2-部署新版本的gitlab服务"><a href="#3-2-部署新版本的gitlab服务" class="headerlink" title="3.2 部署新版本的gitlab服务"></a>3.2 部署新版本的gitlab服务</h3><p>安装源可以使用<a href="https://packages.gitlab.com/gitlab/" target="_blank" rel="noopener">gitlab官方源</a></p><h3 id="3-3-还原gitlab数据"><a href="#3-3-还原gitlab数据" class="headerlink" title="3.3 还原gitlab数据"></a>3.3 还原gitlab数据</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">还原gitlab数据</span><br><span class="line"><span class="meta">#</span><span class="bash"> gitlab-rake gitlab:backup:restore BACKUP=1649181808_2022_04_06_13.5.3-ee</span></span><br><span class="line"></span><br><span class="line">将备份的gitlab-secrets.json、gitlab.rb文件拷贝回/etc/gitlab/ 目录下</span><br><span class="line"></span><br><span class="line">刷新配置</span><br><span class="line"><span class="meta">#</span><span class="bash"> gitlab-ctl reconfigure</span></span><br><span class="line"></span><br><span class="line">重启gitlab服务</span><br><span class="line"><span class="meta">#</span><span class="bash"> gitlab-ctl restart</span></span><br></pre></td></tr></table></figure><p>至此gitlab迁移就已完成，jenkins进行CI/CD的时候可能会出现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Host key verification failed.</span><br><span class="line">fatal: Could not read from remote repository.</span><br></pre></td></tr></table></figure><p>需要手动克隆一下代码，然后在gitlab对应项目的 Settings –&gt; Repository –&gt; Deploy Keys 中允许jenkins服务器，只需要手动克隆一个项目即可。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;一、简介&quot;&gt;&lt;a href=&quot;#一、简介&quot; class=&quot;headerlink&quot; title=&quot;一、简介&quot;&gt;&lt;/a&gt;一、简介&lt;/h2&gt;
      
    
    </summary>
    
    
      <category term="版本库服务" scheme="http://chenzhonzhou.github.io/categories/%E7%89%88%E6%9C%AC%E5%BA%93%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="Gitlab" scheme="http://chenzhonzhou.github.io/categories/%E7%89%88%E6%9C%AC%E5%BA%93%E6%9C%8D%E5%8A%A1/Gitlab/"/>
    
    
      <category term="Gitlab" scheme="http://chenzhonzhou.github.io/tags/Gitlab/"/>
    
      <category term="版本库服务" scheme="http://chenzhonzhou.github.io/tags/%E7%89%88%E6%9C%AC%E5%BA%93%E6%9C%8D%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>Linux 升级内核版本</title>
    <link href="http://chenzhonzhou.github.io/2022/03/09/linux-sheng-ji-nei-he-ban-ben/"/>
    <id>http://chenzhonzhou.github.io/2022/03/09/linux-sheng-ji-nei-he-ban-ben/</id>
    <published>2022-03-09T08:33:11.000Z</published>
    <updated>2022-03-16T10:08:44.873Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><h3 id="小版本升级"><a href="#小版本升级" class="headerlink" title="小版本升级"></a>小版本升级</h3><p>5.10.75 升级到 5.10.96，这里以AWS服务器为例</p><p>查看当前内核版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> uname -sr</span></span><br></pre></td></tr></table></figure><p>查看可以升级的内核版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum list kernel -q</span></span><br><span class="line">Installed Packages</span><br><span class="line">kernel.x86_64       5.10.75-79.358.amzn2        installed</span><br><span class="line">Available Packages</span><br><span class="line">kernel.x86_64       5.10.96-90.460.amzn2        @amzn2extra-kernel-5.10</span><br></pre></td></tr></table></figure><p>安装新版本内核</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install kernel-5.10.96-90.460.amzn2</span></span><br></pre></td></tr></table></figure><p>查看系统可用内核，并设置启动顺序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> rpm -q kernel</span></span><br><span class="line"></span><br><span class="line">查看启动顺序</span><br><span class="line"><span class="meta">#</span><span class="bash"> awk -F\<span class="string">' '</span><span class="variable">$1</span>==<span class="string">"menuentry "</span> &#123;<span class="built_in">print</span> i++ <span class="string">" : "</span> <span class="variable">$2</span>&#125;<span class="string">' /boot/grub2/grub.cfg</span></span></span><br><span class="line">0 : Amazon Linux (5.10.96-90.460.amzn2.x86_64) 2</span><br><span class="line">1 : Amazon Linux (5.10.75-79.358.amzn2.x86_64) 2</span><br></pre></td></tr></table></figure><p>已经自动将新内核设置为默认启动项了</p><p>重启系统生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> reboot</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>手动设置内核启动项，例如设置 1为默认启动项</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install grub2-pc</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> grub2-set-default 1</span></span><br></pre></td></tr></table></figure><p>生成 grub 配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> grub2-mkconfig -o /boot/grub2/grub.cfg </span></span><br></pre></td></tr></table></figure><p>重启系统</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> reboot</span></span><br></pre></td></tr></table></figure></blockquote><h3 id="大版本升级"><a href="#大版本升级" class="headerlink" title="大版本升级"></a>大版本升级</h3><p>4.x 升级到 5.x</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">查看当前内核版本</span><br><span class="line"><span class="meta">#</span><span class="bash"> uname -sr</span></span><br><span class="line">Linux 4.14.248-189.473.el7.x86_64</span><br><span class="line"></span><br><span class="line">查看操作系统版本</span><br><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/redhat-release</span></span><br><span class="line">CentOS Linux release 7.9.2009 (Core)</span><br><span class="line"></span><br><span class="line">导入 ELRepo 仓库的公共密钥</span><br><span class="line"><span class="meta">#</span><span class="bash"> rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org</span></span><br><span class="line"></span><br><span class="line">安装 ELRepo 仓库的 yum 源</span><br><span class="line"><span class="meta">#</span><span class="bash"> yum install -y https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm</span></span><br><span class="line"></span><br><span class="line">替换为清华 ELRepo 源</span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">"s/mirrorlist=/#mirrorlist=/g"</span> /etc/yum.repos.d/elrepo.repo</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">"s#elrepo.org/linux#mirrors.tuna.tsinghua.edu.cn/elrepo#g"</span> /etc/yum.repos.d/elrepo.repo</span></span><br><span class="line"></span><br><span class="line">更新 yum 缓存</span><br><span class="line"><span class="meta">#</span><span class="bash"> yum makecache</span></span><br><span class="line"></span><br><span class="line">查看可用的内核版本，kernel-ml（mainline stable）：稳定主线版本，kernel-lt（long term support）：长期支持版本</span><br><span class="line"><span class="meta">#</span><span class="bash"> yum --disablerepo=<span class="string">"*"</span> --enablerepo=<span class="string">"elrepo-kernel"</span> list available</span></span><br><span class="line">已加载插件：fastestmirror, langpacks</span><br><span class="line">Repository base is listed more than once in the configuration</span><br><span class="line">Repository updates is listed more than once in the configuration</span><br><span class="line">Repository extras is listed more than once in the configuration</span><br><span class="line">Repository centosplus is listed more than once in the configuration</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line"> * elrepo-kernel: linux-mirrors.fnal.gov</span><br><span class="line">可安装的软件包</span><br><span class="line">kernel-lt.x86_64                           5.4.127-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-lt-devel.x86_64                     5.4.127-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-lt-doc.noarch                       5.4.127-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-lt-headers.x86_64                   5.4.127-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-lt-tools.x86_64                     5.4.127-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-lt-tools-libs.x86_64                5.4.127-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-lt-tools-libs-devel.x86_64          5.4.127-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-ml.x86_64                           5.12.12-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-ml-devel.x86_64                     5.12.12-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-ml-doc.noarch                       5.12.12-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-ml-headers.x86_64                   5.12.12-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-ml-tools.x86_64                     5.12.12-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-ml-tools-libs.x86_64                5.12.12-1.el7.elrepo          elrepo-kernel</span><br><span class="line">kernel-ml-tools-libs-devel.x86_64          5.12.12-1.el7.elrepo          elrepo-kernel</span><br><span class="line">perf.x86_64                                5.12.12-1.el7.elrepo          elrepo-kernel</span><br><span class="line">python-perf.x86_64                         5.12.12-1.el7.elrepo          elrepo-kernel</span><br><span class="line"></span><br><span class="line">升级为主线版本</span><br><span class="line"><span class="meta">#</span><span class="bash"> yum --enablerepo=elrepo-kernel install kernel-ml -y</span></span><br></pre></td></tr></table></figure><p>配置开机加载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">查看可用内核版本及启动顺序</span><br><span class="line"><span class="meta">#</span><span class="bash"> awk -F\<span class="string">' '</span><span class="variable">$1</span>==<span class="string">"menuentry "</span> &#123;<span class="built_in">print</span> i++ <span class="string">" : "</span> <span class="variable">$2</span>&#125;<span class="string">' /boot/grub2/grub.cfg</span></span></span><br><span class="line"></span><br><span class="line">查看启动顺序</span><br><span class="line"><span class="meta">#</span><span class="bash"> yum install grub2-pc</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> grub2-editenv list</span></span><br><span class="line"></span><br><span class="line">设置开机启动</span><br><span class="line"><span class="meta">#</span><span class="bash"> grub2-set-default 0</span></span><br><span class="line">或者</span><br><span class="line"><span class="meta">#</span><span class="bash"> grub2-mkconfig -o /boot/grub2/grub.cfg</span></span><br><span class="line"></span><br><span class="line">重启生效</span><br><span class="line"><span class="meta">#</span><span class="bash"> reboot</span></span><br><span class="line"></span><br><span class="line">重启后查看内核版本</span><br><span class="line"><span class="meta">#</span><span class="bash"> uname -sr</span></span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;小版本升级&quot;&gt;&lt;a href=&quot;#小版本升级&quot; class=&quot;headerlink&quot; title=&quot;小版本升级&quot;&gt;&lt;/a&gt;小版本升级&lt;
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://chenzhonzhou.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://chenzhonzhou.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>k8s Helm3部署和使用</title>
    <link href="http://chenzhonzhou.github.io/2022/02/16/k8s-helm-bu-shu-he-shi-yong/"/>
    <id>http://chenzhonzhou.github.io/2022/02/16/k8s-helm-bu-shu-he-shi-yong/</id>
    <published>2022-02-16T06:43:49.000Z</published>
    <updated>2022-03-22T07:00:32.149Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --><h2 id="一、Helm-介绍"><a href="#一、Helm-介绍" class="headerlink" title="一、Helm 介绍"></a>一、Helm 介绍</h2><p>没有使用Helm之前，在Kubernetes部署应用，我们要依次部署deployment、service等，步骤比较繁琐。况且随着很多项目微服务化，复杂的应用在容器中部署以及管理显得较为复杂。</p><blockquote><p>helm通过打包的方式，支持发布的版本管理和控制，很大程度上简化了Kubernetes应用的部署和管理。</p></blockquote><p>Helm本质就是让k8s的应用管理（Deployment、Service等）可配置，能动态生成。通过动态生成K8S资源清单文件（deployment.yaml、service.yaml）。然后kubectl自动调用K8S资源部署。</p><blockquote><p>Helm是官方提供类似于YUM的包管理，是部署环境的流程封装，Helm有三个重要的概念：chart、release和Repository</p></blockquote><ul><li>chart是创建一个应用的信息集合，包括各种Kubernetes对象的配置模板、参数定义、依赖关系、文档说明等。可以将chart想象成apt、yum中的软件安装包。</li><li>release是chart的运行实例，代表一个正在运行的应用。当chart被安装到Kubernetes集群，就生成一个release。chart能多次安装到同一个集群，每次安装都是一个release【根据chart赋值不同，完全可以部署出多个release出来】。</li><li>Repository用于发布和存储 Chart 的存储库。</li></ul><blockquote><p><strong>Helm包含两个组件：Helm客户端和Tiller服务端，如下图所示：</strong></p></blockquote><p><img src="/2022/02/16/k8s-helm-bu-shu-he-shi-yong/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><p><strong>Helm</strong> 客户端 负责 chart 和 release 的创建和管理以及和 Tiller 的交互。<br><strong>Tiller</strong> 服务端 运行在 Kubernetes 集群中，它会处理Helm客户端的请求，与 Kubernetes API Server 交互（在heml3中已弃用 Tiller）。<br><strong>KubeAPI</strong> 将数据、资源得生成写入到 etcd ，被 kubelet 接受并创建。</p><h2 id="二、Helm-部署"><a href="#二、Helm-部署" class="headerlink" title="二、Helm 部署"></a>二、Helm 部署</h2><p>Helm由客户端命令helm工具和服务端tiller组成。</p><p>部署前可以前往help官网查看 <a href="https://helm.sh/zh/docs/topics/version_skew/" target="_blank" rel="noopener">Helm支持的 <strong>Kubernetes</strong> 版本</a> （不推荐将Helm用于比编译它所依赖的版本更高的Kubernetes版本，因为Helm并没有做出任何向前兼容的保证）。</p><h3 id="2-1-安装-helm"><a href="#2-1-安装-helm" class="headerlink" title="2.1 安装 helm"></a>2.1 安装 helm</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@czz ~]# wget https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz</span><br><span class="line">[root@czz ~]# tar xf helm-v3.8.1-linux-amd64.tar.gz</span><br><span class="line">[root@czz ~]# mv linux-amd64/helm /usr/local/bin/helm</span><br></pre></td></tr></table></figure><p>或使用脚本安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3</span><br><span class="line">chmod 700 get_helm.sh</span><br><span class="line">./get_helm.sh</span><br></pre></td></tr></table></figure><h3 id="2-2-验证-helm"><a href="#2-2-验证-helm" class="headerlink" title="2.2 验证 helm"></a>2.2 验证 helm</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@czz ~]# helm version</span><br><span class="line">version.BuildInfo&#123;Version:"v3.8.1", GitCommit:"5cb9af4b1b271d11d7a97a71df3ac337dd94ad37", GitTreeState:"clean", GoVersion:"go1.17.5"&#125;</span><br></pre></td></tr></table></figure><h2 id="三、使用-helm-部署应用"><a href="#三、使用-helm-部署应用" class="headerlink" title="三、使用 helm 部署应用"></a>三、使用 helm 部署应用</h2><h3 id="3-1-使用-helm-源部署"><a href="#3-1-使用-helm-源部署" class="headerlink" title="3.1 使用 helm 源部署"></a>3.1 使用 helm 源部署</h3><h4 id="3-1-添加-chart-源"><a href="#3-1-添加-chart-源" class="headerlink" title="3.1 添加 chart 源"></a>3.1 添加 chart 源</h4><p>准备好 helm 后，需要添加 helm 源数据仓库。有以下几个常见的源库</p><blockquote><p><a href="https://kubernetes-charts.storage.googleapis.com/" target="_blank" rel="noopener">https://kubernetes-charts.storage.googleapis.com/</a> helm官网 chart 库，稳定<br><a href="https://apphub.aliyuncs.com" target="_blank" rel="noopener">https://apphub.aliyuncs.com</a> 阿里云chart 库，速度最快</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@czz ~]# helm repo add aliyun https://apphub.aliyuncs.com</span><br><span class="line">"aliyun" has been added to your repositories</span><br></pre></td></tr></table></figure><h4 id="3-2-安装一个-Helm-应用"><a href="#3-2-安装一个-Helm-应用" class="headerlink" title="3.2 安装一个 Helm 应用"></a>3.2 安装一个 Helm 应用</h4><p><strong>查看想要安装的服务</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@czz ~]# helm search repo nginx</span><br><span class="line">NAME                           CHART VERSIONAPP VERSION         DESCRIPTION</span><br><span class="line">aliyun/nginx                   5.1.5        1.16.1              Chart for the nginx server</span><br><span class="line">aliyun/nginx-ingress           1.30.3       0.28.0              An nginx Ingress controller that uses ConfigMap...</span><br><span class="line">aliyun/nginx-ingress-controller5.3.4        0.29.0              Chart for the nginx Ingress controller</span><br><span class="line">aliyun/nginx-lego              0.3.1                            Chart for nginx-ingress-controller and kube-lego</span><br><span class="line">aliyun/nginx-php               1.0.0        nginx-1.10.3_php-7.0Chart for the nginx php server</span><br></pre></td></tr></table></figure><p><strong>使用helm安装服务</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@czz ~]# helm install nginx aliyun/nginx</span><br><span class="line"></span><br><span class="line">[root@czz ~]# kubectl get pod,svc</span><br><span class="line">NAME                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/nginx-99fcdd97b-g5xxz   1/1     Running   0          15m</span><br><span class="line"></span><br><span class="line">NAME                 TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)                      AGE</span><br><span class="line">service/kubernetes   ClusterIP      10.10.0.1     &lt;none&gt;        443/TCP                      3d16h</span><br><span class="line">service/nginx        LoadBalancer   10.10.18.28   &lt;pending&gt;     80:30418/TCP,443:32580/TCP   15m</span><br></pre></td></tr></table></figure><p><strong>使用 helm status 跟踪发布状态或重新读取配置信息</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@czz ~]# helm status nginx</span><br><span class="line">NAME: nginx</span><br><span class="line">LAST DEPLOYED: Tue Mar 22 10:47:59 2022</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: deployed</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br><span class="line">NOTES:</span><br><span class="line">Get the NGINX URL:</span><br><span class="line"></span><br><span class="line">  NOTE: It may take a few minutes for the LoadBalancer IP to be available.</span><br><span class="line">        Watch the status with: 'kubectl get svc --namespace default -w nginx'</span><br><span class="line"></span><br><span class="line">  export SERVICE_IP=$(kubectl get svc --namespace default nginx --template "&#123;&#123; range (index .status.loadBalancer.ingress 0) &#125;&#125;&#123;&#123;.&#125;&#125;&#123;&#123;end &#125;&#125;")</span><br><span class="line">  echo "NGINX URL: http://$SERVICE_IP/"</span><br></pre></td></tr></table></figure><h4 id="3-3-升级一个-helm-应用"><a href="#3-3-升级一个-helm-应用" class="headerlink" title="3.3 升级一个 helm 应用"></a>3.3 升级一个 helm 应用</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@czz ~]# helm upgrade nginx aliyun/nginx</span><br><span class="line"></span><br><span class="line">也可以指定命名空间和它的taz包</span><br><span class="line">[root@czz ~]# helm upgrade --install --force hello-world ./hello.tgz --namespace default</span><br></pre></td></tr></table></figure><h4 id="3-4-回滚一个-Helm-应用"><a href="#3-4-回滚一个-Helm-应用" class="headerlink" title="3.4 回滚一个 Helm 应用"></a>3.4 回滚一个 Helm 应用</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">向上归滚一个版本</span><br><span class="line">[root@czz ~]# helm rollback nginx 1</span><br></pre></td></tr></table></figure><h4 id="3-5-卸载一个-Helm-应用"><a href="#3-5-卸载一个-Helm-应用" class="headerlink" title="3.5 卸载一个 Helm 应用"></a>3.5 卸载一个 Helm 应用</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@czz ~]# helm uninstall nginx -ndefault</span><br></pre></td></tr></table></figure><h3 id="3-2-使用-helm-构建应用"><a href="#3-2-使用-helm-构建应用" class="headerlink" title="3.2 使用 helm 构建应用"></a>3.2 使用 helm 构建应用</h3><h4 id="3-2-1-建立一个-helm-charts"><a href="#3-2-1-建立一个-helm-charts" class="headerlink" title="3.2.1 建立一个 helm charts"></a>3.2.1 建立一个 helm charts</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@czz ~]# helm create hello-world</span><br><span class="line"></span><br><span class="line">[root@czz ~]# ls hello-world/</span><br><span class="line">charts  Chart.yaml  templates  values.yaml</span><br></pre></td></tr></table></figure><ul><li>Chart.yaml 用于描述这个Chart的相关信息，包括名字、描述信息以及版本等。<br>仅仅是一些简单的文本描述</li><li>values.yaml 用于存储 templates 目录中模板文件中用到变量的值。</li><li>NOTES.txt 用于介绍 Chart 部署后的一些信息，例如：如何使用这个 Chart、列出缺省的设置等。</li><li>Templates 目录下是 YAML 文件的模板，该模板文件遵循 Go template 语法。</li></ul><blockquote><p>Templates 目录下 YAML 文件模板的值默认都是在 values.yaml 里定义的，比如在 deployment.yaml 中定义的容器镜像。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image: &quot;&#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag &#125;&#125;&quot;</span><br></pre></td></tr></table></figure><p>其中的 .Values.image.repository 的值就是在 <strong>values.yaml</strong> 里定义的 nginx，.Values.image.tag 的值就是 stable。<br>以上两个变量值是在 create chart 的时候就自动生成的默认值，你可以根据实际情况进行修改。实际上都是静态文本,只在是执行的时候才被解析.</p></blockquote><h4 id="3-2-2-构建一个-helm-应用"><a href="#3-2-2-构建一个-helm-应用" class="headerlink" title="3.2.2 构建一个 helm 应用"></a>3.2.2 构建一个 helm 应用</h4><p>打开 Chart.yaml，可以看到内容如下，配置名称和版本</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@czz</span> <span class="string">~]#</span>  <span class="string">cat</span> <span class="string">hello-world/Chart.yaml</span> <span class="string">|grep</span> <span class="bullet">-Ev</span> <span class="string">"^#|^$"</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v2</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">hello-world</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">A</span> <span class="string">Helm</span> <span class="string">chart</span> <span class="string">for</span> <span class="string">Kubernetes</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">application</span></span><br><span class="line"><span class="attr">version:</span> <span class="number">0.1</span><span class="number">.0</span></span><br><span class="line"><span class="attr">appVersion:</span> <span class="string">"1.16.0"</span></span><br></pre></td></tr></table></figure><p>编辑 values.yaml，它默认会在 Kubernetes 部署一个 Nginx。下面是 mychart 应用的 values.yaml 文件的内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@czz</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">hello-world/values.yaml</span></span><br><span class="line"><span class="comment"># Default values for hello-world.</span></span><br><span class="line"><span class="comment"># This is a YAML-formatted file.</span></span><br><span class="line"><span class="comment"># Declare variables to be passed into your templates.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">replicaCount:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">image:</span></span><br><span class="line"><span class="attr">  repository:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  pullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="comment"># Overrides the image tag whose default is the chart appVersion.</span></span><br><span class="line"><span class="attr">  tag:</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="attr">imagePullSecrets:</span> <span class="string">[]</span></span><br><span class="line"><span class="attr">nameOverride:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">fullnameOverride:</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="attr">serviceAccount:</span></span><br><span class="line">  <span class="comment"># Specifies whether a service account should be created</span></span><br><span class="line"><span class="attr">  create:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Annotations to add to the service account</span></span><br><span class="line"><span class="attr">  annotations:</span> <span class="string">&#123;&#125;</span></span><br><span class="line">  <span class="comment"># The name of the service account to use.</span></span><br><span class="line">  <span class="comment"># If not set and create is true, a name is generated using the fullname template</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="attr">podAnnotations:</span> <span class="string">&#123;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">podSecurityContext:</span> <span class="string">&#123;&#125;</span></span><br><span class="line">  <span class="comment"># fsGroup: 2000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">securityContext:</span> <span class="string">&#123;&#125;</span></span><br><span class="line">  <span class="comment"># capabilities:</span></span><br><span class="line">  <span class="comment">#   drop:</span></span><br><span class="line">  <span class="comment">#   - ALL</span></span><br><span class="line">  <span class="comment"># readOnlyRootFilesystem: true</span></span><br><span class="line">  <span class="comment"># runAsNonRoot: true</span></span><br><span class="line">  <span class="comment"># runAsUser: 1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">service:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">ClusterIP</span></span><br><span class="line"><span class="attr">  port:</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ingress:</span></span><br><span class="line"><span class="attr">  enabled:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  className:</span> <span class="string">""</span></span><br><span class="line"><span class="attr">  annotations:</span> <span class="string">&#123;&#125;</span></span><br><span class="line">    <span class="comment"># kubernetes.io/ingress.class: nginx</span></span><br><span class="line">    <span class="comment"># kubernetes.io/tls-acme: "true"</span></span><br><span class="line"><span class="attr">  hosts:</span></span><br><span class="line"><span class="attr">    - host:</span> <span class="string">chart-example.local</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">        - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">          pathType:</span> <span class="string">ImplementationSpecific</span></span><br><span class="line"><span class="attr">  tls:</span> <span class="string">[]</span></span><br><span class="line">  <span class="comment">#  - secretName: chart-example-tls</span></span><br><span class="line">  <span class="comment">#    hosts:</span></span><br><span class="line">  <span class="comment">#      - chart-example.local</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span> <span class="string">&#123;&#125;</span></span><br><span class="line">  <span class="comment"># We usually recommend not to specify default resources and to leave this as a conscious</span></span><br><span class="line">  <span class="comment"># choice for the user. This also increases chances charts run on environments with little</span></span><br><span class="line">  <span class="comment"># resources, such as Minikube. If you do want to specify resources, uncomment the following</span></span><br><span class="line">  <span class="comment"># lines, adjust them as necessary, and remove the curly braces after 'resources:'.</span></span><br><span class="line">  <span class="comment"># limits:</span></span><br><span class="line">  <span class="comment">#   cpu: 100m</span></span><br><span class="line">  <span class="comment">#   memory: 128Mi</span></span><br><span class="line">  <span class="comment"># requests:</span></span><br><span class="line">  <span class="comment">#   cpu: 100m</span></span><br><span class="line">  <span class="comment">#   memory: 128Mi</span></span><br><span class="line"></span><br><span class="line"><span class="attr">autoscaling:</span></span><br><span class="line"><span class="attr">  enabled:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  minReplicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  maxReplicas:</span> <span class="number">100</span></span><br><span class="line"><span class="attr">  targetCPUUtilizationPercentage:</span> <span class="number">80</span></span><br><span class="line">  <span class="comment"># targetMemoryUtilizationPercentage: 80</span></span><br><span class="line"></span><br><span class="line"><span class="attr">nodeSelector:</span> <span class="string">&#123;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">tolerations:</span> <span class="string">[]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">affinity:</span> <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><h4 id="3-2-3-检查模块配置"><a href="#3-2-3-检查模块配置" class="headerlink" title="3.2.3 检查模块配置"></a>3.2.3 检查模块配置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@czz ~]# helm lint hello-world/</span><br></pre></td></tr></table></figure><h4 id="3-2-4-部署-helm-本地应用"><a href="#3-2-4-部署-helm-本地应用" class="headerlink" title="3.2.4 部署 helm 本地应用"></a>3.2.4 部署 helm 本地应用</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@czz ~]# helm install hello ./hello-world</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;一、Helm-介绍&quot;&gt;&lt;a href=&quot;#一、Helm-介绍&quot; class=&quot;headerlink&quot; title=&quot;一、Helm 介绍
      
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://chenzhonzhou.github.io/categories/kubernetes/"/>
    
    
      <category term="kubernetes - Helm" scheme="http://chenzhonzhou.github.io/tags/kubernetes-Helm/"/>
    
  </entry>
  
  <entry>
    <title>Alluxio 分布式内存虚拟文件系统</title>
    <link href="http://chenzhonzhou.github.io/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/"/>
    <id>http://chenzhonzhou.github.io/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/</id>
    <published>2021-12-11T06:56:08.000Z</published>
    <updated>2021-12-14T03:39:37.959Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><h2 id="一、Alluxio概览"><a href="#一、Alluxio概览" class="headerlink" title="一、Alluxio概览"></a>一、Alluxio概览</h2><h3 id="1-1-什么是-Alluxio"><a href="#1-1-什么是-Alluxio" class="headerlink" title="1.1 什么是 Alluxio"></a>1.1 什么是 Alluxio</h3><p><strong><a href="https://docs.alluxio.io/os/user/stable/cn/Overview.html" target="_blank" rel="noopener">Alluxio</a>（之前名为 Tachyon）是一个开源的具有内存级速度的虚拟分布式存储系统。</strong>它统一了数据访问的方式，为上层计算框架和底层存储系统构建了桥梁。应用只需要连接Alluxio即可访问存储在底层任意存储系统中的数据。此外，Alluxio的以内存为中心的架构使得数据的访问速度能比现有常规方案快几个数量级。</p><p>在大数据生态系统中，Alluxio 位于数据驱动框架或应用（如 Apache Spark、Presto、Tensorflow、Apache HBase、Apache Hive 或 Apache Flink）和各种持久化存储系统（如 Amazon S3、Google Cloud Storage、OpenStack Swift、HDFS、GlusterFS、IBM Cleversafe、EMC ECS、Ceph、NFS 、Minio和 Alibaba OSS）之间。 Alluxio 统一了存储在这些不同存储系统中的数据，为其上层数据驱动型应用提供统一的客户端 API 和全局命名空间。用户可以以独立集群方式(如Amazon EC2)运行Alluxio，也可以从Apache Mesos或Apache YARN上启动Alluxio。</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><blockquote><p>Alluxio作为一个内存级的虚拟分布式存储系统有几个常见的使用场景：</p><ul><li>计算层需要反复访问远程（比如在云端，或跨机房）的数据；</li><li>计算层需要同时访问多个独立的持久化数据源（比如同时访问S3和HDFS中的数据）；</li><li>多个独立的大数据应用（比如不同的Spark Job）需要高速有效的共享数据；</li><li>当计算层有着较为严重的内存资源、以及JVM GC压力，或者较高的任务失败率时，Alluxio作为输入输出数据的Off heap存储可以极大缓解这一压力，并使计算消耗的时间和资源更可控可预测。</li></ul><blockquote><p>Alluixo利用的是堆内内存，如果不符合典型场景，用起来就是鸡肋！<br>Ignite是堆外内存，真正加速任务</p></blockquote><p>以上引用原文：<a href="https://www.cnblogs.com/liugh/articles/7324488.html" target="_blank" rel="noopener">https://www.cnblogs.com/liugh/articles/7324488.html</a></p></blockquote><h3 id="1-2-Alluxio的优势"><a href="#1-2-Alluxio的优势" class="headerlink" title="1.2 Alluxio的优势"></a>1.2 Alluxio的优势</h3><p>通过简化应用程序访问其数据的方式（无论数据是什么格式或位置），Alluxio能够帮助克服从数据中提取信息所面临的困难。Alluxio的优势包括：</p><h4 id="1-2-1-内存速度-I-O"><a href="#1-2-1-内存速度-I-O" class="headerlink" title="1.2.1 内存速度 I/O"></a>1.2.1 内存速度 I/O</h4><p>Alluxio 能够用作分布式共享缓存服务，这样与 Alluxio 通信的计算应用程序可以透明地缓存频繁访问的数据（尤其是从远程位置），以提供内存级 I/O 吞吐率。此外，Alluxio的层次化存储机制能够充分利用内存、固态硬盘或者磁盘，降低具有弹性扩张特性的数据驱动型应用的成本开销。</p><h4 id="1-2-2-统一数据访问接口"><a href="#1-2-2-统一数据访问接口" class="headerlink" title="1.2.2 统一数据访问接口"></a>1.2.2 统一数据访问接口</h4><p>Alluxio 能够屏蔽底层持久化存储系统在API、客户端及版本方面的差异，从而使整个系统易于扩展和管理。</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%873.png" alt="图片3"></p><ul><li>能够将多个数据源中的数据挂载到Alluxio中</li><li>多个数据源使用统一的命名空间</li><li>用户使用统一的路径访问</li></ul><h4 id="1-2-3-提升远程存储读写性能"><a href="#1-2-3-提升远程存储读写性能" class="headerlink" title="1.2.3 提升远程存储读写性能"></a>1.2.3 提升远程存储读写性能</h4><p>以Hadoop为代表的存储计算紧耦合的传统架构具有优良的计算本地性。通过在邻近所需数据的节点上来部署运行计算任务，可以尽量减少通过网络传输数据，从而有效地提升性能。然而，维持这种紧耦合结构所需要付出的成本代价正逐渐让性能优势带来的意义变得微乎其微。</p><p>Alluxio通过在当前主流的存储计算分离解耦的架构中，提供与紧耦合架构相似甚至更优的性能，来解决解耦后性能降低的难题。推荐把Alluxio与集群的计算框架并置部署（co-locate），从而能够提供靠近计算的跨存储缓存来实现高效本地性。</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%874.png" alt="图片4">与传统的架构方案相比，用户需要注意采用Alluxio架构带来的两个关键区别。</p><blockquote><ol><li>Alluxio存储中<strong>不需要保存底层存储中的所有数据</strong>，它只需要保存工作集（WorkingSet）。即使全体数据的规模非常大，Alluxio也不需要大量存储空间来存储所有数据，而是可以在有限的存储空间中只缓存作业所需要的数据。</li><li>Alluxio<strong>存储采用了一种弹性的缓存机制来管理</strong>、使用存储资源。<strong>访问热度越高的数据</strong>（如被很多作业读取的数据表），<strong>会产生越多的副本</strong>，而请求很少甚至没有复用的数据则会被逐渐替换出Alluxio存储层（其在远端存储系统中的副本不会被清除）。而以HDFS为代表的存储系统通常是采用一个固定的副本数目（如3副本），很难根据具体的数据访问热度动态调节存储资源的使用。</li></ol></blockquote><h4 id="1-2-4-数据快速复用与共性"><a href="#1-2-4-数据快速复用与共性" class="headerlink" title="1.2.4 数据快速复用与共性"></a>1.2.4 数据快速复用与共性</h4><p>Alluxio可以帮助实现跨计算、作业间的数据快速复用和共享。</p><p>对于用户应用程序和大数据计算框架来说，Alluxio存储通常与计算框架并置。这种部署方式使Alluxio可以提供快速存储，促进作业之间的数据共享，无论它们是否在同一计算平台上运行。</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%875.png" alt="图片5"></p><h4 id="1-2-5-集成机器学习和深度学习"><a href="#1-2-5-集成机器学习和深度学习" class="headerlink" title="1.2.5 集成机器学习和深度学习"></a>1.2.5 集成机器学习和深度学习</h4><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%876.png" alt="图片6"></p><p>机器学习和深度学习框架往往需要从Hadoop或对象存储中提取大规模数据，这通常是手动且非常耗时的过程。通过Alluxio POSIX API克服该问题。</p><p>Alluxio的FUSE功能支持POSIX兼容的API，因此通过Alluxio，TensorFlow、 Caffe等框架以及其他基于Python的模型可以使用传统文件系统的访问方式直接访问任何存储系统中的数据。</p><h3 id="1-3-技术创新"><a href="#1-3-技术创新" class="headerlink" title="1.3 技术创新"></a>1.3 技术创新</h3><p>Alluxio将三个关键领域的创新结合在一起，提供了一套独特的功能。</p><ol><li><strong>全局命名空间</strong>: Alluxio 能够对多个独立存储系统提供单点访问，无论这些存储系统的物理位置在何处。这提供了所有数据源的统一视图和应用程序的标准接口。有关详细信息，请参阅命名空间管理。</li><li><strong>智能多层级缓存</strong>: Alluxio 集群能够充当底层存储系统中数据的读写缓存。可配置自动优化数据放置策略，以实现跨内存和磁盘（SSD/HDD）的性能和可靠性。缓存对用户是透明的，使用缓冲来保持与持久存储的一致性。有关详细信息，请参阅 Alluxio存储管理。</li><li><strong>服务器端 API 翻译转换</strong>: Alluxio支持工业界场景的API接口，例如HDFS API, S3 API, FUSE API, REST API。它能够透明地从标准客户端接口转换到任何存储接口。Alluxio 负责管理应用程序和文件或对象存储之间的通信，从而消除了对复杂系统进行配置和管理的需求。文件数据可以看起来像对象数据，反之亦然。</li></ol><h3 id="1-4-功能简介"><a href="#1-4-功能简介" class="headerlink" title="1.4 功能简介"></a>1.4 功能简介</h3><ul><li>灵活的API</li><li>兼容Haddop 的HDFS文件系统接口</li><li>分级存储，自定义分配和回收策略</li><li>统一命名空间</li><li>完整的命令行</li><li>Web UI</li></ul><h2 id="二、系统架构"><a href="#二、系统架构" class="headerlink" title="二、系统架构"></a>二、系统架构</h2><h3 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h3><p>Alluxio作为大数据和机器学习生态系统中的一个新的数据访问层，配置在任何持久性存储系统(如Amazon S3、Microsoft Azure对象存储、Apache HDFS或OpenStack Swift)和计算框架(如Apache Spark、Presto或Hadoop MapReduce)之间。<strong>请注意，Alluxio不是一个持久化存储系统。</strong>使用Alluxio作为数据访问层有如下好处：</p><ul><li>对于用户应用程序和计算框架，Alluxio提供了快速存储，促进了作业之间的数据共享和局部性，而不管使用的是哪种计算引擎。因此，当数据位于本地时，Alluxio可以以内存速度提供数据;当数据位于Alluxio时，Alluxio可以以计算集群网络的速度提供数据。第一次访问数据时，只从存储系统上读取一次数据。为了得到更好的性能，Alluxio推荐部署在计算集群上。</li><li>对于存储系统，Alluxio弥补了大数据应用与传统存储系统之间的差距，扩大了可用的数据工作负载集。当同时挂载多个数据源时，Alluxio可以作为任意数量的不同数据源的统一层。</li></ul><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><p>Alluxio可以分为三个组件：<strong>masters</strong>、<strong>worker</strong>和<strong>clients</strong>。一个典型的集群由一个单一的主要主节点<code>leading masters</code>、备用主节点<code>job master</code>、一个作业主节点<code>standby job masters</code>、备用作业主节点<code>standby job masters</code>、工作节点<code>workers</code>和作业工作节点<code>job workers</code>组成。master 和 worker 进程构成了 Alluxio 服务器，它们是系统管理员需要维护的组件。客户端用于通过 Spark 或 MapReduce 作业、Alluxio CLI 或 Alluxio FUSE 层等应用程序与 Alluxio 服务器通信。</p><p>Alluxio Job Masters 和 Job Workers 可以分离成一个单独的功能，称为 <strong>Job Service</strong>。Alluxio Job Service 是一个轻量级的任务调度框架，负责为 Job Workers 分配许多不同类型的操作。这些任务包括</p><ul><li>从 UFS 加载数据到 Alluxio</li><li>将数据持久化到 UFS</li><li>在 Alluxio 中复制文件</li><li>在 UFS 或 Alluxio 位置之间移动或复制数据</li></ul><p><strong>Job Service</strong>的设计使得所有与作业相关的进程不一定需要位于 Alluxio 集群的其余部分。但是，我们确实建议job worker 与相应的 Alluxio worker 位于同一位置，因为它为 RPC 和数据传输提供了更低的延迟。</p><h3 id="2-2-Mstaers"><a href="#2-2-Mstaers" class="headerlink" title="2.2 Mstaers"></a>2.2 Mstaers</h3><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%877.png" alt="图片7"></p><p>Alluxio 包含两种不同类型的主进程。<strong>Alluxio Master</strong> 为所有用户请求和日志文件系统元数据更改提供服务。<strong>Alluxio Job Master</strong>是作为文件系统操作的轻量级调度程序。</p><p>为了<strong>容错，Alluxio Master</strong>可以部署为一个主master和多个备用master。当主master节点宕机时，会选举一个备用master节点成为新的主master节点。</p><h4 id="2-2-1-主master"><a href="#2-2-1-主master" class="headerlink" title="2.2.1 主master"></a>2.2.1 主master</h4><p>Alluxio中只有一个master进程为主master。主master用于管理全局的元数据。这里面包含文件系统元数据（文件系统 inode 树）、数据块元数据（数据块位置）、以及worker的容量元数据（空闲或已占用空间）。Alluxio clients与主master通信用来读取或修改元数据。所有的worker都会定期的向主master发送心跳。主master会在一个分布式的持久化系统上记录所有的文件系统事务，这样可以恢复主master的信息。这组日志被称为journal。</p><h4 id="2-2-2-备用master"><a href="#2-2-2-备用master" class="headerlink" title="2.2.2 备用master"></a>2.2.2 备用master</h4><p>备用master读取主master写入的journal日志，以保持与主master的状态同步。它们会对journal日志写入检查点，用于快速恢复。它们不处理来自Alluxio组件的任何请求。</p><h4 id="2-2-3-Secondary-Masters-for-UFS-journal"><a href="#2-2-3-Secondary-Masters-for-UFS-journal" class="headerlink" title="2.2.3 Secondary Masters (for UFS journal)"></a>2.2.3 Secondary Masters (for UFS journal)</h4><p>当使用 UFS 日志在没有 HA 模式的情况下运行单个 Alluxio master 时，可以在与主要 master 相同的服务器上启动辅助 master 来写入日志检查点。请注意，辅助 master 并非旨在提供高可用性，而是从主要 master 卸载工作以实现快速恢复。与备用 Master 不同，从属 Master 永远无法升级为领先的 Master。</p><h4 id="2-2-4-Job-Masters"><a href="#2-2-4-Job-Masters" class="headerlink" title="2.2.4 Job Masters"></a>2.2.4 Job Masters</h4><p>Alluxio Job Master 是一个独立的进程，负责在 Alluxio 中异步调度多个重量级文件系统操作。通过分离领先的 Alluxio Master 在同一进程中执行这些操作，Alluxio Master 使用更少的资源，并能够在更短的时间内为更多的客户端提供服务。此外，它还为将来添加更复杂的操作提供了一个可扩展的框架。</p><h3 id="2-3-Workers"><a href="#2-3-Workers" class="headerlink" title="2.3 Workers"></a>2.3 Workers</h3><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%878.png" alt="图片8"></p><h4 id="2-3-1-Alluxio-Workers"><a href="#2-3-1-Alluxio-Workers" class="headerlink" title="2.3.1 Alluxio Workers"></a>2.3.1 Alluxio Workers</h4><p>Alluxio Workers 负责管理分配给 Alluxio 的用户可配置的本地资源（例如内存、SSD、HDD）。Alluxio Worker将数据存储为块，并通过在其本地资源中读取或创建新块来响应 client 请求。Workers 只用于管理数据块；从文件到块的实际映射仅由 master 存储。</p><p>Worker 在底层存储上执行数据操作。这带来了两个重要的好处：</p><ul><li>从底层存储读取的数据可以存储在 worker 中，并立即可供其他 client 使用。</li><li>client 可以是轻量级的，并且不依赖于底层存储连接器。</li></ul><p>由于 RAM 通常提供的容量有限，因此当空间已满时，可以逐出 worker 中的块。工作人员采用驱逐策略来决定将哪些数据保留在 Alluxio 空间中。有关此主题的更多信息，请查看<a href="https://docs.alluxio.io/os/user/stable/en/core-services/Caching.html#multiple-tier-storage" target="_blank" rel="noopener">分层存储</a>的文档。</p><h4 id="2-3-2-Alluxio-Job-Workers"><a href="#2-3-2-Alluxio-Job-Workers" class="headerlink" title="2.3.2 Alluxio Job Workers"></a>2.3.2 Alluxio Job Workers</h4><p>Alluxio Job Workers 是 Alluxio 文件系统的客户端。他们负责运行 Alluxio Job Master 交给他们的任务。Job Workers 接收指令以在任何给定的文件系统位置上运行加载、持久化、复制、移动或复制操作。</p><p>Alluxio job workers 不一定要与普通 workers 位于同一位置，但建议两者在同一个物理节点上。</p><h3 id="2-4-Client"><a href="#2-4-Client" class="headerlink" title="2.4 Client"></a>2.4 Client</h3><p>Alluxio Client 为用户提供了一个与 Alluxio 服务器交互的网关。它发起与 leading master 通信以执行元数据操作，并与 worker 通信以读取和写入存储在 Alluxio 中的数据。Alluxio 支持 Java 中的本机文件系统 API 和多种语言（包括 REST、Go 和 Python）。Alluxio 还支持与 HDFS API 和 Amazon S3 API 兼容的 API。</p><p>请注意，Alluxio Client 从不直接访问底层存储系统。数据通过 Alluxio Workers 传输。</p><p><strong>alluxio服务的几个组件功能总结</strong></p><blockquote><table><thead><tr><th>组件名称</th><th>作用</th></tr></thead><tbody><tr><td>Master</td><td>1.管理全局的元数据信息（包括目录树(inode tree)，块信息(block)，worker的容量信息)；2.worker发送心跳；3.client获取元数据</td></tr><tr><td>SecondaryMaster</td><td>1.读取主master写入的journal日志，以保持与主master的状态同步；2.对journal日志写入检查点，用于快速恢复；3.不处理来自Alluxio组件的任何请求</td></tr><tr><td>JobMaster</td><td>1.负责在Alluxio中异步调度许多重量级文件系统操作，提高master吞吐量；2.为后期复杂操作提供扩展性</td></tr><tr><td>Worker</td><td>1.底层数据存储在worker中供客户端使用；2.alluxio客户端不依赖底层存储</td></tr><tr><td>JobWorker</td><td>执行job master分配的任务，建议与worker放在同一节点上</td></tr><tr><td>Client</td><td>集成到其他计算框架中，调用Alluxio leading master以执行元数据操作，与 workers 通信以读取和写入存储在 Alluxio 中的数据</td></tr></tbody></table></blockquote><h3 id="2-5-读数据流"><a href="#2-5-读数据流" class="headerlink" title="2.5 读数据流"></a>2.5 读数据流</h3><p>Alluxio 位于底层存储和计算框架之间，充当数据读取的缓存层。</p><h4 id="2-5-1-本地缓存命中"><a href="#2-5-1-本地缓存命中" class="headerlink" title="2.5.1 本地缓存命中"></a>2.5.1 本地缓存命中</h4><p>本地缓存命中发生在请求数据位于本地Alluxio worker。举例说明，如果一个应用通过Alluxio client请求数据，client向Alluxio master请求数据所在的worker。如果数据在本地可用，Alluxio client使用<code>短路</code>读取来绕过Alluxio worker，并直接通过本地文件系统读取文件。短路读取避免通过TCP套接字传输数据，并提供数据的直接访问。</p><p>还要注意，Alluxio可以管理除了内存之外还可以管理其他存储介质（例如SSD、HDD），因此本地数据访问速度可能会因本地存储介质的不同而有所不同。</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%879.gif" alt="图片9"></p><h4 id="2-5-2-远程缓存命中"><a href="#2-5-2-远程缓存命中" class="headerlink" title="2.5.2 远程缓存命中"></a>2.5.2 远程缓存命中</h4><p>当请求的数据存储在Alluxio中，而不是存储在client的本地worker上时，client将对具有数据的worker进行远程读取。client完成读取后，会要求本地的worker（如果存在）创建一个copy，这样以后读取的时候可以在本地读取相同的数据。远程缓存命中提供了网络级别速度的数据读取。Alluxio优先从远程worker读取数据，而不是从底层存储，因为Alluxio workers之间的速度一般会快过Alluxio workers和底层存储的速度。</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8710.gif" alt="图片10"></p><h4 id="2-5-3-缓存Miss"><a href="#2-5-3-缓存Miss" class="headerlink" title="2.5.3 缓存Miss"></a>2.5.3 缓存Miss</h4><p>如果数据在 Alluxio 中找不到，则会发生缓存未命中，应用程序将不得不从底层存储中读取数据。Alluxio client 将读取从 UFS 委托给worker（有限本地worker）。该worker从底层存储中读取和缓存数据。缓存未命中通常会导致最大的延迟，因为必须从底层存储中获取数据。首次读取数据时预计会发生缓存未命中。</p><p>当 client 仅读取块的一部分或非顺序读取块时，client 将指示worker异步缓存完整个块。这称为异步缓存。异步缓存不会阻塞client，但如果 Alluxio 和底层存储系统之间的网络带宽成为瓶颈，仍然可能会影响性能。您可以通过<code>alluxio.worker.network.async.cache.manager.threads.max</code>对worker进行设置来调整异步缓存的影响 。默认值为<code>8</code>。</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8711.gif" alt="图片11"></p><h4 id="2-5-4-缓存跳过"><a href="#2-5-4-缓存跳过" class="headerlink" title="2.5.4 缓存跳过"></a>2.5.4 缓存跳过</h4><p>可以通过将<a href="https://docs.alluxio.io/os/user/stable/en/reference/Properties-List.html#alluxio.user.file.readtype.default" target="_blank" rel="noopener"><code>alluxio.user.file.readtype.default</code></a> client中的属性设置为 <code>NO_CACHE</code> 来关闭 Alluxio 中的缓存。</p><h3 id="2-6-写数据流"><a href="#2-6-写数据流" class="headerlink" title="2.6 写数据流"></a>2.6 写数据流</h3><p>用户可以通过选择不同的写入类型来配置数据的写入方式。写入类型可以通过 Alluxio API 或通过配置属性 <a href="https://docs.alluxio.io/os/user/stable/en/reference/Properties-List.html#alluxio.user.file.writetype.default" target="_blank" rel="noopener"><code>alluxio.user.file.writetype.default</code></a> 来设置写入类型。</p><h4 id="2-6-1-仅写入-Alluxio-MUST-CACHE"><a href="#2-6-1-仅写入-Alluxio-MUST-CACHE" class="headerlink" title="2.6.1 仅写入 Alluxio ( MUST_CACHE)"></a>2.6.1 仅写入 Alluxio ( <code>MUST_CACHE</code>)</h4><p>当写类型设置为MUST_CACHE，Alluxio client将数据写入本地Alluxio worker，而不会写入到底层存储。如果<code>短路</code>写可用，Alluxio client直接写入到本地RAM的文件，绕过Alluxio worker 避免网络传输。由于数据没有持久存储在under storage中，因此如果机器崩溃或需要释放数据以进行更新的写操作，数据可能会丢失。当可以容忍数据丢失时，MUST_CACHE 设置可用于写入临时数据。</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8712.gif" alt="图片12"></p><h4 id="2-6-2-直写到-UFS-CACHE-THROUGH"><a href="#2-6-2-直写到-UFS-CACHE-THROUGH" class="headerlink" title="2.6.2 直写到 UFS ( CACHE_THROUGH)"></a>2.6.2 直写到 UFS ( <code>CACHE_THROUGH</code>)</h4><p>使用CACHE_THROUGH写类型，数据同步写入Alluxio worker和底层存储系统。Alluxio client将写操作委托给本地worker，而worker同时将对本地内存和底层存储。由于底层存储的写入速度通常比本地存储慢，所以client的写入速度将与底层存储的速度相匹配。当需要数据持久化时，建议使用CACHE_THROUGH写类型。在本地还存了一份副本，以便可以直接从本地内存中读取数据。</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8713.gif" alt="图片13"></p><h4 id="2-6-3-写回-UFS-ASYNC-THROUGH"><a href="#2-6-3-写回-UFS-ASYNC-THROUGH" class="headerlink" title="2.6.3 写回 UFS ( ASYNC_THROUGH)"></a>2.6.3 写回 UFS ( <code>ASYNC_THROUGH</code>)</h4><p>Alluxio 提供了一种写类型<code>ASYNC_THROUGH</code>. 使用<code>ASYNC_THROUGH</code>，数据首先被同步写入一个 Alluxio worker，并在后台持久化到 under storage。<code>ASYNC_THROUGH</code>可以以接近 <code>MUST_CACHE</code> 的速度提供数据写入，同时仍然能够持久保存数据。从 Alluxio 2.0 开始， <code>ASYNC_THROUGH</code>是默认的写入类型。</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8714.gif" alt="图片14"></p><p>为了提供容错，使用的一个重要属性<code>ASYNC_THROUGH</code>是 <code>alluxio.user.file.replication.durable</code> 该属性设置了Alluxio在写入完成后数据持久化到under storage之前的新数据的目标复制级别，默认值为1。Alluxio将在后台持久化进程完成之前保持文件的目标复制级别，并且之后回收Alluxio中的空间，因此数据只会写入UFS一次。</p><p>如果您正在写入副本<code>ASYNC_THROUGH</code>并且所有具有副本的worker在您保存数据之前崩溃，那么您将导致数据丢失。</p><h4 id="2-6-4-仅写入-UFS-THROUGH"><a href="#2-6-4-仅写入-UFS-THROUGH" class="headerlink" title="2.6.4 仅写入 UFS ( THROUGH)"></a>2.6.4 仅写入 UFS ( <code>THROUGH</code>)</h4><p>使用<code>THROUGH</code>，数据被同步写入under storage，而不会缓存到Alluxio worker。这种写类型保证了写完成后数据会持久化，但是速度受限于under storage的吞吐量。</p><h4 id="2-6-5-数据一致性"><a href="#2-6-5-数据一致性" class="headerlink" title="2.6.5 数据一致性"></a>2.6.5 数据一致性</h4><p>无论写入类型如何，<strong>Alluxio 空间</strong>中的文件/目录始终是强一致的，因为所有这些写入操作都将首先通过 Alluxio master 并在将成功返回给客户端/应用程序之前修改 Alluxio 文件系统。因此，只要其对应的写操作成功完成，不同的 Alluxio 客户端将始终看到最新的更新。</p><p>但是，对于考虑 UFS 中状态的用户或应用程序，它可能因写入类型而异：</p><ul><li><code>MUST_CACHE</code> 不向 UFS 写入数据，因此 Alluxio 空间永远不会与 UFS 保持一致。</li><li>CACHE_THROUGH 在将成功返回给应用程序之前，将数据同步写入 Alluxio 和 UFS。<ul><li>如果写入UFS也是强一致性（例如HDFS），如果UFS中没有其他带外更新，Alluxio空间将始终与UFS一致；</li><li>如果写入 UFS 最终一致（例如 S3），则文件可能成功写入 Alluxio，但稍后会显示在 UFS 中。在这种情况下，Alluxio 客户端仍然会看到一致的文件系统，因为他们总是会咨询强一致性的 Alluxio master；因此，尽管不同的 Alluxio 客户端仍然在 Alluxio 空间中看到一致的状态，但在数据最终传播到 UFS 之前可能会有一个不一致的窗口。</li></ul></li><li><code>ASYNC_THROUGH</code>将数据写入 Alluxio 并返回到应用程序，让 Alluxio 将数据异步传播到 UFS。从用户的角度来看，该文件可以成功写入Alluxio，但稍后会在UFS中持久化。</li><li><code>THROUGH</code>直接将数据写入 UFS，无需在 Alluxio 中缓存数据，但是，Alluxio 知道文件及其状态。因此元数据仍然是一致的。</li></ul><blockquote><table><thead><tr><th>策略</th><th>注释</th></tr></thead><tbody><tr><td>MUST_CACHE</td><td>只写入缓存</td></tr><tr><td>CACHE_THROUGH</td><td>同步写入缓存和底层存储系统</td></tr><tr><td>ASYNC_THROUGH</td><td>异步写入缓存和底层存储系统，调用jobmaster和jobworker完成</td></tr><tr><td>THROUGH</td><td>只写入底层存储</td></tr></tbody></table></blockquote><h2 id="三、核心功能"><a href="#三、核心功能" class="headerlink" title="三、核心功能"></a>三、核心功能</h2><h3 id="3-1-Alluxio存储概述"><a href="#3-1-Alluxio存储概述" class="headerlink" title="3.1 Alluxio存储概述"></a>3.1 Alluxio存储概述</h3><p>Alluxio在帮助统一跨各种平台用户数据的同时还有助于为用户提升总体I/O吞吐量。 Alluxio是通过把存储分为两个不同的类别来实现这一目标的。</p><ul><li><strong>UFS（底层文件存储，也称为底层存储）</strong>该存储空间代表不受Alluxio管理的空间。 UFS存储可能来自外部文件系统，包括如HDFS或S3。 Alluxio可能连接到一个或多个UFS并在一个命名空间中统一呈现这类底层存储。 -通常，UFS存储旨在相当长一段时间持久存储大量数据。</li><li>Alluxio存储<ul><li>Alluxio做为一个分布式缓存来管理Alluxio workers本地存储，包括内存。这个在用户应用程序与各种底层存储之间的快速数据层带来的是显著提高的I/O性能；</li><li>Alluxio存储主要用于存储热数据，暂态数据，而不是长期持久数据存储；</li><li>每个Alluxio节点要管理的存储量和类型由用户配置决定；</li><li>即使数据当前不在Alluxio存储中，通过Alluxio连接的UFS中的文件仍然对Alluxio客户可见。当客户端尝试读取仅可从UFS获得的文件时数据将被复制到Alluxio存储中。</li></ul></li></ul><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8715.png" alt="图片15"></p><p>Alluxio存储通过将数据存储在计算节点内存中来提高性能。 Alluxio存储中的数据可以被复制来形成“热”数据，更易于I/O并行操作和使用。</p><p>Alluxio中的数据副本独立于UFS中可能已存在的副本。 Alluxio存储中的数据副本数是由集群活动动态决定的。 由于Alluxio依赖底层文件存储来存储大部分数据， Alluxio不需要保存未使用的数据副本。</p><p>Alluxio还支持让系统存储软件可感知的分层存储，使类似L1/L2 CPU缓存一样的数据存储优化成为可能。</p><h3 id="3-2-配置Alluxio存储"><a href="#3-2-配置Alluxio存储" class="headerlink" title="3.2 配置Alluxio存储"></a>3.2 配置Alluxio存储</h3><h4 id="3-2-1-单层存储"><a href="#3-2-1-单层存储" class="headerlink" title="3.2.1 单层存储"></a>3.2.1 单层存储</h4><p>配置Alluxio存储的最简单方法是使用默认的单层模式。</p><p>请注意，此部分是讲本地存储，诸如<code>mount</code>之类的术语指在本地存储文件系统上挂载，不要与Alluxio的外部底层存储的<code>mount</code>概念混淆。</p><p>在启动时，Alluxio将在每个worker节点上发放一个ramdisk并占用一定比例的系统的总内存。 此ramdisk将用作分配给每个Alluxio worker的唯一存储介质。</p><p>通过Alluxio配置中的<code>alluxio-site.properties</code>来配置Alluxio存储。 详细信息见<a href="https://docs.alluxio.io/os/user/stable/en/operation/Configuration.html" target="_blank" rel="noopener">configuration settings</a>。</p><p>对默认值的常见修改是明确设置ramdisk的大小。 例如，设置每个worker的ramdisk大小为16GB：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">alluxio.worker.ramdisk.size</span>=<span class="string">16GB</span></span><br></pre></td></tr></table></figure><p>另一个常见更改是指定多个存储介质，例如ramdisk和SSD。 需要 更新<code>alluxio.worker.tieredstore.level0.dirs.path</code>以指定想用的每个存储介质 为一个相应的存储目录。 例如，要使用ramdisk(挂载在<code>/mnt/ramdisk</code>上)和两个SSD(挂载在<code>/mnt/ssd1</code>和<code>/mnt/ssd2</code>):</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">alluxio.worker.tieredstore.level0.dirs.path</span>=<span class="string">/mnt/ramdisk,/mnt/ssd1,/mnt/ssd2</span></span><br><span class="line"><span class="meta">alluxio.worker.tieredstore.level0.dirs.mediumtype</span>=<span class="string">MEM,SSD,SSD</span></span><br></pre></td></tr></table></figure><p>请注意，介质类型的顺序必须与路径的顺序相符。 MEM和SSD是Alluxio中的两种预配置存储类型。 <code>alluxio.master.tieredstore.global.mediumtype</code>是包含所有可用的介质类型的配置参数，默认情况下设置为<code>MEM，SSD，HDD</code>。 如果用户有额外存储介质类型可以通过修改这个配置来增加。</p><p>提供的路径应指向挂载适当存储介质的本地文件系统中的路径。 为了实现短路操作，对于这些路径，应允许客户端用户在这些路径上进行读取，写入和执行。 例如，对于与启动Alluxio服务的用户组同组用户应给予<code>770</code>权限。</p><p>更新存储介质后，需要指出每个存储目录分配了多少存储空间。 例如，如果要在ramdisk上使用16 GB，在每个SSD上使用100 GB：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">alluxio.worker.tieredstore.level0.dirs.quota</span>=<span class="string">16GB,100GB,100GB</span></span><br></pre></td></tr></table></figure><p>注意存储空间配置的顺序一定与存储目录的配置相符。</p><p>Alluxio在通过<code>Mount</code>或<code>SudoMount</code>选项启动时，配置并挂载ramdisk。 这个ramdisk大小是由<code>alluxio.worker.ramdisk.size</code>确定的。 默认情况下，tier 0设置为MEM并且管理整个ramdisk。 此时<code>alluxio.worker.tieredstore.level0.dirs.quota</code>的值同<code>alluxio.worker.ramdisk.size</code>一样。 如果tier0要使用除默认的ramdisk以外的设备，应该显式地设置<code>alluxio.worker.tieredstore.level0.dirs.quota</code>选项。</p><h4 id="3-2-2-多层存储"><a href="#3-2-2-多层存储" class="headerlink" title="3.2.2 多层存储"></a>3.2.2 多层存储</h4><p>通常建议异构存储介质也使用单个存储层。 在特定环境中，工作负载将受益于基于I/O速度存储介质明确排序。 Alluxio假定根据按I/O性能从高到低来对多层存储进行排序。 例如，用户经常指定以下层：</p><ul><li>MEM(内存)</li><li>SSD(固态存储)</li><li>HDD(硬盘存储)</li></ul><h5 id="3-2-2-1-写数据"><a href="#3-2-2-1-写数据" class="headerlink" title="3.2.2.1 写数据"></a>3.2.2.1 写数据</h5><p>用户写新的数据块时，默认情况下会将其写入顶层存储。如果顶层没有足够的可用空间， 则会尝试下一层存储。如果在所有层上均未找到存储空间，因Alluxio的设计是易失性存储，Alluxio会释放空间来存储新写入的数据块。 根据块注释策略，空间释放操作会从work中释放数据块。 <a href="https://docs.alluxio.io/os/user/stable/cn/core-services/Caching.html#block-annotation-policies" target="_blank" rel="noopener">块注释策略</a>。 如果空间释放操作无法释放新空间，则写数据将失败。</p><p><strong>注意：</strong>新的释放空间模型是同步模式并会代表要求为其要写入的数据块释放新空白存储空间的客户端来执行释放空间操作。 在块注释策略的帮助下，同步模式释放空间不会引起性能下降，因为总有已排序的数据块列表可用。 然而，可以将<code>alluxio.worker.tieredstore.free.ahead.bytes</code>(默认值：0)配置为每次释放超过释放空间请求所需字节数来保证有多余的已释放空间满足写数据需求。</p><p>用户还可以通过<a href="https://docs.alluxio.io/os/user/stable/cn/core-services/Caching.html#configuring-tiered-storage" target="_blank" rel="noopener">configuration settings</a>来指定写入数据层。</p><h5 id="3-2-2-2-读取数据"><a href="#3-2-2-2-读取数据" class="headerlink" title="3.2.2.2 读取数据"></a>3.2.2.2 读取数据</h5><p>如果数据已经存在于Alluxio中，则客户端将简单地从已存储的数据块读取数据。 如果将Alluxio配置为多层，则不一定是从顶层读取数据块， 因为数据可能已经透明地挪到更低的存储层。</p><p>用<code>ReadType.CACHE_PROMOTE</code>读取数据将在从worker读取数据前尝试首先将数据块挪到顶层存储。也可以将其用作为一种数据管理策略 明确地将热数据移动到更高层存储读取。</p><h5 id="3-2-2-3-配置分层存储"><a href="#3-2-2-3-配置分层存储" class="headerlink" title="3.2.2.3 配置分层存储"></a>3.2.2.3 配置分层存储</h5><p>可以使用以下方式在Alluxio中启用分层存储 <a href="https://docs.alluxio.io/os/user/stable/en/operation/Configuration.html" target="_blank" rel="noopener">配置参数</a>。 为Alluxio指定额外存储层，使用以下配置参数：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">alluxio.worker.tieredstore.levels</span></span><br><span class="line"><span class="attr">alluxio.worker.tieredstore.level&#123;x&#125;.alias</span></span><br><span class="line"><span class="attr">alluxio.worker.tieredstore.level&#123;x&#125;.dirs.quota</span></span><br><span class="line"><span class="attr">alluxio.worker.tieredstore.level&#123;x&#125;.dirs.path</span></span><br><span class="line"><span class="attr">alluxio.worker.tieredstore.level&#123;x&#125;.dirs.mediumtype</span></span><br></pre></td></tr></table></figure><p>例如，如果计划将Alluxio配置为具有两层存储，内存和硬盘存储， 可以使用类似于以下的配置：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># configure 2 tiers in Alluxio</span></span><br><span class="line"><span class="meta">alluxio.worker.tieredstore.levels</span>=<span class="string">2</span></span><br><span class="line"><span class="comment"># the first (top) tier to be a memory tier</span></span><br><span class="line"><span class="meta">alluxio.worker.tieredstore.level0.alias</span>=<span class="string">MEM</span></span><br><span class="line"><span class="comment"># defined `/mnt/ramdisk` to be the file path to the first tier</span></span><br><span class="line"><span class="meta">alluxio.worker.tieredstore.level0.dirs.path</span>=<span class="string">/mnt/ramdisk</span></span><br><span class="line"><span class="comment"># defined MEM to be the medium type of the ramdisk directory</span></span><br><span class="line"><span class="meta">alluxio.worker.tieredstore.level0.dirs.mediumtype</span>=<span class="string">MEM</span></span><br><span class="line"><span class="comment"># set the quota for the ramdisk to be `100GB`</span></span><br><span class="line"><span class="meta">alluxio.worker.tieredstore.level0.dirs.quota</span>=<span class="string">100GB</span></span><br><span class="line"><span class="comment"># configure the second tier to be a hard disk tier</span></span><br><span class="line"><span class="meta">alluxio.worker.tieredstore.level1.alias</span>=<span class="string">HDD</span></span><br><span class="line"><span class="comment"># configured 3 separate file paths for the second tier</span></span><br><span class="line"><span class="meta">alluxio.worker.tieredstore.level1.dirs.path</span>=<span class="string">/mnt/hdd1,/mnt/hdd2,/mnt/hdd3</span></span><br><span class="line"><span class="comment"># defined HDD to be the medium type of the second tier</span></span><br><span class="line"><span class="meta">alluxio.worker.tieredstore.level1.dirs.mediumtype</span>=<span class="string">HDD,HDD,HDD</span></span><br><span class="line"><span class="comment"># define the quota for each of the 3 file paths of the second tier</span></span><br><span class="line"><span class="meta">alluxio.worker.tieredstore.level1.dirs.quota</span>=<span class="string">2TB,5TB,500GB</span></span><br></pre></td></tr></table></figure><p>可配置层数没有限制 但是每个层都必须使用唯一的别名进行标识。 典型的配置将具有三层，分别是内存，SSD和HDD。 要在HDD层中使用多个硬盘存储，需要在配置<code>alluxio.worker.tieredstore.level{x}.dirs.path</code>时指定多个路径。</p><h4 id="3-2-3-块注释策略"><a href="#3-2-3-块注释策略" class="headerlink" title="3.2.3 块注释策略"></a>3.2.3 块注释策略</h4><p>Alluxio从v2.3开始使用块注释策略来维护存储中数据块的严格顺序。 注释策略定义了跨层块的顺序，并在以下操作过程中进行用来参考: -释放空间 -<a href="https://docs.alluxio.io/os/user/stable/cn/core-services/Caching.html#block-aligning-dynamic-block-placement" target="_blank" rel="noopener">动态块放置</a>。</p><p>与写操作同步发生的释放空间操作将尝试根据块注释策略强制顺序删除块并释放其空间给写操作。注释顺序的最后一个块是第一个释放空间候选对象，无论它位于哪个层上。</p><p>开箱即用的注释策略实施包括:</p><ul><li><strong>LRUAnnotator：</strong>根据最近最少使用的顺序对块进行注释和释放。 <strong>这是Alluxio的默认注释策略</strong>。</li><li><strong>LRFUAnnotator：</strong>根据配置权重的最近最少使用和最不频繁使用的顺序对块进行注释。<ul><li>如果权重完全偏设为最近最少使用，则行为将 与LRUAnnotator相同。</li><li>适用的配置属性包括<code>alluxio.worker.block.annotator.lrfu.step.factor</code> <code>alluxio.worker.block.annotator.lrfu.attenuation.factor</code>。</li></ul></li></ul><p>workers选择使用的注释策略由Alluxio属性 <a href="https://docs.alluxio.io/os/user/stable/en/reference/Properties-List.html#alluxio.worker.block.annotator.class" target="_blank" rel="noopener">alluxio.worker.block.annotator.class</a>决定。 该属性应在配置中指定完全验证的策略名称。当前可用的选项有:</p><ul><li><code>alluxio.worker.block.annotator.LRUAnnotator</code></li><li><code>alluxio.worker.block.annotator.LRFUAnnotator</code></li></ul><h5 id="3-2-3-1-释放空间模拟"><a href="#3-2-3-1-释放空间模拟" class="headerlink" title="3.2.3.1 释放空间模拟"></a>3.2.3.1 释放空间模拟</h5><p>旧的释放空间策略和Alluxio提供的实施现在已去掉了，并用适当的注释策略替换。 配置旧的Alluxio释放空间策略将导致worker启动失败，并报错<code>java.lang.ClassNotFoundException</code>。 同样，旧的基于水位标记配置已失效。因此，以下配置选项是无效的:</p><ul><li><code>alluxio.worker.tieredstore.levelX.watermark.low.ratio</code></li><li><code>alluxio.worker.tieredstore.levelX.watermark.high.ratio</code></li></ul><p>然而，Alluxio支持基于自定义释放空间实施算法数据块注释的仿真模式。该仿真模式假定已配置的释放空间策略创建一个基于某种顺序释放空间的计划，并通过定期提取这种自定义顺序来支持块注释活动。</p><p>旧的释放空间配置应进行如下更改。 (由于旧的释放空间实施已删除，如未能更改基于旧实施的以下配置就会导致class加载错误。)</p><p>-LRUEvictor-&gt; LRUAnnotator -GreedyEvictor-&gt; LRUAnnotator -PartialLRUEvictor-&gt; LRUAnnotator -LRFUEvictor-&gt; LRFUAnnotator</p><h4 id="3-2-4-分层存储管理"><a href="#3-2-4-分层存储管理" class="headerlink" title="3.2.4 分层存储管理"></a>3.2.4 分层存储管理</h4><p>因为块分配/释放不再强制新的写入必须写到特定存储层，新数据块可能最终被写到任何已配置的存储层中。这样允许写入超过Alluxio存储容量的数据。但是，这就需要Alluxio动态管理块放置。 为了确保层配置为从最快到最慢的假设，Alluxio会基于块注释策略在各层存储之间移动数据块。</p><p>每个单独层管理任务都遵循以下配置:</p><ul><li><code>alluxio.worker.management.task.thread.count</code>:管理任务所用线程数。 (默认值:CPU核数)</li><li><code>alluxio.worker.management.block.transfer.concurrency.limit</code>:可以同时执行多少个块传输。 (默认:<code>CPU核数</code>/2)</li></ul><h5 id="3-2-4-1-块对齐-动态块放置"><a href="#3-2-4-1-块对齐-动态块放置" class="headerlink" title="3.2.4.1 块对齐(动态块放置)"></a>3.2.4.1 块对齐(动态块放置)</h5><p>Alluxio将动态地跨层移动数据块，以使块组成与配置的块注释策略一致。</p><p>为辅助块对齐，Alluxio会监视I/O模式并会跨层重组数据块，以确保 <strong>较高层的最低块比下面层的最高块具有更高的次序</strong>。</p><p>这是通过“对齐”这个管理任务来实现的。此管理任务在检测到层之间 顺序已乱时，会通过在层之间交换块位置来有效地将各层与已配置的注释策略对齐以消除乱序。 有关如何控制这些新的后台任务对用户I/O的影响，参见<a href="https://docs.alluxio.io/os/user/stable/cn/core-services/Caching.html#management-task-back-off" target="_blank" rel="noopener">管理任务推后</a>部分。</p><p>用于控制层对齐:</p><ul><li><code>alluxio.worker.management.tier.align.enabled</code>:是否启用层对齐任务。 (默认: <code>true</code>)</li><li><code>alluxio.worker.management.tier.align.range</code>:单个任务运行中对齐多少个块。 (默认值:<code>100</code>)</li><li><code>alluxio.worker.management.tier.align.reserved.bytes</code>:配置多层时，默认情况下在所有目录上保留的空间大小。 (默认:1GB) 用于内部块移动。</li><li><code>alluxio.worker.management.tier.swap.restore.enabled</code>:控制一个特殊任务，该任务用于在内部保留空间用尽时unblock层对齐。 (默认:true) 由于Alluxio支持可变的块大小，因此保留空间可能会用尽，因此，当块大小不匹配时在块对齐期间在层之间块交换会导致一个目录保留空间的减少。</li></ul><h5 id="3-2-4-2-块升级"><a href="#3-2-4-2-块升级" class="headerlink" title="3.2.4.2 块升级"></a>3.2.4.2 块升级</h5><p>当较高层具有可用空间时，低层的块将向上层移动，以便更好地利用较快的磁盘介质，因为假定较高的层配置了较快的磁盘介质。</p><p>用于控制动态层升级:</p><ul><li><code>alluxio.worker.management.tier.promote.enabled</code>:是否启用层升级任务。 (默认: <code>true</code>)</li><li><code>alluxio.worker.management.tier.promote.range</code>:单个任务运行中升级块数。 (默认值:<code>100</code>)</li><li><code>alluxio.worker.management.tier.promote.quota.percent</code>:每一层可以用于升级最大百分比。 一旦其已用空间超过此值，向此层升级将停止。 (0表示永不升级，100表示总是升级。)</li></ul><h5 id="3-2-4-3-管理任务推后"><a href="#3-2-4-3-管理任务推后" class="headerlink" title="3.2.4.3 管理任务推后"></a>3.2.4.3 管理任务推后</h5><p>层管理任务(对齐/升级)会考虑用户I/O并在worker/disk重载情况下推后运行。 这是为了确保内部管理任务不会对用户I/O性能产生负面影响。</p><p>可以在<code>alluxio.worker.management.backoff.strategy</code>属性中设置两种可用的推后类型，分别是Any和DIRECTORY。</p><ul><li><code>ANY</code>; 当有任何用户I/O时，worker管理任务将推后。 此模式将确保较低管理任务开销，以便提高即时用户I/O性能。 但是，管理任务要取得进展就需要在worker上花更长的时间。</li><li><code>DIRECTORY</code>; 管理任务将从有持续用户I/O的目录中推后。 此模式下管理任务更易取得进展。 但是，由于管理任务活动的增加，可能会降低即时用户I/O吞吐量。</li></ul><p>影响这两种推后策略的另一个属性是<code>alluxio.worker.management.load.detection.cool.down.time</code>，控制多长时间的用户I/O计为在目标directory/worker上的一个负载。</p><h3 id="3-3-Alluxio中数据生命周期管理"><a href="#3-3-Alluxio中数据生命周期管理" class="headerlink" title="3.3 Alluxio中数据生命周期管理"></a>3.3 Alluxio中数据生命周期管理</h3><p>用户需要理解以下概念，以正确利用可用资源:</p><ul><li><strong>free：</strong>释放数据是指从Alluxio缓存中删除数据，而不是从底层UFS中删除数据。 释放操作后，数据仍然可供用户使用，但对Alluxio释放文件后尝试访问该文件 的客户端来讲性能可能会降低。</li><li><strong>load：</strong>加载数据意味着将其从UFS复制到Alluxio缓存中。如果Alluxio使用基于内存的存储，加载后用户可能会看到I/O性能的提高。</li><li><strong>persist：</strong>持久数据是指将Alluxio存储中可能被修改过或未被修改过的数据写回UFS。 通过将数据写回到UFS，可以保证如果Alluxio节点发生故障数据还是可恢复的。</li><li><strong>TTL(Time to Live)：</strong>TTL属性设置文件和目录的生存时间，以在数据超过其生存时间时将它们从Alluxio空间中删除。还可以配置 TTL来删除存储在UFS中的相应数据。</li></ul><h4 id="3-3-1-从Alluxio存储中释放数据"><a href="#3-3-1-从Alluxio存储中释放数据" class="headerlink" title="3.3.1 从Alluxio存储中释放数据"></a>3.3.1 从Alluxio存储中释放数据</h4><p>为了在Alluxio中手动释放数据，可以使用<code>./bin/alluxio</code>文件系统命令 行界面。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio fs free <span class="variable">$&#123;PATH_TO_UNUSED_DATA&#125;</span></span></span><br></pre></td></tr></table></figure><p>这将从Alluxio存储中删除位于给定路径的数据。如果数据是持久存储到UFS的则仍然可以访问该数据。有关更多信息，参考 <a href="https://docs.alluxio.io/os/user/stable/en/operation/User-CLI.html#free" target="_blank" rel="noopener">命令行界面文档</a></p><p>注意，用户通常不需要手动从Alluxio释放数据，因为配置的<a href="https://docs.alluxio.io/os/user/stable/cn/core-services/Caching.html#block-annotation-policies" target="_blank" rel="noopener">注释策略</a>将负责删除未使用或旧数据。</p><h4 id="3-3-2-将数据加载到Alluxio存储中"><a href="#3-3-2-将数据加载到Alluxio存储中" class="headerlink" title="3.3.2 将数据加载到Alluxio存储中"></a>3.3.2 将数据加载到Alluxio存储中</h4><p>如果数据已经在UFS中，使用 <a href="https://docs.alluxio.io/os/user/stable/en/operation/User-CLI.html#load" target="_blank" rel="noopener"><code>alluxio fs load</code></a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio fs load <span class="variable">$&#123;PATH_TO_FILE&#125;</span></span></span><br></pre></td></tr></table></figure><p>要从本地文件系统加载数据，使用命令 <a href="https://docs.alluxio.io/os/user/stable/en/operation/User-CLI.html#copyfromlocal" target="_blank" rel="noopener"><code>alluxio fs copyFromLocal</code></a>。 这只会将文件加载到Alluxio存储中，而不会将数据持久保存到UFS中。 将写入类型设置为<code>MUST_CACHE</code>写入类型将不会将数据持久保存到UFS， 而设置为<code>CACHE</code>和<code>CACHE_THROUGH</code>将会持久化保存。不建议手动加载数据，因为，当首次使用文件时Alluxio会自动将数据加载到Alluxio缓存中。</p><h4 id="3-3-3-在Alluxio中持久化保留数据"><a href="#3-3-3-在Alluxio中持久化保留数据" class="headerlink" title="3.3.3 在Alluxio中持久化保留数据"></a>3.3.3 在Alluxio中持久化保留数据</h4><p>命令<a href="https://docs.alluxio.io/os/user/stable/en/operation/User-CLI.html#persist" target="_blank" rel="noopener"><code>alluxio fs persist</code></a> 允许用户将数据从Alluxio缓存推送到UFS。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio fs persist <span class="variable">$&#123;PATH_TO_FILE&#125;</span></span></span><br></pre></td></tr></table></figure><p>如果您加载到Alluxio的数据不是来自已配置的UFS，则上述命令很有用。 在大多数情况下，用户不必担心手动来持久化保留数据。</p><h4 id="3-3-4-设置生存时间-TTL"><a href="#3-3-4-设置生存时间-TTL" class="headerlink" title="3.3.4 设置生存时间(TTL)"></a>3.3.4 设置生存时间(TTL)</h4><p>Alluxio支持命名空间中每个文件和目录的”生存时间(TTL)”设置。此功能可用于有效地管理Alluxio缓存，尤其是在严格保证数据访问模式的环境中。例如，如果对上一周提取数据进行分析， 则TTL功能可用于明确刷新旧数据，从而为新文件释放缓存空间。</p><p>Alluxio具有与每个文件或目录关联的TTL属性。这些属性将保存为日志的一部分，所以集群重启后也能持久保持。活跃master节点负责当Alluxio提供服务时将元数据保存在内存中。在内部，master运行一个后台线程，该线程定期检查文件是否已达到其TTL到期时间。</p><p><strong>注意：</strong>后台线程按配置的间隔运行，默认设置为一个小时。 在检查后立即达到其TTL期限的数据不会马上删除， 而是等到一个小时后下一个检查间隔才会被删除。</p><p>如将间隔设置为10分钟，在<code>alluxio-site.properties</code>添加以下配置：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">alluxio.master.ttl.checker.interval</span>=<span class="string">10m</span></span><br></pre></td></tr></table></figure><p>请参考<a href="https://docs.alluxio.io/os/user/stable/cn/operation/Configuration.html" target="_blank" rel="noopener">配置页</a> CN以获取有关设置Alluxio配置的更多详细信息。</p><h5 id="3-3-4-1-API"><a href="#3-3-4-1-API" class="headerlink" title="3.3.4.1 API"></a>3.3.4.1 API</h5><p>有两种设置路径的TTL属性的方法。</p><blockquote><ol><li>通过Alluxio shell命令行</li><li>每个元数据加载或文件创建被动设置</li></ol></blockquote><p>TTL API如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SetTTL(path，duration，action)</span><br><span class="line">`path` Alluxio命名空间中的路径</span><br><span class="line">`duration`TTL动作生效前的毫秒数，这会覆盖任何先前的设置</span><br><span class="line">`action`生存时间过去后要执行的`action`。</span><br><span class="line"> `FREE` 将导致文件从Alluxio存储中删除释放，无论其目前的状态如何。</span><br><span class="line"> `DELETE` 将导致文件从Alluxio命名空间和底层存储中删除。注意:`DELETE`是某些命令的默认设置，它将导致文件被永久删除。</span><br></pre></td></tr></table></figure><h5 id="3-3-4-2-命令行用法"><a href="#3-3-4-2-命令行用法" class="headerlink" title="3.3.4.2 命令行用法"></a>3.3.4.2 命令行用法</h5><p>了解如何使用<code>setTtl</code>命令在Alluxio shell中修改TTL属性参阅详细的 <a href="https://docs.alluxio.io/os/user/stable/cn/operation/User-CLI.html#setttl" target="_blank" rel="noopener">命令行文档</a>。</p><h5 id="3-3-4-3-Alluxio中文件上的被动TTL设置"><a href="#3-3-4-3-Alluxio中文件上的被动TTL设置" class="headerlink" title="3.3.4.3 Alluxio中文件上的被动TTL设置"></a>3.3.4.3 Alluxio中文件上的被动TTL设置</h5><p>Alluxio客户端可以配置为只要在Alluxio命名空间添加新文件时就添加TTL属性。 当预期用户是临时使用文件情况下，被动TTL很有用 ，但它不灵活，因为来自同一客户端的所有请求将继承 相同的TTL属性。</p><p>被动TTL通过以下选项配置:</p><ul><li><code>alluxio.user.file.create.ttl</code>：在Alluxio中文件上设置的TTL持续时间。 默认情况下，未设置TTL持续时间。</li><li><code>alluxio.user.file.create.ttl.action</code>：对文件设置的TTL到期后的操作 在Alluxio中。<strong>注意：默认情况下，此操作为“DELETE”，它将导致文件永久被删除。</strong></li></ul><p>TTL默认情况下处于不使用状态，仅当客户有严格数据访问模式才启用。</p><p>例如，要3分钟后删除由<code>runTests</code>创建的文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio runTests -Dalluxio.user.file.create.ttl=3m \</span></span><br><span class="line">  -Dalluxio.user.file.create.ttl.action=DELETE</span><br></pre></td></tr></table></figure><p>对于这个例子，确保<code>alluxio.master.ttl.checker.interval</code>被设定为短间隔，例如一分钟，以便master能快速识别过期文件。</p><h3 id="3-4-在Alluxio中管理数据复制"><a href="#3-4-在Alluxio中管理数据复制" class="headerlink" title="3.4 在Alluxio中管理数据复制"></a>3.4 在Alluxio中管理数据复制</h3><h4 id="3-4-1-被动复制"><a href="#3-4-1-被动复制" class="headerlink" title="3.4.1 被动复制"></a>3.4.1 被动复制</h4><p>与许多分布式文件系统一样，Alluxio中的每个文件都包含一个或多个分布在集群中存储的存储块。默认情况下，Alluxio可以根据工作负载和存储容量自动调整不同块的复制级别。例如，当更多的客户以类型<code>CACHE</code>或<code>CACHE_PROMOTE</code>请求来读取此块时Alluxio可能会创建此特定块更多副本。当较少使用现有副本时，Alluxio可能会删除一些不常用现有副来为经常访问的数据征回空间(<a href="https://docs.alluxio.io/os/user/stable/cn/core-services/Caching.html#block-annotation-policies" target="_blank" rel="noopener">块注释策略</a>)。 在同一文件中不同的块可能根据访问频率不同而具有不同数量副本。</p><p>默认情况下，此复制或征回决定以及相应的数据传输对访问存储在Alluxio中数据的用户和应用程序完全透明。</p><h4 id="3-4-2-主动复制"><a href="#3-4-2-主动复制" class="headerlink" title="3.4.2 主动复制"></a>3.4.2 主动复制</h4><p>除了动态复制调整之外，Alluxio还提供API和命令行 界面供用户明确设置文件的复制级别目标范围。 尤其是，用户可以在Alluxio中为文件配置以下两个属性:</p><ol><li><code>alluxio.user.file.replication.min</code>是此文件的最小副本数。 默认值为0，即在默认情况下，Alluxio可能会在文件变冷后从Alluxio管理空间完全删除该文件。 通过将此属性设置为正整数，Alluxio 将定期检查此文件中所有块的复制级别。当某些块的复制数不足时，Alluxio不会删除这些块中的任何一个，而是主动创建更多副本以恢复其复制级别。</li><li><code>alluxio.user.file.replication.max</code>是最大副本数。一旦文件该属性设置为正整数，Alluxio将检查复制级别并删除多余的副本。将此属性设置为-1为不设上限(默认情况)，设置为0以防止在Alluxio中存储此文件的任何数据。注意，<code>alluxio.user.file.replication.max</code>的值必须不少于<code>alluxio.user.file.replication.min</code>。</li></ol><p>例如，用户可以最初使用至少两个副本将本地文件<code>/path/to/file</code>复制到Alluxio:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio fs -Dalluxio.user.file.replication.min=2 \</span></span><br><span class="line">copyFromLocal /path/to/file /file</span><br></pre></td></tr></table></figure><p>接下来，设置<code>/file</code>的复制级别区间为3到5。需要注意的是，在后台进程中完成新的复制级别范围设定后此命令将马上返回，实现复制目标是异步完成的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio fs setReplication --min 3 --max 5 /file</span></span><br></pre></td></tr></table></figure><p>设置<code>alluxio.user.file.replication.max</code>为无上限。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio fs setReplication --max -1 /file</span></span><br></pre></td></tr></table></figure><p>重复递归复制目录<code>/dir</code>下所有文件复制级别(包括其子目录）使用<code>-R</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio fs setReplication --min 3 --max -5 -R /dir</span></span><br></pre></td></tr></table></figure><p>要检查的文件的目标复制水平，运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio fs <span class="built_in">stat</span> /foo</span></span><br></pre></td></tr></table></figure><p>并在输出中查找<code>replicationMin</code>和<code>replicationMax</code>字段。</p><h3 id="3-5-检查Alluxio缓存容量和使用情况"><a href="#3-5-检查Alluxio缓存容量和使用情况" class="headerlink" title="3.5 检查Alluxio缓存容量和使用情况"></a>3.5 检查Alluxio缓存容量和使用情况</h3><p>Alluxio shell命令<code>fsadmin report</code>提供可用空间的简短摘要以及其他有用的信息。输出示例如下:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio fsadmin report</span></span><br><span class="line">Alluxio cluster summary:</span><br><span class="line">    Master Address: hadoop2:19998</span><br><span class="line">    Web Port: 19999</span><br><span class="line">    Rpc Port: 19998</span><br><span class="line">    Started: 12-10-2021 18:34:50:977</span><br><span class="line">    Uptime: 2 day(s), 16 hour(s), 47 minute(s), and 46 second(s)</span><br><span class="line">    Version: 2.6.2</span><br><span class="line">    Safe Mode: false</span><br><span class="line">    Zookeeper Enabled: true</span><br><span class="line">    Zookeeper Addresses:</span><br><span class="line">        hadoop1:2181</span><br><span class="line">        hadoop2:2181</span><br><span class="line">        hadoop3:2181</span><br><span class="line">    Live Workers: 3</span><br><span class="line">    Lost Workers: 0</span><br><span class="line">    Total Capacity: 7.40GB</span><br><span class="line">        Tier: MEM  Size: 7.40GB</span><br><span class="line">    Used Capacity: 1804B</span><br><span class="line">        Tier: MEM  Size: 1804B</span><br><span class="line">    Free Capacity: 7.40GB</span><br></pre></td></tr></table></figure><p><strong>Alluxio shell还允许用户检查Alluxio缓存中多少空间可用和在用。</strong></p><p>获得Alluxio缓存总使用字节数运行:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio fs getUsedBytes</span></span><br></pre></td></tr></table></figure><p>获得Alluxio缓存以字节为单位的总容量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/alluxio fs getCapacityBytes</span></span><br></pre></td></tr></table></figure><p>Alluxio master web界面为用户提供了集群的可视化总览包括已用多少存储空间。可以在<code>http:/{MASTER_IP}:${alluxio.master.web.port}/</code>中找到。 有关Alluxio Web界面的更多详细信息可以在 <a href="https://docs.alluxio.io/os/user/stable/en/operation/Web-Interface.html" target="_blank" rel="noopener">相关文档</a> 中找到。</p><h2 id="四、Alluxio-高可用集群部署"><a href="#四、Alluxio-高可用集群部署" class="headerlink" title="四、Alluxio 高可用集群部署"></a>四、Alluxio 高可用集群部署</h2><p><strong>服务规划</strong></p><table><thead><tr><th>主机名</th><th>Alluxio 角色</th><th>依赖服务</th></tr></thead><tbody><tr><td>hadoop1</td><td>master、worker</td><td>JDK1.8、zookeeper</td></tr><tr><td>hadoop2</td><td>master、worker</td><td>JDK1.8、zookeeper</td></tr><tr><td>hadoop3</td><td>worker</td><td>JDK1.8、zookeeper</td></tr></tbody></table><p><strong>前置条件</strong></p><blockquote><p>服务器免密登录（确保master可以免密登录到worker节点）<br>zk环境是可用的</p><p>Hadoop HDFS是可用的，保存master检查点信息用于HA快速恢复</p></blockquote><h3 id="4-1-安装-jdk"><a href="#4-1-安装-jdk" class="headerlink" title="4.1 安装 jdk"></a>4.1 安装 jdk</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/jdk-8u301-linux-x64.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ vim .bash_profile</span><br><span class="line">export JAVA_HOME=/home/hadoop/jdk1.8.0_301</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~]$ source .bash_profile</span><br></pre></td></tr></table></figure><h3 id="4-2-配置-Alluxio"><a href="#4-2-配置-Alluxio" class="headerlink" title="4.2 配置 Alluxio"></a>4.2 配置 Alluxio</h3><p><strong>准备Alluxio</strong> ，在各节点分别下载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://downloads.alluxio.io/downloads/files/2.6.2/alluxio-2.6.2-bin.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/alluxio-2.6.2-bin.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ cd alluxio-2.6.2/</span><br></pre></td></tr></table></figure><p><strong>编辑 alluxio-site.properties 配置文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/alluxio-2.6.2]$ cp conf/alluxio-site.properties.template conf/alluxio-site.properties</span><br><span class="line">[hadoop@hadoop1 ~/alluxio-2.6.2]$ vim conf/alluxio-site.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 主节点名称</span></span><br><span class="line">alluxio.master.hostname=hadoop1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 高可用配置</span></span><br><span class="line">alluxio.zookeeper.enabled=true</span><br><span class="line">alluxio.zookeeper.address=hadoop1:2181,hadoop2:2181,hadoop3:2181</span><br><span class="line">alluxio.master.journal.type=UFS</span><br><span class="line">alluxio.master.journal.folder=hdfs://hadoop2:8020/alluxio/journal/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Worker properties <span class="comment">#有几个层级的存储就写几，比如我们要配置的只有level0一层，就写1</span></span></span><br><span class="line">alluxio.worker.ramdisk.size=16GB</span><br><span class="line">alluxio.worker.tieredstore.levels=1</span><br><span class="line">alluxio.worker.tieredstore.level0.alias=MEM</span><br><span class="line">alluxio.worker.tieredstore.level0.dirs.path=/home/hadoop/alluxio-2.6.2/ramdisk</span><br></pre></td></tr></table></figure><p><strong>编辑 masters 配置文件</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~/alluxio-2.6.2]<span class="formula">$ vim conf/masters</span></span><br><span class="line"><span class="formula">hadoop1</span></span><br><span class="line"><span class="formula">hadoop2</span></span><br></pre></td></tr></table></figure><p><strong>编辑 masters 配置文件</strong></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~/alluxio-2.6.2]<span class="formula">$ vim conf/workers</span></span><br><span class="line"><span class="formula">hadoop1</span></span><br><span class="line"><span class="formula">hadoop2</span></span><br><span class="line"><span class="formula">hadoop3</span></span><br></pre></td></tr></table></figure><p><strong>编辑 alluxio-env.sh 配置文件</strong>，添加java环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/alluxio-2.6.2]$ vim conf/alluxio-env.sh</span><br><span class="line"></span><br><span class="line">JAVA_HOME=/home/hadoop/jdk1.8.0_301</span><br></pre></td></tr></table></figure><p><strong>将配置颁发到其它alluxio节点上</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/alluxio-2.6.2]$ scp conf/* hadoop@hadoop2:~/alluxio-2.6.2/conf/</span><br><span class="line">[hadoop@hadoop1 ~/alluxio-2.6.2]$ scp conf/* hadoop@hadoop3:~/alluxio-2.6.2/conf/</span><br></pre></td></tr></table></figure><blockquote><p>确保master节点的 <code>alluxio.master.hostname</code> 配置为该节点的本机地址（hadoop2节点的 <code>alluxio.master.hostname=hadoop2</code>）</p></blockquote><h3 id="4-3-启动-Alluxio-HA-集群"><a href="#4-3-启动-Alluxio-HA-集群" class="headerlink" title="4.3 启动 Alluxio HA 集群"></a>4.3 启动 Alluxio HA 集群</h3><p><strong>格式 Alluxio</strong>，Alluxio 集群在其中一个主节点中使用以下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/alluxio-2.6.2]$ ./bin/alluxio format</span><br></pre></td></tr></table></figure><p><strong>启动 Alluixo</strong>，在可以进行免密登录的机器上执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/alluxio-2.6.2]$ ./bin/alluxio-start.sh all SudoMount</span><br></pre></td></tr></table></figure><blockquote><p>参数<code>SudoMount</code>表示使用<code>sudo</code>特权将 RamFS 挂载到每个 worker 上（如果尚未挂载）</p></blockquote><p><strong>验证 Alluxio 集群</strong></p><p>要验证 Alluxio 是否正在运行，您可以访问主master 的 web UI。要确定主master可在任意节点使用 <code>alluxio fs masterInfo</code> 查询</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~/alluxio-2.6.2]$ ./bin/alluxio fs masterInfo</span><br><span class="line">Current leader master: hadoop1:19998</span><br><span class="line">All masters: [hadoop1:19998, hadoop2:19998]</span><br></pre></td></tr></table></figure><p><strong>访问 web UI</strong>，master和worker都拥有各自的Web UI页面。master Web界面的默认 端口是19999，worker的端口是30000。</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8716.png" alt="图片16"></p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8717.png" alt="图片17"></p><h3 id="4-4-挂载-HDFS-存储"><a href="#4-4-挂载-HDFS-存储" class="headerlink" title="4.4 挂载 HDFS 存储"></a>4.4 挂载 HDFS 存储</h3><h4 id="4-4-1-配置文件挂载"><a href="#4-4-1-配置文件挂载" class="headerlink" title="4.4.1 配置文件挂载"></a>4.4.1 配置文件挂载</h4><p><strong>编辑Master节点的 alluxio-site.properties 配置文件</strong>，添加以下配置</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~/alluxio-2.6.2]<span class="formula">$ vim conf/alluxio-site.properties</span></span><br><span class="line"><span class="formula">alluxio.master.mount.table.root.ufs=hdfs://hadoop2:8020/alluxio</span></span><br></pre></td></tr></table></figure><blockquote><p>Alluxio默认使用本地文件作为文件系统 <code>alluxio.master.mount.table.root.ufs=${alluxio.work.dir}/underFSStorage</code></p></blockquote><p><strong>重启 Master 服务</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/alluxio-2.6.2]$ ./bin/alluxio-stop.sh master</span><br><span class="line">[hadoop@hadoop2 ~/alluxio-2.6.2]$ ./bin/alluxio-stop.sh master</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~/alluxio-2.6.2]$ ./bin/alluxio-start.sh master</span><br><span class="line">[hadoop@hadoop2 ~/alluxio-2.6.2]$ ./bin/alluxio-start.sh master</span><br></pre></td></tr></table></figure><p><strong>查看挂载的hdfs存储</strong></p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8718.png" alt="图片18"></p><p>现在已经把hdfs的alluxio目录挂载为alluxio的根目录，接下来我们运行alluxio自带的示例程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~/alluxio-2.6.2]$ ./bin/alluxio runTests</span><br></pre></td></tr></table></figure><p>创建完示例文件后可以在alluxio和hdfs查看到了</p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8719.png" alt="图片19"></p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8721.png" alt="图片21"></p><h4 id="4-4-2-命令行挂载"><a href="#4-4-2-命令行挂载" class="headerlink" title="4.4.2 命令行挂载"></a>4.4.2 命令行挂载</h4><p>配置文件挂载，只能挂载到<code>/</code>根目录，如果需要挂载多个hdfs目录或多个hdfs集群，只能通过命令行进行挂载了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 挂载多个hdfs目录</span></span><br><span class="line">[hadoop@hadoop2 ~/alluxio-2.6.2]$ ./bin/alluxio fs mount /hdfs2 hdfs://hadoop2:8020/hbase</span><br><span class="line">[hadoop@hadoop2 ~/alluxio-2.6.2]$ ./bin/alluxio fs mount /hive hdfs://hadoop2:8020/hive</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 挂载多个hdfs集群需要指定hdfs-site.xml,core-site.xml配置文件</span></span><br><span class="line">[hadoop@hadoop2 ~/alluxio-2.6.2]$ ./bin/alluxio fs mount /alluxio hdfs://hadoop2:8020/alluxio \</span><br><span class="line">--option alluxio.underfs.hdfs.configuration=/home/hadoop/hadoop-2.7.2/etc/hadoop/core-site.xml:/home/hadoop/hadoop-2.7.2/etc/hadoop/hdfs-site.xml</span><br><span class="line">[hadoop@hadoop2 ~/alluxio-2.6.2]$ ./bin/alluxio fs mount /druid hdfs://mycluster/druid \</span><br><span class="line">--option alluxio.underfs.hdfs.configuration=/home/hadoop/hadoop-2.7.2/etc/hadoop/core-site.xml:/home/hadoop/hadoop-2.7.2/etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure><p><strong>查看挂载的hdfs存储</strong></p><p><img src="/2021/12/11/alluxio-fen-bu-shi-nei-cun-wen-jian-xi-tong/%E5%9B%BE%E7%89%8722.png" alt="图片22"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;一、Alluxio概览&quot;&gt;&lt;a href=&quot;#一、Alluxio概览&quot; class=&quot;headerlink&quot; title=&quot;一、All
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
      <category term="Alluxio" scheme="http://chenzhonzhou.github.io/tags/Alluxio/"/>
    
  </entry>
  
  <entry>
    <title>Apache Doris和 ClickHouse深度对比</title>
    <link href="http://chenzhonzhou.github.io/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/"/>
    <id>http://chenzhonzhou.github.io/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/</id>
    <published>2021-12-07T08:03:39.000Z</published>
    <updated>2021-12-07T10:54:39.905Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><h2 id="一、背景介绍"><a href="#一、背景介绍" class="headerlink" title="一、背景介绍"></a>一、背景介绍</h2><p>Apache Doris是由百度贡献的开源MPP分析型数据库产品，亚秒级查询响应时间，支持实时数据分析；分布式架构简洁，易于运维，可以支持10PB以上的超大数据集；可以满足多种数据分析需求，例如固定历史报表，实时数据分析，交互式数据分析和探索式数据分析等。</p><p>ClickHouse是俄罗斯的搜索公司Yandex开源的MPP架构的分析引擎，号称比事务数据库快100-1000倍，团队有计算机体系结构的大牛，最大的特色是高性能的向量化执行引擎，而且功能丰富、可靠性高。</p><h2 id="二、差异和选择建议"><a href="#二、差异和选择建议" class="headerlink" title="二、差异和选择建议"></a>二、差异和选择建议</h2><p><strong>Doris 更优的方面</strong></p><ul><li>使用更简单，如建表更简单，SQL标准支持更好， Join性能更好，导数功能更强大；</li><li>运维更简单，如灵活的扩缩容能力，故障节点自动恢复，社区提供的支持更好；</li><li>分布式更强，支持事务和幂等性导数，物化视图自动聚合，查询自动路由，全面元数据管理。</li></ul><p><strong>ClickHouse 更优的方面</strong></p><ul><li>性能更佳，导入性能和单表查询性能更好，同时可靠性更好；</li><li>功能丰富，非常多的表引擎，更多类型和函数支持，更好的聚合函数以及庞大的优化参数选项；</li><li>集群管理工具更多，更好多租户和配额管理，灵活的集群管理，方便的集群间迁移工具。</li></ul><p><strong>那么两者之间如何选择呢？</strong></p><ul><li>业务场景复杂数据规模巨大，希望投入研发力量做定制开发，选ClickHouse</li><li>希望一站式的分析解决方案，少量投入研发资源，选择Doris</li></ul><p>另外， Doris源自在线广告系统，偏交易系统数据分析；ClickHouse起源于网站流量分析服务，偏互联网数据分析，但是这两类场景这两个引擎都可以覆盖。如果说两者不那么强的地方，ClickHouse的问题是使用门槛高、运维成本高和分布式能力太弱，需要较多的定制化和较深的技术实力，Doris的问题是性能差一些可靠性差一些，下面就深入分析两者的差异。</p><h2 id="三、架构分析"><a href="#三、架构分析" class="headerlink" title="三、架构分析"></a>三、架构分析</h2><p>从部署运维、分布式能力、数据导入、查询、存储和使用成本等方面进行对比，这部分会涉及到内核中的设计原理、方案和实现，了解这些原理会有助于理解上文的结论。</p><h3 id="3-1-部署运维"><a href="#3-1-部署运维" class="headerlink" title="3.1 部署运维"></a>3.1 部署运维</h3><p><strong>1）部署和日常运维</strong></p><p>部署指部署集群，安装相关依赖和核心组件，修改配置文件，让集群正常运行起来；运维指日常集群版本更新，配置文件更改、扩缩容等相关事项。集群所需组件如下：</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><ul><li>左侧是Doris的部署架构图，JDBC指接入协议，DNS是域名和请求分发系统。Managerment Panel是管控面。Frontend指前端模块简称FE，包含了SQL解析、查询计划、计划调度、元数据管理等功能，Backend指后端模块简称BE负责存储层、数据读取和写入，另外还有一个BrokerLoad导数组件最好是单独部署。所以，Doris一般只需要FE和BE两个组件。</li><li>右侧是ClickHouse的部署架构图，ClickHouse本身只有一个模块，就是ClickHouse Server，周边有两个模块，如ClickHouseProxy主要是转发请求、配额限制和容灾等，ZooKeeper这块负责分布式DDL和副本间数据同步，ClickHouseCopier负责集群和数据迁移，ClickHouse一般需要Server、ZooKeeper和CHProxy三个组件。</li><li>日常运维如更新版本、更改配置文件两者都需要依赖Ansible或者SaltStack来进行批量更新。两者都有部分配置文件可以热更新，不用重启节点，而且有Session相关参数可以设置可以覆盖配置文件。Doris有较多的SQL命令协助运维，比如增加节点，Doris中Add Backend即可，ClickHouse中需要更改配置文件并下发到各个节点上。</li></ul><p><strong>2）多租户管理</strong></p><p>ClickHouse的权限和Quota的粒度更细，可以很方便的支持多租户使用共享集群。比如可以设置查询内存、查询线程数量、查询超时等，以便来限制查询的大小；同时结合查询并发和一定时间窗口内的查询数量，以便来控制查询数量。多租户的方案，对发展中的业务非常友好，因为使用共享集群资源，可以快速动态调整配额，如果是独占集群资源利用率不高、扩容相对麻烦。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><p><strong>3）集群迁移</strong></p><p>Doris通过内置的backup/restore命令将数据和元数据备份到三方对象存储或者HDFS上，backup可以通过快照的方式完整导出一致性的数据和元数据，并且可以按照分区来实现增量备份，降低备份的成本。在Doris中，有一种变通的迁移集群的方法，把新机器分批加入到已有的集群，然后再把旧机器逐步下线，集群能够自动均衡，这个过程视集群数据量可能会持续数天。</p><p>ClickHouse有几个方法实现数据迁移，数据量大通过自带的Clickhouse-copier工具进行集群间的数据拷贝，实现数据的跨集群迁移，需要手工配置很多信息，我们做了一些完善和改进；数据量小通过SQL命令remote关键字实现跨集群的数据迁移。而官方对实现其他存储介质的备份和恢复的推荐是采用文件系统的snapshot实现，或者可以通过三方工具（<a href="https://github.com/AlexAkulov/clickhouse-backup" target="_blank" rel="noopener">https://github.com/AlexAkulov/clickhouse-backup</a> ）来实现。</p><p><strong>4）扩容/缩容</strong></p><p>Doris支持集群的在线动态扩缩容，通过内置的SQL命令 <code>alter system add/decomission backends</code> 即可进行节点的扩缩容，数据均衡的粒度是tablet，每个tablet大概是数百兆，扩容后表的tablet会自动拷贝到新的BE节点，如果在线扩容，应该小批量去增加BE，避免过于剧烈导致集群不稳定。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%873.png" alt="图片3"></p><p>ClickHouse的扩容缩容复杂且繁琐，目前做不到自动在线操作，需要自研工具支持。扩容时需要部署新的节点，添加新分片和副本到配置文件中，并在新节点上创建元数据，如果是扩副本数据会自动均衡，如果是扩分片，需要手工去做均衡，或自研相关工具，让均衡自动进行。</p><h3 id="3-2-分布式能力"><a href="#3-2-分布式能力" class="headerlink" title="3.2 分布式能力"></a>3.2 分布式能力</h3><p><strong>1）分布式协议和高可用</strong></p><p>Doris在FrontEnd中包含元数据的管理能力，内置了BerkeleyDB JE HA组件，包含选举策略和副本数据同步，提供了FE的高可用方案。FE中管理的元数据也非常丰富，包含节点、集群、库、表和用户信息，以及分区、Tablet等数据信息，也包含事务、后台任务、DDL操作和导数相关任务等信息。</p><p>Doris的 FrontEnd可以部署3个Follwer + n个Observer（n&gt;=0）的方式来实现元数据和访问连接的高可用，Follower参与选主，在有Leader宕机时，会自动的选举出新节点保证读写高可用，Observer是只读的扩展节点，可以水平扩展实现读的扩展。BE通过多副本来实现高可用，一般来说也采取默认的三副本，写入的时候采用Quroum协议保证数据一致性。Doris的元数据和数据多副本存储的，能自动复制具有自动灾备的能力，服务挂了可以自动重启，坏一块盘数据自动均衡，小范围的节点宕机不会影响集群对外的服务，但宕机后数据均衡过程会消耗集群资源，引发短时间的负载过高。架构如下图：</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%874.png" alt="图片4"></p><p>ClickHouse目前版本是基于ZooKeeper来存储元数据，包含分布式的DDL、表和数据Part信息，从元数据丰富程度来说稍弱，因为存储了大量细粒度的文件信息，导致ZooKeeper经常出现性能瓶颈，社区也有基于Raft协议的改进计划。ClickHouse依赖Zookeeper来实现数据的高可用，Zookeeper带来额外的运维复杂性的同时也有性能问题。ClickHouse没有集中的元数据管理，每个节点分别管理，高可用一般依赖业务方来实现。ClickHouse中某个副本节点宕机，对查询和分布式表的导入没有影响，本地表导入要在导数程序中做灾备方案比如选择健康的副本，对DDL操作是有影响的，需要及时处理。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%875.png" alt="图片5"></p><p>在分布式能力这块，Doris在内核侧已经实现，使用的代价更低；而ClickHouse需要依赖于外部配套的措施去保障，使用成本较高。</p><p><strong>2）事务支持</strong></p><p>ACID指事务的原子性、一致性、隔离性和持久化，OLAP的事务体现在几个方面，一是导数，需要保证导数的原子性，同时也要保证明细数据和物化视图的数据一致性；二是元数据的变更，需要保证所有节点的元数据统一变更的强一致性；三是在节点间做数据均衡时，需要保证数据的一致性。</p><p>Doris提供了导入的事务支持，可以保证导数的幂等性，比如数据导入的原子性，如果有其他错误会自动回滚，同时相同标签的数据不会重复导入。基于导入事务的功能，Doris还实现了Flink-connector这样的外部组件可以实现数据导入不丢不重。两者均不支持通用TP场景中的BEGIN/END/COMMIT语义的事务，很明显有事务支持的Doris比无事务支持的ClickHouse要节省很多开发成本，因为在ClickHouse中，这一切都需要外部导数程序来保证。</p><p>ClickHouse不支持事务，需要在外部去做各种校验和检查，在导数这块能保证100万以内的原子性，但是不保证一致性，比如要更新某些字段或者更新物化视图，这个操作是后台异步的，需要显示指定关键字FINAL来查询最终数据，而且其他操作没有事务支持。</p><p>DDL操作两者都是异步的，但是Doris能保证各个节点元数据的一致性，但ClickHouse中保证不了，会出现局部节点元数据和其他节点不一致的情况。</p><h3 id="3-3-数据导入"><a href="#3-3-数据导入" class="headerlink" title="3.3 数据导入"></a>3.3 数据导入</h3><p>Doris中有RoutineLoad、BrokerLoad和StreamLoad等丰富内置的导数方式，这些功能非常好用，虽然无法处理复杂的ETL逻辑，但是支持简单过滤和转换函数，也能容忍少量的数据异常，同时支持ACID和导数幂等性。</p><ul><li>Routineload支持消费Kafka的实时数据，按每批条数、导入间隔和并发数等设置导数参数，用于实时数据的导入；</li><li>Brokerload支持从HDFS上导入数据文件，用于离线导数，速度不是很快；</li><li>Streamload是导数的底层接口，更加高级的功能可以外部程序处理后通过Steamload来导入。</li></ul><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%876.png" alt="图片6"></p><p>ClickHouse中并没有后台导数任务这一概念，它更多的是通过各种引擎去连接到各种存储系统中。导数在1048576条以内是原子的，要么都生效，要么都失败，但是没有类似Doris中事务ID的概念，在Doris中相同的事务ID插入数据是无效的，这也避免了重复的导数，在CH中如果导数重复，只能删除重新导入。CH中比较有特色的是既可以写分布式表又可以写本地表。</p><p>导入性能因为ClickHouse可以导入本地表，而且没有事务的限制，所以导入性能差不多是节点磁盘写入的性能，而Doris的导数受限于只能分布式表的导入，导入性能差一些。</p><p>如果数据量少可以使用OLAP中的导数，数据量大逻辑复杂，一般使用Spark/Flink等外部计算引擎来做ETL和导数功能，主要是导数消耗集群资源。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%877.png" alt="图片7"></p><h3 id="3-4-存储架构"><a href="#3-4-存储架构" class="headerlink" title="3.4 存储架构"></a>3.4 存储架构</h3><p><strong>1）MVCC模型</strong></p><p>Doris的存储部分参考GoogleMesa，采用的MVCC模式，MVCC指Multi-version concurrency control多版本控制，通过版本可以实现事务的两段提交，可以通过版本进行小文件合并，也可以在明细表和物化视图之间实现强一致性。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%878.png" alt="图片8"></p><p>ClickHouse中也是类似，有两个操作，一种是Merge合并小的Part文件到一个大的Part，提升查询性能避免扫描多个小文件，合并过程类似上图。另外一种是Mutation就是在已有的Part中实现数据的变更或元数据的变更，如下图的SQL：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> [db.]<span class="keyword">table</span> <span class="keyword">DELETE</span> <span class="keyword">WHERE</span> filter_expr;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> [db.]<span class="keyword">table</span> <span class="keyword">UPDATE</span> column1 = expr1 [, ...] <span class="keyword">WHERE</span> filter_expr;</span><br></pre></td></tr></table></figure><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%879.png" alt="图片9"></p><p><strong>2）存储结构</strong></p><p>两者都是列存，列存的好处就是：</p><ul><li>分析场景中需要读大量行但是少数几个列，只需要读取参与计算的列即可，极大的减低了IO，加速了查询；</li><li>同一列中的数据属于同一类型，压缩效果显著，节省存储空间，降低了存储成本；</li><li>高压缩比，意味着同等大小的内存能够存放更多数据，系统Cache效果更好。</li></ul><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8710.png" alt="图片10"></p><p>Doris的数据划分方式是Table、Partition、Bucket/Tablet、Segment几个部分，其中Partition代表数据的纵向划分分区一般是日期列，Bucket/Tablet一般指数据的横向切割分桶规则一般为某主键， Segment是具体的存储文件。Segment中包含数据和索引，数据部分包含多个列的数据按列存放，有三种索引：物理索引、稀疏索引和ZoneMap索引。</p><p>ClickHouse中分为DistributeTable、LocalTable、Partition、Shard、Part、Column几个部分，差不多能和Doris对应起来，区别就是CH中每个Column都对应一组数据文件和索引文件，好处就是命中系统Cache性能更高，不好的地方就是IO较高且文件数量较多，另外CH有Count索引，所以Count时命中索引会比较快。</p><p>通过分区分桶的方式可以让用户自定义数据在集群中的数据分布方式，降低数据查询的扫描量，方便集群的管理。分区作为数据管理的手段， Doris支持按照range分区，ClickHouse可以表达式来自定义。Doris可以通过动态分区的配置来按照时间自动创建新的分区，也可以做冷热数据的分级存储。ClickHouse通过distrubute引擎来进行多节点的数据分布，但是因为缺少bucket这一层，会导致集群的迁移扩容比较麻烦， Doris通过分桶的配置可以进一步对数据划分，方便数据的均衡和迁移。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8711.png" alt="图片11"></p><p><strong>3）表引擎/模型</strong></p><p>两者都有典型的表类型（引擎类型）的支持</p><ul><li>Doris：可重复的Duplicated Key就是明细表，按维度聚合的Aggregate Key，唯一主键Unique Key，UniqueKey这个可以视为AggregateKey的特例，另外在这三种基础上可以建立Rollup（上卷），可以理解为物化视图。</li><li>ClickHouse : 主要是MergeTree表引擎家族，主要是ReplicatedMergeTree带副本的、ReplacingMergeTree可以更新数据的和AggregatingMergeTree可以聚合数据，另外还有内存字典表可以加载数据字典、内存表可以加速查询或获得更好写入性能。CH比较特殊地方是分布式表和每个节点的本地表都要单独创建，物化视图无法自动路由。</li></ul><p>另外，Doris新开发的Primary Key模型，对实时更新场景下的读性能进行了深度优化，在支持update语义的同时，避免了Unique key的sort merge开销。在实时update的压力下，查询性能跟是Unique key的3-15倍。类似的，相比ClickHouse的ReplicatedMergeTree，也避免了select final/optimize final的问题。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8712.png" alt="图片12"></p><p><strong>4）数据类型</strong></p><p>ClickHouse中存在较多的复杂类型的支持如Array/Nested/Map/Tuple/Enum等，这些类型能够满足一些特性场景，还是比较好用的。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8713.png" alt="图片13"></p><h3 id="3-5-数据查询"><a href="#3-5-数据查询" class="headerlink" title="3.5 数据查询"></a>3.5 数据查询</h3><p><strong>1）查询架构</strong></p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8714.png" alt="图片14"></p><p>分布式查询指查询分布在多台服务器上的数据，就如同使用一张表一样，分布式Join比较繁琐，Doris的分布式Join有Local join，Broadcast join，Shuffle join，Hash join等方式。ClickHouse只有Local和Broadcast两种Join，这种架构比较简单，也限制了Join SQL的自由度，变通的方式是通过子查询和查询嵌套来实现多级的Join。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8715.png" alt="图片15"></p><p>Doris和ClickHouse都支持向量化执行，向量化简单理解就是一批数据一批数据去执行，可以多行并发执行，同时也提升了CPU Cache命中率。在数据库领域，一直是Codegen和Vectorized并存，如下图是TPC-H的五个测试SQL，纵轴是查询时间，Type是编译执行，TW是向量化执行，可以看出两者在不同场景下，性能表现不一样。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8716.png" alt="图片16"></p><p><strong>2）并发能力</strong></p><p>OLAP因为MPP架构，每一个SQL所有节点都会参与计算，以此来加速海量计算，因此一个集群的并发能力和单台没有太大的区别，所以，OLAP和数据库类似，并不是能够承担极高并发的系统。但是也并非毫无办法，比如通过增加副本数来达到承载较大并发的能力。比如4个分片1个副本，能承担100QPS，那么如果要承担500的QPS，则只需要把副本数扩展到5个副本即可。另外一点很重要的是查询能否利用到Cache，包括ResultCache，Page Cache和CPU Cache，这样并发还能提升一个很大的台阶。</p><p>Doris有两点比较优势，一是副本数的设置是在表级别的，只需要把并发大的表设置副本数多一些即可，当然副本数不能超过集群的节点数，而ClickHouse的副本数设置是集群级别的。</p><p><strong>3）SQL支持</strong></p><p>Doris与MySQL语法兼容，支持SQL99规范以及部分SQL2003新标准(比如窗口函数，Grouping sets)。</p><p>ClickHouse部分支持SQL-2011 标准（<a href="https://clickhouse.tech/docs/en/sql-reference/ansi/" target="_blank" rel="noopener">https://clickhouse.tech/docs/en/sql-reference/ansi/</a> ），但是由于Planner的一些限制，ClickHouse的多表关联需要对SQL做大量改写工作，比如需要手动下推条件到子查询中，所以复杂查询使用不太方便。</p><p>ClickHouse支持支持ODBC、JDBC、HTTP接口，Doris支持JDBC和ODBC接口。</p><p><strong>4）联邦查询</strong></p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8717.png" alt="图片17"></p><p><strong>5）函数支持</strong></p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8718.png" alt="图片18"></p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8719.png" alt="图片19"></p><h3 id="3-6-使用成本"><a href="#3-6-使用成本" class="headerlink" title="3.6 使用成本"></a>3.6 使用成本</h3><p><strong>1）使用成本</strong></p><p>Doris使用成本低，是一个强一致性元数据的系统，导数功能较为完备，查询SQL的标准兼容好无需额外的工作，弹性伸缩能力要好，而ClickHouse则需要做较多工作：</p><ul><li>ZooKeeper存在性能瓶颈导致集群规模不能特别大</li><li>基本无法做到弹性伸缩，纯手工扩缩容工作量巨大且容易出错</li><li>故障节点的容忍度较低，出现一个节点故障会引发某些操作失败</li><li>导数需要外部保证数据不重不丢，导数失败需要删了重导</li><li>元数据需要自己保证各个节点一致性，偶发性的不一致情况较多</li><li>分布式表和本地表有两套表结构，较多用户难以理解</li><li>多表Join SQL需要改写和优化，方言较多几乎是不兼容其他引擎的SQL</li></ul><p>所以，在大规模实施ClickHouse时，需要研发一个比较好用的运维系统的支持，处理大部分的日常运维工作。</p><p><strong>2）代码框架</strong></p><p>Doris整体架构分为FrontEnd和BackEnd，FE由Java编写，BE是C/C++，通信部分是BRPC。FE中包含了元数据、SQL Parser、Optimizer、Planner和Coodinator几个部分，BE中包含写入、存储、索引和查询执行部分。Doris的代码风格整体质量是比较高的，风格统一，有较为完善的单测用例，如果要在Doris上做二次开发，则需要熟悉Java或C++。</p><p>ClickHouse包含ClickHouse Client/Copier/Server这几个比较主要的模块，其中Client是日常使用的命令行客户端，Copier是数据迁移工具，Server是集群核心服务。Server部分包含Parser、Interpreter、Storage、Database、Function等模块。代码整体上是C++11以上的风格，大量使用Poco库，大量使用较新的语言特性。</p><p>因此ClickHouse对二次开发更加友好，技术栈单一，且测试框架完善，模块间互相依赖关系相对较小。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8720.png" alt="图片20"></p><h2 id="四、性能测试"><a href="#四、性能测试" class="headerlink" title="四、性能测试"></a>四、性能测试</h2><p>TPC-DS 测试是大数据领域比较常用的一个测试，24张表、99个SQL，可以生成不同容量的数据，京东内部常用来做不同引擎的对比测试。</p><ul><li>这两个引擎都无法全部支持99个SQL，不支持的部分我们根据各个引擎不同特点，进行手工SQL改写让其能正确执行，Doris改动量较小，ClickHouse的多表关联几乎都要改写；</li><li>为了简化测试过程，我们挑选了9个关联查询的SQL，然后自己构造了9个单表查询的SQL，共18个SQL来测试性能；</li></ul><p><strong>举个例子</strong></p><p>如下是一个典型的多表关联例子，在Doris中不需要做改动，但是在ClickHouse中，需要改为多个Global Inner Join来执行， ClickHouse的多表关联查询一般都需要改写。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#--Doris/DorisDB SQL 1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> i_item_id, <span class="keyword">avg</span>(cs_quantity) agg1, <span class="keyword">avg</span>(cs_list_price) agg2, <span class="keyword">avg</span>(cs_coupon_amt) agg3, <span class="keyword">avg</span>(cs_sales_price) agg4from catalog_sales, customer_demographics, date_dim, item, promotionwhere cs_sold_date_sk = d_date_sk <span class="keyword">and</span>     cs_item_sk = i_item_sk <span class="keyword">and</span>    cs_bill_cdemo_sk = cd_demo_sk <span class="keyword">and</span>    cs_promo_sk = p_promo_sk <span class="keyword">and</span>    cd_gender = <span class="string">'M'</span> <span class="keyword">and</span>     cd_marital_status = <span class="string">'D'</span> <span class="keyword">and</span>    cd_education_status = <span class="string">'Advanced Degree'</span> <span class="keyword">and</span>    (p_channel_email = <span class="string">'N'</span> <span class="keyword">or</span> p_channel_event = <span class="string">'N'</span>) <span class="keyword">and</span> d_year = <span class="number">1998</span> <span class="keyword">group</span> <span class="keyword">by</span> i_item_id <span class="keyword">order</span> <span class="keyword">by</span> i_item_id <span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#--ClickHouse SQL 1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> i_item_id, <span class="keyword">avg</span>(cs_quantity) agg1, <span class="keyword">avg</span>(cs_list_price) agg2, <span class="keyword">avg</span>(cs_coupon_amt) agg3, <span class="keyword">avg</span>(cs_sales_price) agg4 <span class="keyword">from</span> catalog_sales_distglobal <span class="keyword">inner</span> <span class="keyword">join</span> (<span class="keyword">select</span> cd_demo_sk <span class="keyword">from</span> customer_demographics_dist <span class="keyword">where</span> cd_gender = <span class="string">'M'</span> <span class="keyword">and</span> cd_marital_status = <span class="string">'D'</span> <span class="keyword">and</span> cd_education_status = <span class="string">'Advanced Degree'</span> ) <span class="keyword">on</span> cs_bill_cdemo_sk = cd_demo_skglobal <span class="keyword">inner</span> <span class="keyword">join</span> (<span class="keyword">select</span> d_date_sk <span class="keyword">from</span> date_dim_dist <span class="keyword">where</span> d_year = <span class="number">1998</span> ) <span class="keyword">on</span>  cs_sold_date_sk = d_date_skglobal <span class="keyword">inner</span> <span class="keyword">join</span> ( <span class="keyword">select</span> i_item_sk, i_item_id <span class="keyword">from</span> item_dist ) <span class="keyword">on</span> cs_item_sk = i_item_skglobal <span class="keyword">inner</span> <span class="keyword">join</span> ( <span class="keyword">select</span> p_promo_sk <span class="keyword">from</span> promotion_dist <span class="keyword">where</span> p_channel_email = <span class="string">'N'</span> <span class="keyword">or</span> p_channel_event = <span class="string">'N'</span>) <span class="keyword">on</span> cs_promo_sk = p_promo_skgroup <span class="keyword">by</span> i_item_id <span class="keyword">order</span> <span class="keyword">by</span> i_item_id <span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><p><strong>测试环境</strong></p><ul><li>硬件：3台32核/128G内存/HDD磁盘的服务器</li><li>软件：Doris 0.13.1、ClickHouse 21.3.13.1</li><li>配置：3个分片1副本，都是默认配置</li></ul><p><strong>测试总结</strong></p><ul><li>单表性能ClickHouse更好，无论是查询延时还是并发能力</li><li>多表性能Doris优势更明显，特别是复杂Join和大表Join大表的场景，另外ClickHouse需要改写SQL有一些工作量</li></ul><p><strong>单表延时和并发</strong></p><p>单表的SQL都比较简单，大部分是全表分组group by之后avg/sum/count/count distinct的各种聚合，单表查询时间如下，可以看出整体上ClickHouse要明显好一些，同时，为了压测方便我们找了2个延时低的SQL Single4和Single5测试了一下不同并发下的QPS，发现也是ClickHouse更优一些。</p><p>ClickHouse的单表性能好，得益于向量化执行引擎，在数据密集情况下，利用内存的PageCache和CPU的L2 Cache可以大大加速查询过程。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8721.png" alt="图片21"></p><p><img src="https://p6.itc.cn/images01/20211124/be013966ab2b438ebb0a03b88baba823.jpeg" alt="img"></p><p><strong>Join的延时和并发</strong></p><p>从多表测试来看，Doris在Join6、Join7、Join8、Join9要好一些，Join3两者差不多，其他情况ClickHouse好一些。同样，我们挑选了2个延时低的SQL Join8和Join9做了一下并发测试，并发的测试结果和延时表现比较匹配，延时低的SQL测试并发时QPS同样高。</p><p>Doris多表关联有四种Join方式，BroadCast Join，Shuffle Join/Bucket Shuffle Join和Colocation Join，ClickHouse只有Global Join（就是BroadCast Join）和Local Join（对应Colocation Join），因此在大表Join大表时，要把右表广播到所有节点，性能可想而知。</p><p>Doris的执行计划对SQL进行了较多的优化，因此多表关联中的大部分情况，能找到最优的执行方式，因此多表关联性能较好一些，但是也并不是所有的关联SQL都要好。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8722.png" alt="图片22"></p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8723.png" alt="图片23"></p><p><strong>ClickHouse小表不同数据量下延时</strong></p><p>通过上面的测试，大家肯定有疑问，不是说ClickHouse的Join性能不行么，为什么表现并不差呢？因此，贴一个去年做的一组ClickHouse大小表的测试供大家参考，就是用一个大表关联查询不同数据规模的小表，看Join性能情况怎么样。横轴是指小表的不同数据量，纵轴是执行时间。可以看出，因为Join机制不一样，ClickHouse的延时随小表数据量加大梯度更大，ClickHouse小表数据量1000万以内尚可，超过1000万性能比就比较差了。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8724.png" alt="图片24"></p><h2 id="五、对比表格"><a href="#五、对比表格" class="headerlink" title="五、对比表格"></a>五、对比表格</h2><p>上面的对比，是从大的几个方面来进行的，下面是比较详细的对比，绿色指我们觉得比较占优的部分。</p><p><img src="/2021/12/07/apache-doris-he-clickhouse-shen-du-dui-bi/%E5%9B%BE%E7%89%8725.png" alt="图片25"></p><p>原文地址：<a href="https://maimai.cn/article/detail?fid=1671866194&amp;efid=BcCQ6YGVagzGO0V7lbC5nw" target="_blank" rel="noopener">https://maimai.cn/article/detail?fid=1671866194&amp;efid=BcCQ6YGVagzGO0V7lbC5nw</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;一、背景介绍&quot;&gt;&lt;a href=&quot;#一、背景介绍&quot; class=&quot;headerlink&quot; title=&quot;一、背景介绍&quot;&gt;&lt;/a&gt;一、背
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
      <category term="OLAP" scheme="http://chenzhonzhou.github.io/tags/OLAP/"/>
    
  </entry>
  
  <entry>
    <title>Apache Doris 联机分析处理(OLAP)</title>
    <link href="http://chenzhonzhou.github.io/2021/12/07/apache-doris-lian-ji-fen-xi-chu-li-olap/"/>
    <id>http://chenzhonzhou.github.io/2021/12/07/apache-doris-lian-ji-fen-xi-chu-li-olap/</id>
    <published>2021-12-07T02:31:35.000Z</published>
    <updated>2021-12-08T10:58:57.735Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p><a href="https://doris.apache.org/master/zh-CN/" target="_blank" rel="noopener">Apache Doris</a>（原百度 Palo）是一款<strong>基于大规模并行处理（MPP）技术的联机分析处理查询（OLAP）系统</strong>，由百度在 2017 年开源，2018 年 8 月进入 Apache 孵化。</p><p><strong>MPP ( Massively Parallel Processing，大规模并行处理)</strong>，在数据库非共享集群中，每个节点都有独立的磁盘存储系统和内存系统，业务数据根据数据库模型和应用特点划分到各个节点上，每台数据节点通过专用网络或者商业通用网络互相连接，彼此协同计算，作为整体提供数据库服务。非共享数据库集群有完全的可伸缩性、高可用、高性能、优秀的性价比、资源共享等优势。简单来说，MPP 是将任务并行的分散到多个服务器和节点上，在每个节点上计算完成后，将各自部分的结果汇总在一起得到最终的结果 ( 与 Hadoop 相似 )。</p><p><strong>OLAP（On-line Analytical Processing，联机分析处理）</strong>是在基于数据仓库多维模型的基础上实现的面向分析的各类操作的集合。</p><p>Apache Doris 仅需亚秒级响应时间即可获得查询结果，有效地支持实时数据分析。Apache Doris的分布式架构非常简洁，易于运维，并且可以支持10PB以上的超大数据集。Apache Doris可以满足多种数据分析需求，例如固定<strong>历史报表</strong>，<strong>实时数据分析</strong>，<strong>交互式数据分析</strong>和<strong>探索式数据分析</strong>等。让数据分析工作更加简单高效！</p><p>Doris 主要解决 PB 级别的数据量（如果高于 PB 级别，不推荐使用 Doris 解决，可以考虑用 Hive 等工具），解决结构化数据，查询时间一般在<strong>秒级</strong>或<strong>毫秒级</strong>。</p><p><img src="/2021/12/07/apache-doris-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%874.png" alt="图片4"></p><h2 id="二、Doris-架构"><a href="#二、Doris-架构" class="headerlink" title="二、Doris 架构"></a>二、Doris 架构</h2><h3 id="2-1-整体架构"><a href="#2-1-整体架构" class="headerlink" title="2.1 整体架构"></a>2.1 整体架构</h3><p><img src="/2021/12/07/apache-doris-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><p>Doris 的架构很简洁，只设 <strong>FE(Frontend)</strong>、<strong>BE(Backend)</strong>两种角色，不依赖于外部组件，方便部署和运维。</p><blockquote><ul><li>FE：Frontend，即 Doris 的前端节点。主要负责接收和返回客户端请求、元数据以及集群管理、查询计划生成等工作。</li><li>BE：Backend，即 Doris 的后端节点。主要负责数据存储与管理、查询计划执行等工作。</li></ul></blockquote><p>在整个架构中FE，BE都可线性扩展。</p><p>FE 主要有有三个角色，一个是 leader，一个是 follower，还有一个 observer。leader 跟 follower，主要是用来达到元数据的高可用，保证单节点宕机的情况下，元数据能够实时地在线恢复，而不影响整个服务。</p><p>右边 observer 只是用来扩展查询节点，就是说如果在发现集群压力非常大的情况下，需要去扩展整个查询的能力，那么可以加 observer 的节点。observer 不参与任何的写入，只参与读取。</p><p>数据的可靠性由 BE 保证，BE 会对整个数据存储多副本或者是三副本。副本数可根据需求动态调整。</p><h3 id="2-2-元数据"><a href="#2-2-元数据" class="headerlink" title="2.2 元数据"></a>2.2 元数据</h3><p>Doris 采用 Paxos 协议以及 Memory + Checkpoint + Journal 的机制来确保元数据的高性能及高可靠。元数据的每次更新，都首先写入到磁盘的日志文件中，然后再写到内存中，最后定期 checkpoint 到本地磁盘上。相当于是一个纯内存的一个结构，也就是说所有的元数据都会缓存在内存之中，从而保证 FE 在宕机后能够快速恢复元数据，而且不丢失元数据。Leader、follower 和 observer 它们三个构成一个可靠的服务，一般是部署一个 leader 两个 follower，在单机节点故障的时候其实基本上三个就够了，因为 FE 节点毕竟它只存了一份元数据，它的压力不大，所以如果 FE 太多的时候它会去消耗机器资源，所以多数情况下三个就足够了，可以达到一个很高可用的元数据服务。</p><p>在百度内部，一个包含2500张表，100万个分片（300万副本）的集群，元数据在内存中仅占用约 2GB。（当然，查询所使用的中间对象、各种作业信息等内存开销，需要根据实际情况估算。但总体依然维持在一个较低的内存开销范围内。）</p><p>同时，元数据在内存中整体采用树状的层级结构存储，并且通过添加辅助结构，能够快速访问各个层级的元数据信息。</p><p><img src="/2021/12/07/apache-doris-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><p>如上图，Doris 的元数据主要存储4类数据：</p><blockquote><ul><li>用户数据信息。包括数据库、表的 Schema、分片信息等。</li><li>各类作业信息。如导入作业，Clone 作业、SchemaChange 作业等。</li><li>用户及权限信息。</li><li>集群及节点信息。</li></ul></blockquote><h3 id="2-3-数据流"><a href="#2-3-数据流" class="headerlink" title="2.3 数据流"></a>2.3 数据流</h3><p><img src="/2021/12/07/apache-doris-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%873.png" alt="图片3"></p><p>元数据的数据流具体过程如下：</p><ol><li>只有 leader FE 可以对元数据进行写操作。写操作在修改 leader 的内存后，会序列化为一条log，按照 key-value 的形式写入 bdbje。其中 key 为连续的整型，作为 log id，value 即为序列化后的操作日志。</li><li>日志写入 bdbje 后，bdbje 会根据策略（写多数/全写），将日志复制到其他 non-leader 的 FE 节点。non-leader FE 节点通过对日志回放，修改自身的元数据内存镜像，完成与 leader 节点的元数据同步。</li><li>leader 节点的日志条数达到阈值后（默认 10w 条），会启动 checkpoint 线程。checkpoint 会读取已有的 image 文件，和其之后的日志，重新在内存中回放出一份新的元数据镜像副本。然后将该副本写入到磁盘，形成一个新的 image。之所以是重新生成一份镜像副本，而不是将已有镜像写成 image，主要是考虑写 image 加读锁期间，会阻塞写操作。所以每次 checkpoint 会占用双倍内存空间。</li><li>image 文件生成后，leader 节点会通知其他 non-leader 节点新的 image 已生成。non-leader 主动通过 http 拉取最新的 image 文件，来更换本地的旧文件。</li><li>bdbje 中的日志，在 image 做完后，会定期删除旧的日志。</li></ol><blockquote><p>bdbje：<a href="http://www.oracle.com/technetwork/database/berkeleydb/overview/index-093405.html" target="_blank" rel="noopener">Oracle Berkeley DB Java Edition (opens new window)</a>。在 Doris 中，使用 bdbje 完成元数据操作日志的持久化、FE 高可用等功能。</p></blockquote><h3 id="2-3-实现细节"><a href="#2-3-实现细节" class="headerlink" title="2.3 实现细节"></a>2.3 实现细节</h3><h4 id="2-3-1-元数据目录"><a href="#2-3-1-元数据目录" class="headerlink" title="2.3.1 元数据目录"></a>2.3.1 元数据目录</h4><ol><li>元数据目录通过 FE 的配置项 <code>meta_dir</code> 指定。</li><li><code>bdb/</code> 目录下为 bdbje 的数据存放目录。</li><li><code>image/</code> 目录下为 image 文件的存放目录。<ul><li><code>image.[logid]</code> 是最新的 image 文件。后缀 <code>logid</code> 表明 image 所包含的最后一条日志的 id。</li><li><code>image.ckpt</code> 是正在写入的 image 文件，如果写入成功，会重命名为 <code>image.[logid]</code>，并替换掉旧的 image 文件。</li><li><code>VERSION</code> 文件中记录着 <code>cluster_id</code>。<code>cluster_id</code> 唯一标识一个 Doris 集群。是在 leader 第一次启动时随机生成的一个 32 位整型。也可以通过 fe 配置项 <code>cluster_id</code> 来指定一个 cluster id。</li><li><code>ROLE</code> 文件中记录的 FE 自身的角色。只有 <code>FOLLOWER</code> 和 <code>OBSERVER</code> 两种。其中 <code>FOLLOWER</code> 表示 FE 为一个可选举的节点。（注意：即使是 leader 节点，其角色也为 <code>FOLLOWER</code>）</li></ul></li></ol><h4 id="2-3-2-启动流程"><a href="#2-3-2-启动流程" class="headerlink" title="2.3.2 启动流程"></a>2.3.2 启动流程</h4><ol><li><p>FE 第一次启动，如果启动脚本不加任何参数，则会尝试以 leader 的身份启动。在 FE 启动日志中会最终看到 <code>transfer from UNKNOWN to MASTER</code>。</p></li><li><p>FE 第一次启动，如果启动脚本中指定了 <code>-helper</code> 参数，并且指向了正确的 leader FE 节点，那么该 FE 首先会通过 http 向 leader 节点询问自身的角色（即 ROLE）和 cluster_id。然后拉取最新的 image 文件。读取 image 文件，生成元数据镜像后，启动 bdbje，开始进行 bdbje 日志同步。同步完成后，开始回放 bdbje 中，image 文件之后的日志，完成最终的元数据镜像生成。</p><blockquote><p>注1：使用 <code>-helper</code> 参数启动时，需要首先通过 mysql 命令，通过 leader 来添加该 FE，否则，启动时会报错。</p></blockquote><blockquote><p>注2：<code>-helper</code> 可以指向任何一个 follower 节点，即使它不是 leader。</p></blockquote><blockquote><p>注2：bdbje 在同步日志过程中，fe 日志会显示 <code>xxx detached</code>, 此时正在进行日志拉取，属于正常现象。</p></blockquote></li><li><p>FE 非第一次启动，如果启动脚本不加任何参数，则会根据本地存储的 ROLE 信息，来确定自己的身份。同时根据本地 bdbje 中存储的集群信息，获取 leader 的信息。然后读取本地的 image 文件，以及 bdbje 中的日志，完成元数据镜像生成。（如果本地 ROLE 中记录的角色和 bdbje 中记录的不一致，则会报错。）</p></li><li><p>FE 非第一次启动，且启动脚本中指定了 <code>-helper</code> 参数。则和第一次启动的流程一样，也会先去询问 leader 角色。但是会和自身存储的 ROLE 进行比较。如果不一致，则会报错。</p></li></ol><p><strong>元数据读写与同步</strong></p><ol><li><p>用户可以使用 mysql 连接任意一个 FE 节点进行元数据的读写访问。如果连接的是 non-leader 节点，则该节点会将写操作转发给 leader 节点。leader 写成功后，会返回一个 leader 当前最新的 log id。之后，non-leader 节点会等待自身回放的 log id 大于回传的 log id 后，才将命令成功的消息返回给客户端。这种方式保证了任意 FE 节点的 Read-Your-Write 语义。</p><blockquote><p>注：一些非写操作，也会转发给 leader 执行。比如 <code>SHOW LOAD</code> 操作。因为这些命令通常需要读取一些作业的中间状态，而这些中间状态是不写 bdbje 的，因此 non-leader 节点的内存中，是没有这些中间状态的。（FE 之间的元数据同步完全依赖 bdbje 的日志回放，如果一个元数据修改操作不写 bdbje 日志，则在其他 non-leader 节点中是看不到该操作修改后的结果的。）</p></blockquote></li><li><p>leader 节点会启动一个 TimePrinter 线程。该线程会定期向 bdbje 中写入一个当前时间的 key-value 条目。其余 non-leader 节点通过回放这条日志，读取日志中记录的时间，和本地时间进行比较，如果发现和本地时间的落后大于指定的阈值（配置项：<code>meta_delay_toleration_second</code>。写入间隔为该配置项的一半），则该节点会处于<strong>不可读</strong>的状态。此机制解决了 non-leader 节点在长时间和 leader 失联后，仍然提供过期的元数据服务的问题。</p></li><li><p>各个 FE 的元数据只保证最终一致性。正常情况下，不一致的窗口期仅为毫秒级。我们保证同一 session 中，元数据访问的单调一致性。但是如果同一 client 连接不同 FE，则可能出现元数据回退的现象。（但对于批量更新系统，该问题影响很小。）</p></li></ol><h3 id="2-4-宕机恢复"><a href="#2-4-宕机恢复" class="headerlink" title="2.4 宕机恢复"></a>2.4 宕机恢复</h3><ol><li>leader 节点宕机后，其余 follower 会立即选举出一个新的 leader 节点提供服务。</li><li>当多数 follower 节点宕机时，元数据不可写入。当元数据处于不可写入状态下，如果这时发生写操作请求，目前的处理流程是 <strong>FE 进程直接退出</strong>。后续会优化这个逻辑，在不可写状态下，依然提供读服务。</li><li>observer 节点宕机，不会影响任何其他节点的状态。也不会影响元数据在其他节点的读写。</li></ol><h3 id="2-5-核心特性"><a href="#2-5-核心特性" class="headerlink" title="2.5 核心特性"></a>2.5 核心特性</h3><p><strong>MySQL协议兼容</strong></p><p>Doris提供兼容MySQL协议的连接接口，使得用户不必再单独部署新的客户端库或者工具，可以直接使用MySQL的相关库或者工具；由于提供了MySQL接口，也容易与上层应用兼容；用户学习曲线降低，方便用户上手使用。</p><p><strong>大查询高吞吐</strong></p><p>利用MPP架构的优势，使得查询能够分布式的在多个节点并行执行，充分利用集群整体计算资源，提高大查询的吞吐能力。</p><p><strong>高并发小查询</strong></p><p>通过使用分区裁剪、预聚合，谓词下推，向量化执行、异步RPC等技术，Doris可以支持高并发点查询场景。100台集群可达10w QPS。</p><p><strong>数据更新</strong></p><p>Doris 支持按主键删除和更新数据。能够方便的从 MySQL 等事务数据库中同步实时更新的数据。</p><p><strong>高可用和高可靠</strong></p><p>Doris中的数据和元数据都默认使用3副本存储（Leader Node节点和Compute Node节点需各自大于等于3）。在少数节点宕机的情况下，依然可以保证数据的可靠性。 Doris会自动检查和修复损坏的数据，并将请求自动路由到健康的节点，7*24 小时保证数据的可用性。</p><p><strong>水平扩展和数据均衡</strong></p><p>Leader Node节点和Compute Node节点都可以进行横向扩展。用户可以根据计算和存储需要，灵活的对节点进行扩展。其中Compute Node节点在扩展后，Doris会自动根据节点间的负载情况，进行数据分片的自动均衡，无需人工干预。</p><p><strong>物化视图和预聚合引擎</strong></p><p>Doris支持通过物化视图或上卷表的形式对数据预聚合计算后的结果进行存储，从而加速部分聚合类场景的查询效率。同时，Doris能够保证物化视图和基础表之间的数据一致性，从而使得物化视图会查询和导入完全透明。Doris内部会自动根据用户的查询语句，选择合适的物化视图进行数据摄取。</p><p><strong>丰富的数据导入功能和导入事务保证</strong></p><p>Doris支持多种导入方式。不仅支持近实时的流式导入，也支持大批量的数据导入。同时还可以直接订阅和消费kafka中的数据。Doris自身提供导入事务支持，配合导入Label机制，可以保证导入数据的不丢不重和原子一致性。</p><p><strong>高效的列式存储引擎</strong></p><p>Doris采用自研的列式存储格式来提升OLAP领域的查询效率。存储采用字典编码、RLE等多种编码方式，配合列式存储的特点，提供了非常高的数据压缩比，帮助用户节省存储空间。同时，存储格式上提供包括Min/Max智能索引、稀疏索引、布隆过滤器、bitmap倒排索引等多种查询加速技术，进一步提升了查询效率。</p><p><strong>在线表结构修改</strong></p><p>支持在已导入数据的情况下修改表结构，包括增加列、删除列、修改列类型和改变列顺序等操作。变更操作不会影响当前数据库的查询和写入操作。</p><p><strong>丰富的生态</strong></p><p>Doris可以方便的导入存储在对象存储、HDFS或Kafka中的数据。用户也可以通过Spark来直接查询Doris中存储的数据。而Doris也可以通过ODBC读取包括MySQL、PostgreSQL、SQLServer、Oracle等外部数据源的数据。同时，Doris也可以读取Elasticsearch中存储的数据，为Elasticsearch提供强大的分布式SQL查询层。</p><h2 id="三、Doris-编译与部署"><a href="#三、Doris-编译与部署" class="headerlink" title="三、Doris 编译与部署"></a>三、Doris 编译与部署</h2><p>不想编译的话可以下载 Baidu Doris 团队基于 Apache Doris 的百度<a href="https://cloud.baidu.com/doc/PALO/s/Wksis5irl" target="_blank" rel="noopener">开源版本</a></p><h3 id="3-1-编译"><a href="#3-1-编译" class="headerlink" title="3.1 编译"></a>3.1 编译</h3><blockquote><p>使用 Docker 开发镜像编译（推荐）</p></blockquote><h4 id="3-1-1-下载-Docker-镜像"><a href="#3-1-1-下载-Docker-镜像" class="headerlink" title="3.1.1 下载 Docker 镜像"></a>3.1.1 下载 Docker 镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker pull apache/incubator-doris:build-env-1.4.2</span></span><br></pre></td></tr></table></figure><p><strong>注：</strong>针对不同的 Doris 版本，需要下载对应的镜像版本</p><table><thead><tr><th>镜像版本</th><th>commit id</th><th>doris 版本</th></tr></thead><tbody><tr><td>apache/incubator-doris:build-env</td><td>before <a href="https://github.com/apache/incubator-doris/commit/ff0dd0d2daa588f18b6db56f947e813a56d8ec81" target="_blank" rel="noopener">ff0dd0d(opens new window)</a></td><td>0.8.x, 0.9.x</td></tr><tr><td>apache/incubator-doris:build-env-1.1</td><td><a href="https://github.com/apache/incubator-doris/commit/ff0dd0d2daa588f18b6db56f947e813a56d8ec81" target="_blank" rel="noopener">ff0dd0d(opens new window)</a></td><td>0.10.x, 0.11.x</td></tr><tr><td>apache/incubator-doris:build-env-1.2</td><td><a href="https://github.com/apache/incubator-doris/commit/4ef5a8c8560351d7fff7ff8fd51c4c7a75e006a8" target="_blank" rel="noopener">4ef5a8c(opens new window)</a></td><td>0.12.x - 0.14.0</td></tr><tr><td>apache/incubator-doris:build-env-1.3.1</td><td><a href="https://github.com/apache/incubator-doris/commit/ad67dd34a04c1ca960cff38e5b335b30fc7d559f" target="_blank" rel="noopener">ad67dd3(opens new window)</a></td><td>0.14.x</td></tr><tr><td>apache/incubator-doris:build-env-1.4.1</td><td><a href="https://github.com/apache/incubator-doris/commit/24d38614a0f21ed606462816a262c2e1d8273ace" target="_blank" rel="noopener">24d3861 (opens new window)</a>or later</td><td>0.15.x(releasing)</td></tr><tr><td>apache/incubator-doris:build-env-1.4.2</td><td><a href="https://github.com/apache/incubator-doris/commit/a81f4da4e461a54782a96433b746d07be89e6b54" target="_blank" rel="noopener">a81f4da (opens new window)</a>or later</td><td>0.15.x(releasing)</td></tr></tbody></table><p><strong>注意</strong>：</p><blockquote><ol><li>编译镜像 <a href="https://github.com/apache/incubator-doris/blob/master/thirdparty/CHANGELOG.md" target="_blank" rel="noopener">ChangeLog (opens new window)</a>。</li></ol></blockquote><blockquote><ol><li>doris 0.14.0 版本仍然使用apache/incubator-doris:build-env-1.2 编译，之后的代码将使用apache/incubator-doris:build-env-1.3.1。</li></ol></blockquote><blockquote><ol><li>在 build-env-1.3.1 的docker镜像中，同时包含了 OpenJDK 8 和 OpenJDK 11，并且默认使用 OpenJDK 11 编译。请确保编译使用的 JDK 版本和运行时使用的 JDK 版本一致，否则会导致非预期的运行错误。你可以使用在进入编译镜像的容器后，使用以下命令切换默认 JDK 版本：</li></ol><p>切换到 JDK 8：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> alternatives --<span class="built_in">set</span> java java-1.8.0-openjdk.x86_64</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> alternatives --<span class="built_in">set</span> javac java-1.8.0-openjdk.x86_64</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-1.8.0</span></span><br></pre></td></tr></table></figure><p>切换到 JDK 11：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> alternatives --<span class="built_in">set</span> java java-11-openjdk.x86_64</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> alternatives --<span class="built_in">set</span> javac java-11-openjdk.x86_64</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-11</span></span><br></pre></td></tr></table></figure></blockquote><h4 id="3-1-2-运行镜像"><a href="#3-1-2-运行镜像" class="headerlink" title="3.1.2 运行镜像"></a>3.1.2 运行镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run -it apache/incubator-doris:build-env-1.4.2</span></span><br></pre></td></tr></table></figure><p>建议以挂载本地 Doris 源码目录的方式运行镜像，这样编译的产出二进制文件会存储在宿主机中，不会因为镜像退出而消失。</p><p>同时，建议同时将镜像中 maven 的 <code>.m2</code> 目录挂载到宿主机目录，以防止每次启动镜像编译时，重复下载 maven 的依赖库。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run -it -v /your/<span class="built_in">local</span>/.m2:/root/.m2 -v /your/<span class="built_in">local</span>/incubator-doris-DORIS-x.x.x-release/:/root/incubator-doris-DORIS-x.x.x-release/ apache/incubator-doris:build-env-1.4.2</span></span><br></pre></td></tr></table></figure><h4 id="3-1-3-下载源码"><a href="#3-1-3-下载源码" class="headerlink" title="3.1.3 下载源码"></a>3.1.3 下载源码</h4><p>启动镜像后，你应该已经处于容器内。可以通过以下命令下载 Doris 源码（已挂载本地源码目录则不用）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget https://dist.apache.org/repos/dist/dev/incubator/doris/xxx.tar.gz</span></span><br><span class="line">or</span><br><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/apache/incubator-doris.git</span></span><br></pre></td></tr></table></figure><h4 id="3-1-4-编译-Doris"><a href="#3-1-4-编译-Doris" class="headerlink" title="3.1.4 编译 Doris"></a>3.1.4 编译 Doris</h4><p><strong>编译FE，BE</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sh build.sh</span></span><br></pre></td></tr></table></figure><p><strong>注意:</strong></p><blockquote><p>如果你使用的是 <code>build-env-1.4.1</code> 这个环境，第一次编译的时候要使用如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sh build.sh --clean --be --fe --ui</span></span><br></pre></td></tr></table></figure><p>这是因为1.4.1 版本镜像升级了 thrift(0.9 -&gt; 0.13)，需要通过 –clean 命令强制使用新版本的 thrift 生成代码文件，否则会出现不兼容的代码。</p></blockquote><p>编译完成后，产出文件在 <code>output/</code> 目录中。</p><p><strong>编译Broker</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> fs_brokers/apache_hdfs_broker/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sh build.sh</span></span><br></pre></td></tr></table></figure><p>编译完成后，产出文件在 <code>output/</code> 目录中。</p><h3 id="3-2-群集部署"><a href="#3-2-群集部署" class="headerlink" title="3.2 群集部署"></a>3.2 群集部署</h3><h4 id="3-2-1-官方建议配置"><a href="#3-2-1-官方建议配置" class="headerlink" title="3.2.1 官方建议配置"></a>3.2.1 官方建议配置</h4><p><strong>Linux 操作系统版本需求</strong></p><table><thead><tr><th>Linux 系统</th><th>版本</th></tr></thead><tbody><tr><td>CentOS</td><td>7.1 及以上</td></tr><tr><td>Ubuntu</td><td>16.04 及以上</td></tr></tbody></table><p><strong>软件需求</strong></p><table><thead><tr><th>软件</th><th>版本</th></tr></thead><tbody><tr><td>Java</td><td>1.8 及以上</td></tr><tr><td>GCC</td><td>4.8.2 及以上</td></tr></tbody></table><p><strong>开发测试环境</strong></p><table><thead><tr><th>模块</th><th>CPU</th><th>内存</th><th>磁盘</th><th>网络</th><th>实例数量</th></tr></thead><tbody><tr><td>Frontend</td><td>8核+</td><td>8GB+</td><td>SSD 或 SATA，10GB+ *</td><td>千兆网卡</td><td>1</td></tr><tr><td>Backend</td><td>8核+</td><td>16GB+</td><td>SSD 或 SATA，50GB+ *</td><td>千兆网卡</td><td>1-3 *</td></tr></tbody></table><p><strong>生产环境</strong></p><table><thead><tr><th>模块</th><th>CPU</th><th>内存</th><th>磁盘</th><th>网络</th><th>实例数量（最低要求）</th></tr></thead><tbody><tr><td>Frontend</td><td>16核+</td><td>64GB+</td><td>SSD 或 RAID 卡，100GB+ *</td><td>万兆网卡</td><td>1-5 *</td></tr><tr><td>Backend</td><td>16核+</td><td>64GB+</td><td>SSD 或 SATA，100G+ *</td><td>万兆网卡</td><td>10-100 *</td></tr></tbody></table><p>注1：</p><blockquote><ol><li>FE 的磁盘空间主要用于存储元数据，包括日志和 image。通常从几百 MB 到几个 GB 不等。</li><li>BE 的磁盘空间主要用于存放用户数据，总磁盘空间按用户总数据量 * 3（3副本）计算，然后再预留额外 40% 的空间用作后台 compaction 以及一些中间数据的存放。</li><li>一台机器上可以部署多个 BE 实例，但是<strong>只能部署一个 FE</strong>。如果需要 3 副本数据，那么至少需要 3 台机器各部署一个 BE 实例（而不是1台机器部署3个BE实例）。<strong>多个FE所在服务器的时钟必须保持一致（允许最多5秒的时钟偏差）</strong></li><li>测试环境也可以仅适用一个 BE 进行测试。实际生产环境，BE 实例数量直接决定了整体查询延迟。</li><li>所有部署节点关闭 Swap。</li></ol></blockquote><p>注2：FE 节点的数量</p><blockquote><ol><li>FE 角色分为 Follower 和 Observer，（Leader 为 Follower 组中选举出来的一种角色，以下统称 Follower，具体含义见 <a href="https://doris.apache.org/master/zh-CN/internal/metadata-design" target="_blank" rel="noopener">元数据设计文档</a>）。</li><li>FE 节点数据至少为1（1 个 Follower）。当部署 1 个 Follower 和 1 个 Observer 时，可以实现读高可用。当部署 3 个 Follower 时，可以实现读写高可用（HA）。</li><li>Follower 的数量<strong>必须</strong>为奇数，Observer 数量随意。</li><li>根据以往经验，当集群可用性要求很高时（比如提供在线业务），可以部署 3 个 Follower 和 1-3 个 Observer。如果是离线业务，建议部署 1 个 Follower 和 1-3 个 Observer。</li></ol></blockquote><ul><li><strong>通常我们建议 10 ~ 100 台左右的机器，来充分发挥 Doris 的性能（其中 3 台部署 FE（HA），剩余的部署 BE）</strong></li><li><strong>当然，Doris的性能与节点数量及配置正相关。在最少4台机器（一台 FE，三台 BE，其中一台 BE 混部一个 Observer FE 提供元数据备份），以及较低配置的情况下，依然可以平稳的运行 Doris。</strong></li><li><strong>如果 FE 和 BE 混部，需注意资源竞争问题，并保证元数据目录和数据目录分属不同磁盘。</strong></li></ul><h4 id="3-2-2-网络需求"><a href="#3-2-2-网络需求" class="headerlink" title="3.2.2 网络需求"></a>3.2.2 网络需求</h4><p>Doris 各个实例直接通过网络进行通讯。以下表格展示了所有需要的端口</p><table><thead><tr><th>实例名称</th><th>端口名称</th><th>默认端口</th><th>通讯方向</th><th>说明</th></tr></thead><tbody><tr><td>BE</td><td>be_port</td><td>9060</td><td>FE –&gt; BE</td><td>BE 上 thrift server 的端口，用于接收来自 FE 的请求</td></tr><tr><td>BE</td><td>webserver_port</td><td>8040</td><td>BE &lt;–&gt; BE</td><td>BE 上的 http server 的端口</td></tr><tr><td>BE</td><td>heartbeat_service_port</td><td>9050</td><td>FE –&gt; BE</td><td>BE 上心跳服务端口（thrift），用于接收来自 FE 的心跳</td></tr><tr><td>BE</td><td>brpc_port*</td><td>8060</td><td>FE&lt;–&gt;BE, BE &lt;–&gt; BE</td><td>BE 上的 brpc 端口，用于 BE 之间通讯</td></tr><tr><td>FE</td><td>http_port *</td><td>8030</td><td>FE &lt;–&gt; FE，用户</td><td>FE 上的 http server 端口</td></tr><tr><td>FE</td><td>rpc_port</td><td>9020</td><td>BE –&gt; FE, FE &lt;–&gt; FE</td><td>FE 上的 thrift server 端口，每个fe的配置需要保持一致</td></tr><tr><td>FE</td><td>query_port</td><td>9030</td><td>用户</td><td>FE 上的 mysql server 端口</td></tr><tr><td>FE</td><td>edit_log_port</td><td>9010</td><td>FE &lt;–&gt; FE</td><td>FE 上的 bdbje 之间通信用的端口</td></tr><tr><td>Broker</td><td>broker_ipc_port</td><td>8000</td><td>FE –&gt; Broker, BE –&gt; Broker</td><td>Broker 上的 thrift server，用于接收请求</td></tr></tbody></table><blockquote><p>注：</p><ol><li>当部署多个 FE 实例时，要保证 FE 的 http_port 配置相同。</li><li>部署前请确保各个端口在应有方向上的访问权限。</li></ol></blockquote><h4 id="3-2-3-服务规划"><a href="#3-2-3-服务规划" class="headerlink" title="3.2.3 服务规划"></a>3.2.3 服务规划</h4><p><strong>服务规划</strong></p><table><thead><tr><th>主机名</th><th>doris角色</th><th>依赖服务</th></tr></thead><tbody><tr><td>hadoop1</td><td>FE（Frontend）、BE（Backend）</td><td>JDK1.8、GCC4.8.2 及以上</td></tr><tr><td>hadoop2</td><td>BE（Backend）</td><td>JDK1.8、GCC4.8.2 及以上</td></tr><tr><td>hadoop3</td><td>BE（Backend）</td><td>JDK1.8、GCC4.8.2 及以上</td></tr></tbody></table><p>前置条件</p><blockquote><p>JDK和GCC是可用的</p></blockquote><h4 id="3-2-4-IP-绑定"><a href="#3-2-4-IP-绑定" class="headerlink" title="3.2.4 IP 绑定"></a>3.2.4 IP 绑定</h4><p>因为有多网卡的存在，或因为安装过 docker 等环境导致的虚拟网卡的存在，同一个主机可能存在多个不同的 ip。当前 Doris 并不能自动识别可用 IP。所以当遇到部署主机上有多个 IP 时，必须通过 priority_networks 配置项来强制指定正确的 IP。</p><p>priority_networks 是 FE 和 BE 都有的一个配置，配置项需写在 fe.conf 和 be.conf 中。该配置项用于在 FE 或 BE 启动时，告诉进程应该绑定哪个IP。示例如下：</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">priority_networks=10.1.3.0/24</span><br></pre></td></tr></table></figure><p>这是一种 <a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing" target="_blank" rel="noopener">CIDR (opens new window)</a>的表示方法。FE 或 BE 会根据这个配置项来寻找匹配的IP，作为自己的 localIP。</p><p><strong>注意</strong>：当配置完 priority_networks 并启动 FE 或 BE 后，只是保证了 FE 或 BE 自身的 IP 进行了正确的绑定。而在使用 ADD BACKEND 或 ADD FRONTEND 语句中，也需要指定和 priority_networks 配置匹配的 IP，否则集群无法建立。举例：</p><p>BE 的配置为：<code>priority_networks=10.1.3.0/24</code></p><p>但是在 ADD BACKEND 时使用的是：<code>ALTER SYSTEM ADD BACKEND &quot;192.168.0.1:9050&quot;;</code></p><p>则 FE 和 BE 将无法正常通信。</p><p>这时，必须 DROP 掉这个添加错误的 BE，重新使用正确的 IP 执行 ADD BACKEND。</p><p>FE 同理。</p><p>BROKER 当前没有，也不需要 priority_networks 这个选项。Broker 的服务默认绑定在 0.0.0.0 上。只需在 ADD BROKER 时，执行正确可访问的 BROKER IP 即可。</p><h4 id="3-2-5-FE-部署"><a href="#3-2-5-FE-部署" class="headerlink" title="3.2.5 FE 部署"></a>3.2.5 FE 部署</h4><p><strong>拷贝 FE 部署文件到指定节点</strong></p><p>将源码编译生成的 output 下的 fe 文件夹拷贝到 FE 的节点指定部署路径下并进入该目录。</p><p><strong>配置 FE</strong></p><p>配置文件为 <code>conf/fe.conf</code>。其中注意：<code>meta_dir</code>是元数据存放位置。默认值为 <code>${DORIS_HOME}/doris-meta</code>。需<strong>手动创建</strong>该目录。</p><blockquote><p><strong>注意：生产环境强烈建议单独指定目录不要放在Doris安装目录下，最好是单独的磁盘（如果有SSD最好），测试开发环境可以使用默认配置</strong></p></blockquote><p>fe.conf 中 JAVA_OPTS 默认 java 最大堆内存为 4GB，<strong>建议生产环境调整至 8G 以上</strong>。</p><p><strong>启动FE</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/doris]$ sh fe/bin/start_fe.sh --daemon</span><br></pre></td></tr></table></figure><p>FE进程启动进入后台执行。日志默认存放在 log/ 目录下。如启动失败，可以通过查看 log/fe.log 或者 log/fe.out 查看错误信息。</p><p><strong>查看 FE 进程</strong></p><p>如果成功的话，可以通过jps查看到进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/doris]$ jps |grep PaloFe</span><br><span class="line">27404 PaloFe</span><br></pre></td></tr></table></figure><blockquote><p>如需部署多 FE，请参见后面的 <strong>FE 扩容和缩容</strong> 部分</p></blockquote><h4 id="3-2-6-BE-部署"><a href="#3-2-6-BE-部署" class="headerlink" title="3.2.6 BE 部署"></a>3.2.6 BE 部署</h4><p><strong>拷贝 BE 部署文件到所有要部署 BE 的节点</strong></p><p>将源码编译生成的 output 下的 be 文件夹拷贝到 BE 的节点的指定部署路径下。</p><p><strong>修改所有 BE 的配置</strong></p><p>修改 <code>be/conf/be.conf</code>。主要是配置 <code>storage_root_path</code>：数据存放目录。默认在be/storage下，需要<strong>手动创建</strong>该目录。多个路径之间使用英文状态的分号 <code>;</code> 分隔（<strong>最后一个目录后不要加 <code>;</code></strong>）。可以通过路径区别存储目录的介质，HDD或SSD。可以添加容量限制在每个路径的末尾，通过英文状态逗号<code>,</code>隔开。</p><p>示例1如下：</p><blockquote><p><strong>注意：如果是SSD磁盘要在目录后面加上<code>.SSD</code>,HDD磁盘在目录后面加<code>.HDD</code></strong><br><code>storage_root_path=/home/disk1/doris.HDD,50;/home/disk2/doris.SSD,10;/home/disk2/doris</code></p></blockquote><p><strong>说明</strong></p><blockquote><p>/home/disk1/doris.HDD, 50，表示存储限制为50GB, HDD;<br>/home/disk2/doris.SSD 10， 存储限制为10GB，SSD；<br>/home/disk2/doris，存储限制为磁盘最大容量，默认为HDD。</p></blockquote><p>示例2如下：</p><blockquote><p><strong>注意：不论HHD磁盘目录还是SSD磁盘目录，都无需添加后缀，storage_root_path参数里指定medium即可</strong><br><code>storage_root_path=/home/disk1/doris,medium:hdd,capacity:50;/home/disk2/doris,medium:ssd,capacity:50</code></p></blockquote><p><strong>说明</strong></p><blockquote><p>/home/disk1/doris,medium:hdd,capacity:10，表示存储限制为10GB, HHD;<br>/home/disk2/doris,medium:ssd,capacity:50，表示存储限制为50GB, SSD;</p></blockquote><p><strong>BE webserver_port端口配置</strong></p><p>如果 be 部署在 hadoop 集群中，注意调整 be.conf 中的 <code>webserver_port = 8040</code> ,以免造成端口冲突</p><p><strong>在 FE 中添加所有 BE 节点</strong></p><p>BE 节点需要先在 FE 中添加，才可加入集群。可以使用 mysql-client(<a href="https://dev.mysql.com/downloads/mysql/5.7.html" target="_blank" rel="noopener">下载MySQL 5.7 (opens new window)</a>) 连接到 FE：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./mysql-client -h host -P port -uroot</span><br></pre></td></tr></table></figure><p>其中 host 为 FE 所在节点 ip；port 为 fe/conf/fe.conf 中的 query_port；默认使用 root 账户，无密码登录。</p><p>登录后，执行以下命令来添加每一个 BE：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> BACKEND <span class="string">"host:port"</span>;</span><br></pre></td></tr></table></figure><blockquote><p>其中 host 为 BE 所在节点 ip；port 为 be/conf/be.conf 中的 heartbeat_service_port。</p></blockquote><p><strong>启动 BE</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/doris]$ sh be/bin/start_be.sh --daemon</span><br></pre></td></tr></table></figure><p>BE 进程将启动并进入后台执行。日志默认存放在 be/log/ 目录下。如启动失败，可以通过查看 be/log/be.log 或者 be/log/be.out 查看错误信息。</p><p><strong>查看BE状态</strong></p><p>使用 mysql-client 连接到 FE，并执行 <code>SHOW PROC &#39;/backends&#39;;</code> 查看 BE 运行情况。如一切正常，<code>Alive</code> 列应为 <code>true</code>。</p><p><img src="/2021/12/07/apache-doris-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%875.png" alt="图片5"></p><p><strong>访问 FE UI 查询页面</strong> <code>fe.conf</code> 配置的 <code>http_port</code>端口</p><p><img src="/2021/12/07/apache-doris-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%876.png" alt="图片6"></p><h4 id="3-2-7-FS-Broker-部署（可选）"><a href="#3-2-7-FS-Broker-部署（可选）" class="headerlink" title="3.2.7 FS_Broker 部署（可选）"></a>3.2.7 FS_Broker 部署（可选）</h4><p>Broker 以插件的形式，独立于 Doris 部署。如果需要从第三方存储系统导入数据，需要部署相应的 Broker，默认提供了读取 HDFS 和百度云 BOS 的 fs_broker。fs_broker 是无状态的，建议每一个 FE 和 BE 节点都部署一个 Broker。</p><blockquote><ul><li><p>拷贝源码 fs_broker 的 output 目录下的相应 Broker 目录到需要部署的所有节点上。建议和 BE 或者 FE 目录保持同级。</p></li><li><p>修改相应 Broker 配置<br>在相应 broker/conf/ 目录下对应的配置文件中，可以修改相应配置。</p></li><li><p>启动 Broker<br><code>sh bin/start_broker.sh --daemon</code> 启动 Broker。</p></li><li><p>添加 Broker<br>要让 Doris 的 FE 和 BE 知道 Broker 在哪些节点上，通过 sql 命令添加 Broker 节点列表。<br>使用 mysql-client 连接启动的 FE，执行以下命令：<br><code>ALTER SYSTEM ADD BROKER broker_name &quot;host1:port1&quot;,&quot;host2:port2&quot;,...;</code><br>其中 host 为 Broker 所在节点 ip；port 为 Broker 配置文件中的 broker_ipc_port。</p></li><li><p>查看 Broker 状态<br>使用 mysql-client 连接任一已启动的 FE，执行以下命令查看 Broker 状态：<code>SHOW PROC &quot;/brokers&quot;;</code></p></li></ul></blockquote><p><strong>注：在生产环境中，所有实例都应使用守护进程启动，以保证进程退出后，会被自动拉起，如 <a href="http://supervisord.org/" target="_blank" rel="noopener">Supervisor</a>。如需使用守护进程启动，在 0.9.0 及之前版本中，需要修改各个 start_xx.sh 脚本，去掉最后的 &amp; 符号</strong>。从 0.10.0 版本开始，直接调用 <code>sh start_xx.sh</code> 启动即可。也可参考 <a href="https://www.cnblogs.com/lenmom/p/9973401.html" target="_blank" rel="noopener">这里</a></p><h2 id="四、扩容缩容"><a href="#四、扩容缩容" class="headerlink" title="四、扩容缩容"></a>四、扩容缩容</h2><p>Doris 可以很方便的扩容和缩容 FE、BE、Broker 实例。</p><h3 id="4-1-FE-扩容和缩容"><a href="#4-1-FE-扩容和缩容" class="headerlink" title="4.1 FE 扩容和缩容"></a>4.1 FE 扩容和缩容</h3><p>可以通过将 FE 扩容至 3 个以上节点来实现 FE 的高可用。</p><p>用户可以通过 mysql 客户端登陆 Master FE。通过:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> PROC <span class="string">'/frontends'</span>;</span><br></pre></td></tr></table></figure><p>来查看当前 FE 的节点情况。</p><p>也可以通过前端页面连接：<code>http://fe_hostname:fe_http_port/frontend</code> 或者 <code>http://fe_hostname:fe_http_port/system?path=//frontends</code> 来查看 FE 节点的情况。</p><p>以上方式，都需要 Doris 的 root 用户权限。</p><p>FE 节点的扩容和缩容过程，不影响当前系统运行。</p><h4 id="4-1-1-增加-FE-节点"><a href="#4-1-1-增加-FE-节点" class="headerlink" title="4.1.1 增加 FE 节点"></a>4.1.1 增加 FE 节点</h4><p>FE 分为 Leader，Follower 和 Observer 三种角色。 默认一个集群，只能有一个 Leader，可以有多个 Follower 和 Observer。其中 Leader 和 Follower 组成一个 Paxos 选择组，如果 Leader 宕机，则剩下的 Follower 会自动选出新的 Leader，保证写入高可用。Observer 同步 Leader 的数据，但是不参加选举。如果只部署一个 FE，则 FE 默认就是 Leader。</p><p>第一个启动的 FE 自动成为 Leader。在此基础上，可以添加若干 Follower 和 Observer。</p><p>添加 Follower 或 Observer。使用 mysql-client 连接到已启动的 FE，并执行：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> FOLLOWER <span class="string">"host:port"</span>;</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> OBSERVER <span class="string">"host:port"</span>;</span><br></pre></td></tr></table></figure><p>其中 host 为 Follower 或 Observer 所在节点 ip，port 为其配置文件 fe.conf 中的 edit_log_port。</p><p>配置及启动 Follower 或 Observer。Follower 和 Observer 的配置同 Leader 的配置。第一次启动时，需执行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/start_fe.sh --helper host:port --daemon</span><br></pre></td></tr></table></figure><p>其中 host 为 Leader 所在节点 ip, port 为 Leader 的配置文件 fe.conf 中的 edit_log_port。--helper 参数仅在 follower 和 observer 第一次启动时才需要。</p><p>查看 Follower 或 Observer 运行状态。使用 mysql-client 连接到任一已启动的 FE，并执行：<code>SHOW PROC &#39;/frontends&#39;;</code> 可以查看当前已加入集群的 FE 及其对应角色。</p><p><strong>FE 扩容注意事项：</strong></p><blockquote><ol><li>Follower FE（包括 Leader）的数量必须为奇数，建议最多部署 3 个组成高可用（HA）模式即可。</li><li>当 FE 处于高可用部署时（1个 Leader，2个 Follower），我们建议通过增加 Observer FE 来扩展 FE 的读服务能力。当然也可以继续增加 Follower FE，但几乎是不必要的。</li><li>通常一个 FE 节点可以应对 10-20 台 BE 节点。建议总的 FE 节点数量在 10 个以下。而通常 3 个即可满足绝大部分需求。</li><li>helper 不能指向 FE 自身，必须指向一个或多个已存在并且正常运行中的 Master/Follower FE。</li></ol></blockquote><h4 id="4-1-2-删除-FE-节点"><a href="#4-1-2-删除-FE-节点" class="headerlink" title="4.1.2 删除 FE 节点"></a>4.1.2 删除 FE 节点</h4><p>使用以下命令删除对应的 FE 节点：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">DROP</span> FOLLOWER[OBSERVER] <span class="string">"fe_host:edit_log_port"</span>;</span><br></pre></td></tr></table></figure><p><strong>FE 缩容注意事项：</strong></p><blockquote><ol><li>删除 Follower FE 时，确保最终剩余的 Follower（包括 Leader）节点为奇数。</li></ol></blockquote><h3 id="4-2-BE-扩容和缩容"><a href="#4-2-BE-扩容和缩容" class="headerlink" title="4.2 BE 扩容和缩容"></a>4.2 BE 扩容和缩容</h3><p>用户可以通过 mysql-client 登陆 Leader FE。通过:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> PROC <span class="string">'/backends'</span>;</span><br></pre></td></tr></table></figure><p>来查看当前 BE 的节点情况。</p><p>也可以通过前端页面连接：<code>http://fe_hostname:fe_http_port/backend</code> 或者 <code>http://fe_hostname:fe_http_port/system?path=//backends</code> 来查看 BE 节点的情况。</p><p>以上方式，都需要 Doris 的 root 用户权限。</p><p>BE 节点的扩容和缩容过程，不影响当前系统运行以及正在执行的任务，并且不会影响当前系统的性能。数据均衡会自动进行。根据集群现有数据量的大小，集群会在几个小时到1天不等的时间内，恢复到负载均衡的状态。集群负载情况，可以参见 <a href="https://doris.apache.org/master/zh-CN/administrator-guide/operation/tablet-repair-and-balance.html" target="_blank" rel="noopener">Tablet 负载均衡文档</a>。</p><h4 id="4-2-1-增加-BE-节点"><a href="#4-2-1-增加-BE-节点" class="headerlink" title="4.2.1 增加 BE 节点"></a>4.2.1 增加 BE 节点</h4><p>BE 节点的增加方式同 <strong>BE 部署</strong> 一节中的方式，通过 <code>ALTER SYSTEM ADD BACKEND &quot;host:port&quot;;</code> 命令增加 BE 后 <code>sh be/bin/start_be.sh --daemon</code> 启动节点。</p><p><strong>BE 扩容注意事项：</strong></p><blockquote><ol><li>BE 扩容后，Doris 会自动根据负载情况，进行数据均衡，期间不影响使用。</li></ol></blockquote><h4 id="4-2-2-删除-BE-节点"><a href="#4-2-2-删除-BE-节点" class="headerlink" title="4.2.2 删除 BE 节点"></a>4.2.2 删除 BE 节点</h4><p>删除 BE 节点有两种方式：DROP 和 DECOMMISSION</p><p><strong>DROP 语句如下：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">DROP</span> BACKEND <span class="string">"be_host:be_heartbeat_service_port"</span>;</span><br></pre></td></tr></table></figure><p><strong>注意：DROP BACKEND 会直接删除该 BE，并且其上的数据将不能再恢复！！！所以我们强烈不推荐使用 DROP BACKEND 这种方式删除 BE 节点。当你使用这个语句时，会有对应的防误操作提示。</strong></p><p><strong>DECOMMISSION 语句如下：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> DECOMMISSION BACKEND <span class="string">"be_host:be_heartbeat_service_port"</span>;</span><br></pre></td></tr></table></figure><p>DECOMMISSION 命令说明：</p><blockquote><ol><li>该命令用于安全删除 BE 节点。命令下发后，Doris 会尝试将该 BE 上的数据向其他 BE 节点迁移，当所有数据都迁移完成后，Doris 会自动删除该节点。</li><li>该命令是一个异步操作。执行后，可以通过 <code>SHOW PROC &#39;/backends&#39;;</code> 看到该 BE 节点的 isDecommission 状态为 true。表示该节点正在进行下线。</li><li>该命令<strong>不一定执行成功</strong>。比如剩余 BE 存储空间不足以容纳下线 BE 上的数据，或者剩余机器数量不满足最小副本数时，该命令都无法完成，并且 BE 会一直处于 isDecommission 为 true 的状态。</li><li>DECOMMISSION 的进度，可以通过 <code>SHOW PROC &#39;/backends&#39;;</code> 中的 TabletNum 查看，如果正在进行，TabletNum 将不断减少。</li><li>该操作可以通过:<br><code>CANCEL DECOMMISSION BACKEND &quot;be_host:be_heartbeat_service_port&quot;;</code><br>命令取消。取消后，该 BE 上的数据将维持当前剩余的数据量。后续 Doris 重新进行负载均衡</li></ol></blockquote><p><strong>对于多租户部署环境下，BE 节点的扩容和缩容，请参阅 <a href="https://doris.apache.org/master/zh-CN/administrator-guide/operation/multi-tenant.html" target="_blank" rel="noopener">多租户设计文档</a>。</strong></p><h3 id="4-3-Broker-扩容缩容"><a href="#4-3-Broker-扩容缩容" class="headerlink" title="4.3 Broker 扩容缩容"></a>4.3 Broker 扩容缩容</h3><p>Broker 实例的数量没有硬性要求。通常每台物理机部署一个即可。Broker 的添加和删除可以通过以下命令完成：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> BROKER broker_name <span class="string">"broker_host:broker_ipc_port"</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">DROP</span> BROKER broker_name <span class="string">"broker_host:broker_ipc_port"</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">DROP</span> <span class="keyword">ALL</span> BROKER broker_name;</span><br></pre></td></tr></table></figure><p>Broker 是无状态的进程，可以随意启停。当然，停止后，正在其上运行的作业会失败，重试即可。</p><h2 id="五、最佳实践"><a href="#五、最佳实践" class="headerlink" title="五、最佳实践"></a>五、最佳实践</h2><h3 id="5-1-建表"><a href="#5-1-建表" class="headerlink" title="5.1 建表"></a>5.1 建表</h3><h4 id="5-1-1-数据模型选择"><a href="#5-1-1-数据模型选择" class="headerlink" title="5.1.1 数据模型选择"></a>5.1.1 数据模型选择</h4><p>Doris 数据模型上目前分为三类: AGGREGATE KEY, UNIQUE KEY, DUPLICATE KEY。三种模型中数据都是按KEY进行排序。</p><p><strong>5.1.1.1 AGGREGATE KEY</strong></p><p>AGGREGATE KEY相同时，新旧记录进行聚合，目前支持的聚合函数有SUM, MIN, MAX, REPLACE。<br>AGGREGATE KEY模型可以提前聚合数据, 适合报表和多维分析业务。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> site_visit</span><br><span class="line">(</span><br><span class="line">    siteid      <span class="built_in">INT</span>,</span><br><span class="line">    city        <span class="built_in">SMALLINT</span>,</span><br><span class="line">    username    <span class="built_in">VARCHAR</span>(<span class="number">32</span>),</span><br><span class="line">    pv <span class="built_in">BIGINT</span>   <span class="keyword">SUM</span> <span class="keyword">DEFAULT</span> <span class="string">'0'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">AGGREGATE</span> <span class="keyword">KEY</span>(siteid, city, username)</span><br><span class="line"><span class="keyword">DISTRIBUTED</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(siteid) BUCKETS <span class="number">10</span>;</span><br></pre></td></tr></table></figure><p><strong>5.1.1.2 UNIQUE KEY</strong></p><p>UNIQUE KEY 相同时，新记录覆盖旧记录。目前 UNIQUE KEY 实现上和 AGGREGATE KEY 的 REPLACE 聚合方法一样，二者本质上相同。适用于有更新需求的分析业务。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sales_order</span><br><span class="line">(</span><br><span class="line">    orderid     <span class="built_in">BIGINT</span>,</span><br><span class="line">    <span class="keyword">status</span>      <span class="built_in">TINYINT</span>,</span><br><span class="line">    username    <span class="built_in">VARCHAR</span>(<span class="number">32</span>),</span><br><span class="line">    amount      <span class="built_in">BIGINT</span> <span class="keyword">DEFAULT</span> <span class="string">'0'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">UNIQUE</span> <span class="keyword">KEY</span>(orderid)</span><br><span class="line"><span class="keyword">DISTRIBUTED</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(orderid) BUCKETS <span class="number">10</span>;</span><br></pre></td></tr></table></figure><p><strong>5.1.1.3 DUPLICATE KEY</strong></p><p>只指定排序列，相同的行不会合并。适用于数据无需提前聚合的分析业务。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> session_data</span><br><span class="line">(</span><br><span class="line">    visitorid   <span class="built_in">SMALLINT</span>,</span><br><span class="line">    sessionid   <span class="built_in">BIGINT</span>,</span><br><span class="line">    visittime   DATETIME,</span><br><span class="line">    city        <span class="built_in">CHAR</span>(<span class="number">20</span>),</span><br><span class="line">    province    <span class="built_in">CHAR</span>(<span class="number">20</span>),</span><br><span class="line">    ip          <span class="built_in">varchar</span>(<span class="number">32</span>),</span><br><span class="line">    brower      <span class="built_in">CHAR</span>(<span class="number">20</span>),</span><br><span class="line">    <span class="keyword">url</span>         <span class="built_in">VARCHAR</span>(<span class="number">1024</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">DUPLICATE</span> <span class="keyword">KEY</span>(visitorid, sessionid)</span><br><span class="line"><span class="keyword">DISTRIBUTED</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(sessionid, visitorid) BUCKETS <span class="number">10</span>;</span><br></pre></td></tr></table></figure><h4 id="5-1-2-大宽表与-Star-Schema"><a href="#5-1-2-大宽表与-Star-Schema" class="headerlink" title="5.1.2 大宽表与 Star Schema"></a>5.1.2 大宽表与 Star Schema</h4><p>业务方建表时, 为了和前端业务适配, 往往不对维度信息和指标信息加以区分, 而将 Schema 定义成大宽表。对于 Doris 而言, 这类大宽表往往性能不尽如人意:</p><ul><li>Schema 中字段数比较多, 聚合模型中可能 key 列比较多, 导入过程中需要排序的列会增加。</li><li>维度信息更新会反应到整张表中，而更新的频率直接影响查询的效率。</li></ul><p>使用过程中，建议用户尽量使用 Star Schema 区分维度表和指标表。频繁更新的维度表也可以放在 MySQL 外部表中。而如果只有少量更新, 可以直接放在 Doris 中。在 Doris 中存储维度表时，可对维度表设置更多的副本，提升 Join 的性能。</p><h4 id="5-1-3-分区和分桶"><a href="#5-1-3-分区和分桶" class="headerlink" title="5.1.3 分区和分桶"></a>5.1.3 分区和分桶</h4><p>Doris 支持两级分区存储, 第一层为分区(partition)，目前支持 RANGE 分区和 LIST 分区两种类型, 第二层为 HASH 分桶(bucket)。</p><p><strong>5.1.3.1 分区(partition)</strong></p><p>分区用于将数据划分成不同区间, 逻辑上可以理解为将原始表划分成了多个子表。可以方便的按分区对数据进行管理，例如，删除数据时，更加迅速。</p><p>5.1.3.1.1 RANGE分区</p><p>业务上，多数用户会选择采用按时间进行partition, 让时间进行partition有以下好处：</p><ul><li>可区分冷热数据；</li><li>可用上Doris分级存储(SSD + SATA)的功能。</li></ul><p>5.1.3.1.2 LIST分区</p><p>业务上，用户可以选择城市或者其他枚举值进行partition。</p><p><strong>5.1.3.2 HASH分桶(bucket)</strong></p><p>根据hash值将数据划分成不同的 bucket。</p><ul><li>建议采用区分度大的列做分桶, 避免出现数据倾斜；</li><li>为方便数据恢复, 建议单个 bucket 的 size 不要太大, 保持在 10GB 以内, 所以建表或增加 partition 时请合理考虑 bucket 数目, 其中不同 partition 可指定不同的 buckets 数。</li></ul><h4 id="5-1-4-稀疏索引和-Bloom-Filter"><a href="#5-1-4-稀疏索引和-Bloom-Filter" class="headerlink" title="5.1.4 稀疏索引和 Bloom Filter"></a>5.1.4 稀疏索引和 Bloom Filter</h4><p>Doris对数据进行有序存储, 在数据有序的基础上为其建立稀疏索引,索引粒度为 block(1024行)。</p><p>稀疏索引选取 schema 中固定长度的前缀作为索引内容, 目前 Doris 选取 36 个字节的前缀作为索引。</p><ul><li>建表时建议将查询中常见的过滤字段放在 Schema 的前面, 区分度越大，频次越高的查询字段越往前放。</li><li>这其中有一个特殊的地方,就是 varchar 类型的字段。varchar 类型字段只能作为稀疏索引的最后一个字段。索引会在 varchar 处截断, 因此 varchar 如果出现在前面，可能索引的长度可能不足 36 个字节。具体可以参阅 <a href="https://doris.apache.org/master/zh-CN/getting-started/data-model-rollup.html" target="_blank" rel="noopener">数据模型、ROLLUP 及前缀索引</a>。</li><li>除稀疏索引之外, Doris还提供bloomfilter索引, bloomfilter索引对区分度比较大的列过滤效果明显。 如果考虑到varchar不能放在稀疏索引中, 可以建立bloomfilter索引。</li></ul><h4 id="5-1-5-物化视图-rollup"><a href="#5-1-5-物化视图-rollup" class="headerlink" title="5.1.5 物化视图(rollup)"></a>5.1.5 物化视图(rollup)</h4><p>Rollup 本质上可以理解为原始表(Base Table)的一个物化索引。建立 Rollup 时可只选取 Base Table 中的部分列作为 Schema。Schema 中的字段顺序也可与 Base Table 不同。</p><p>下列情形可以考虑建立 Rollup：</p><p><strong>5.1.5.1 Base Table 中数据聚合度不高</strong></p><p>这一般是因 Base Table 有区分度比较大的字段而导致。此时可以考虑选取部分列，建立 Rollup。</p><p>如对于 <code>site_visit</code> 表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">site_visit(siteid, city, username, pv)</span><br></pre></td></tr></table></figure><p>siteid 可能导致数据聚合度不高，如果业务方经常根据城市统计pv需求，可以建立一个只有 city, pv 的 Rollup：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> site_visit <span class="keyword">ADD</span> <span class="keyword">ROLLUP</span> rollup_city(city, pv);</span><br></pre></td></tr></table></figure><p><strong>5.1.5.2 Base Table 中的前缀索引无法命中</strong></p><p>这一般是 Base Table 的建表方式无法覆盖所有的查询模式。此时可以考虑调整列顺序，建立 Rollup。</p><p>如对于 session_data 表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session_data(visitorid, sessionid, visittime, city, province, ip, brower, url)</span><br></pre></td></tr></table></figure><p>如果除了通过 visitorid 分析访问情况外，还有通过 brower, province 分析的情形，可以单独建立 Rollup。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> session_data <span class="keyword">ADD</span> <span class="keyword">ROLLUP</span> rollup_brower(brower,province,ip,<span class="keyword">url</span>) <span class="keyword">DUPLICATE</span> <span class="keyword">KEY</span>(brower,province);</span><br></pre></td></tr></table></figure><h3 id="5-2-Schema-Change"><a href="#5-2-Schema-Change" class="headerlink" title="5.2 Schema Change"></a>5.2 Schema Change</h3><p>Doris中目前进行 Schema Change 的方式有三种：Sorted Schema Change，Direct Schema Change, Linked Schema Change。</p><p>2.1. Sorted Schema Change</p><p>改变了列的排序方式，需对数据进行重新排序。例如删除排序列中的一列, 字段重排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> site_visit <span class="keyword">DROP</span> <span class="keyword">COLUMN</span> city;</span><br></pre></td></tr></table></figure><p>2.2. Direct Schema Change: 无需重新排序，但是需要对数据做一次转换。例如修改列的类型，在稀疏索引中加一列等。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> site_visit <span class="keyword">MODIFY</span> <span class="keyword">COLUMN</span> username <span class="built_in">varchar</span>(<span class="number">64</span>);</span><br></pre></td></tr></table></figure><p>2.3. Linked Schema Change: 无需转换数据，直接完成。例如加列操作。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> site_visit <span class="keyword">ADD</span> <span class="keyword">COLUMN</span> click <span class="built_in">bigint</span> <span class="keyword">SUM</span> <span class="keyword">default</span> <span class="string">'0'</span>;</span><br></pre></td></tr></table></figure><p>建表时建议考虑好 Schema，这样在进行 Schema Change 时可以加快速度。</p><h2 id="六、权限管理"><a href="#六、权限管理" class="headerlink" title="六、权限管理"></a>六、权限管理</h2><p>Doris 新的权限管理系统参照了 Mysql 的权限管理机制，做到了表级别细粒度的权限控制，基于角色的权限访问控制，并且支持白名单机制。</p><h3 id="6-1-名词解释"><a href="#6-1-名词解释" class="headerlink" title="6.1 名词解释"></a>6.1 名词解释</h3><ol><li><p>用户标识 user_identity</p><p>在权限系统中，一个用户被识别为一个 User Identity（用户标识）。用户标识由两部分组成：username 和 userhost。其中 username 为用户名，由英文大小写组成。userhost 表示该用户链接来自的 IP。user_identity 以 username@&#39;userhost&#39; 的方式呈现，表示来自 userhost 的 username。<br>user_identity 的另一种表现方式为 username@[&#39;domain&#39;]，其中 domain 为域名，可以通过 DNS 或 BNS（百度名字服务）解析为一组 ip。最终表现为一组 username@&#39;userhost&#39;，所以后面我们统一使用 username@&#39;userhost&#39; 来表示。</p></li><li><p>权限 Privilege</p><p>权限作用的对象是节点、数据库或表。不同的权限代表不同的操作许可。</p></li><li><p>角色 Role</p><p>Doris可以创建自定义命名的角色。角色可以被看做是一组权限的集合。新创建的用户可以被赋予某一角色，则自动被赋予该角色所拥有的权限。后续对角色的权限变更，也会体现在所有属于该角色的用户权限上。</p></li><li><p>用户属性 user_property</p><p>用户属性直接附属于某一用户，而不是用户标识。即 cmy@&#39;192.%&#39; 和 cmy@[&#39;domain&#39;] 都拥有同一组用户属性，该属性属于用户 cmy，而不是 cmy@&#39;192.%&#39; 或 cmy@[&#39;domain&#39;]。</p><p>用户属性包括但不限于： 用户最大连接数、导入集群配置等等。</p></li></ol><h3 id="6-2-支持的操作"><a href="#6-2-支持的操作" class="headerlink" title="6.2 支持的操作"></a>6.2 支持的操作</h3><ol><li>创建用户：CREATE USER</li><li>删除用户：DROP USER</li><li>授权：GRANT</li><li>撤权：REVOKE</li><li>创建角色：CREATE ROLE</li><li>删除角色：DROP ROLE</li><li>查看当前用户权限：SHOW GRANTS</li><li>查看所有用户权限：SHOW ALL GRANTS</li><li>查看已创建的角色：SHOW ROLES</li><li>查看用户属性：SHOW PROPERTY</li></ol><p>关于以上命令的详细帮助，可以通过 mysql 客户端连接 Doris 后，使用 help + command 获取帮助。如 <code>HELP CREATE USER</code>。</p><h3 id="6-3-权限类型"><a href="#6-3-权限类型" class="headerlink" title="6.3 权限类型"></a>6.3 权限类型</h3><p>Doris 目前支持以下几种权限</p><ol><li><p>Node_priv</p><p>节点变更权限。包括 FE、BE、BROKER 节点的添加、删除、下线等操作。目前该权限只能授予 Root 用户。</p></li><li><p>Grant_priv</p><p>权限变更权限。允许执行包括授权、撤权、添加/删除/变更 用户/角色 等操作。</p></li><li><p>Select_priv</p><p>对数据库、表的只读权限。</p></li><li><p>Load_priv</p><p>对数据库、表的写权限。包括 Load、Insert、Delete 等。</p></li><li><p>Alter_priv</p><p>对数据库、表的更改权限。包括重命名 库/表、添加/删除/变更 列、添加/删除 分区等操作。</p></li><li><p>Create_priv</p><p>创建数据库、表、视图的权限。</p></li><li><p>Drop_priv</p><p>删除数据库、表、视图的权限。</p></li><li><p>Usage_priv</p><p>资源的使用权限。</p></li></ol><h3 id="6-4-权限层级"><a href="#6-4-权限层级" class="headerlink" title="6.4 权限层级"></a>6.4 权限层级</h3><p>同时，根据权限适用范围的不同，我们将库表的权限分为以下三个层级：</p><ol><li>GLOBAL LEVEL：全局权限。即通过 GRANT 语句授予的 <code>*.*</code> 上的权限。被授予的权限适用于任意数据库中的任意表。</li><li>DATABASE LEVEL：数据库级权限。即通过 GRANT 语句授予的 <code>db.*</code> 上的权限。被授予的权限适用于指定数据库中的任意表。</li><li>TABLE LEVEL：表级权限。即通过 GRANT 语句授予的 <code>db.tbl</code> 上的权限。被授予的权限适用于指定数据库中的指定表。</li></ol><p>将资源的权限分为以下两个层级：</p><ol><li>GLOBAL LEVEL：全局权限。即通过 GRANT 语句授予的 <code>*</code> 上的权限。被授予的权限适用于资源。</li><li>RESOURCE LEVEL: 资源级权限。即通过 GRANT 语句授予的 <code>resource_name</code> 上的权限。被授予的权限适用于指定资源。</li></ol><h3 id="6-5-ADMIN-GRANT-权限说明"><a href="#6-5-ADMIN-GRANT-权限说明" class="headerlink" title="6.5 ADMIN/GRANT 权限说明"></a>6.5 ADMIN/GRANT 权限说明</h3><p>ADMIN_PRIV 和 GRANT_PRIV 权限同时拥有<strong>授予权限</strong>的权限，较为特殊。这里对和这两个权限相关的操作逐一说明。</p><ol><li>CREATE USER<ul><li>拥有 ADMIN 权限，或任意层级的 GRANT 权限的用户可以创建新用户。</li></ul></li><li>DROP USER<ul><li>只有 ADMIN 权限可以删除用户。</li></ul></li><li>CREATE/DROP ROLE<ul><li>只有 ADMIN 权限可以创建角色。</li></ul></li><li>GRANT/REVOKE<ul><li>拥有 ADMIN 权限，或者 GLOBAL 层级 GRANT 权限的用户，可以授予或撤销任意用户的权限。</li><li>拥有 DATABASE 层级 GRANT 权限的用户，可以授予或撤销任意用户对指定数据库的权限。</li><li>拥有 TABLE 层级 GRANT 权限的用户，可以授予或撤销任意用户对指定数据库中指定表的权限。</li></ul></li><li>SET PASSWORD<ul><li>拥有 ADMIN 权限，或者 GLOBAL 层级 GRANT 权限的用户，可以设置任意用户的密码。</li><li>普通用户可以设置自己对应的 UserIdentity 的密码。自己对应的 UserIdentity 可以通过 <code>SELECT CURRENT_USER();</code> 命令查看。</li><li>拥有非 GLOBAL 层级 GRANT 权限的用户，不可以设置已存在用户的密码，仅能在创建用户时指定密码。</li></ul></li></ol><h3 id="6-6-一些说明"><a href="#6-6-一些说明" class="headerlink" title="6.6 一些说明"></a>6.6 一些说明</h3><ol><li><p>Doris 初始化时，会自动创建如下用户和角色：</p><ol><li>operator 角色：该角色拥有 Node_priv 和 Admin_priv，即对Doris的所有权限。后续某个升级版本中，我们可能会将该角色的权限限制为 Node_priv，即仅授予节点变更权限。以满足某些云上部署需求。</li><li>admin 角色：该角色拥有 Admin_priv，即除节点变更以外的所有权限。</li><li>root@&#39;%&#39;：root 用户，允许从任意节点登陆，角色为 operator。</li><li>admin@&#39;%&#39;：admin 用户，允许从任意节点登陆，角色为 admin。</li></ol></li><li><p>不支持删除或更改默认创建的角色或用户的权限。</p></li><li><p>operator 角色的用户有且只有一个。admin 角色的用户可以创建多个。</p></li><li><p>一些可能产生冲突的操作说明</p><ol><li><p>域名与ip冲突：</p><p>假设创建了如下用户：</p><p>CREATE USER cmy@[&#39;domain&#39;];</p><p>并且授权：</p><p>GRANT SELECT_PRIV ON *.* TO cmy@[&#39;domain&#39;]</p><p>该 domain 被解析为两个 ip：ip1 和 ip2</p><p>假设之后，我们对 cmy@&#39;ip1&#39; 进行一次单独授权：</p><p>GRANT ALTER_PRIV ON <em>.</em> TO cmy@&#39;ip1&#39;;</p><p>则 cmy@&#39;ip1&#39; 的权限会被修改为 SELECT_PRIV, ALTER_PRIV。并且当我们再次变更 cmy@[&#39;domain&#39;] 的权限时，cmy@&#39;ip1&#39; 也不会跟随改变。</p></li><li><p>重复ip冲突：</p><p>假设创建了如下用户：</p><p>CREATE USER cmy@&#39;%&#39; IDENTIFIED BY &quot;12345&quot;;</p><p>CREATE USER cmy@&#39;192.%&#39; IDENTIFIED BY &quot;abcde&quot;;</p><p>在优先级上，&#39;192.%&#39; 优先于 &#39;%&#39;，因此，当用户 cmy 从 192.168.1.1 这台机器尝试使用密码 &#39;12345&#39; 登陆 Doris 会被拒绝。</p></li></ol></li><li><p>忘记密码</p><p>如果忘记了密码无法登陆 Doris，可以在 Doris FE 节点所在机器，使用如下命令无密码登陆 Doris：</p><p><code>mysql-client -h 127.0.0.1 -P query_port -uroot</code></p><p>登陆后，可以通过 SET PASSWORD 命令重置密码。</p></li><li><p>任何用户都不能重置 root 用户的密码，除了 root 用户自己。</p></li><li><p>ADMIN_PRIV 权限只能在 GLOBAL 层级授予或撤销。</p></li><li><p>拥有 GLOBAL 层级 GRANT_PRIV 其实等同于拥有 ADMIN_PRIV，因为该层级的 GRANT_PRIV 有授予任意权限的权限，请谨慎使用。</p></li><li><p><code>current_user()</code> 和 <code>user()</code></p><p>用户可以通过 <code>SELECT current_user();</code> 和 <code>SELECT user();</code> 分别查看 <code>current_user</code> 和 <code>user</code>。其中 <code>current_user</code> 表示当前用户是以哪种身份通过认证系统的，而 <code>user</code> 则是用户当前实际的 <code>user_identity</code>。举例说明：</p><p>假设创建了 <code>user1@&#39;192.%&#39;</code> 这个用户，然后以为来自 192.168.10.1 的用户 user1 登陆了系统，则此时的 <code>current_user</code> 为 <code>user1@&#39;192.%&#39;</code>，而 <code>user</code> 为 <code>user1@&#39;192.168.10.1&#39;</code>。</p><p>所有的权限都是赋予某一个 <code>current_user</code> 的，真实用户拥有对应的 <code>current_user</code> 的所有权限。</p></li></ol><h3 id="6-7-最佳实践"><a href="#6-7-最佳实践" class="headerlink" title="6.7 最佳实践"></a>6.7 最佳实践</h3><p>这里举例一些 Doris 权限系统的使用场景。</p><ol><li><p>场景一</p><p>Doris 集群的使用者分为管理员（Admin）、开发工程师（RD）和用户（Client）。其中管理员拥有整个集群的所有权限，主要负责集群的搭建、节点管理等。开发工程师负责业务建模，包括建库建表、数据的导入和修改等。用户访问不同的数据库和表来获取数据。</p><p>在这种场景下，可以为管理员赋予 ADMIN 权限或 GRANT 权限。对 RD 赋予对任意或指定数据库表的 CREATE、DROP、ALTER、LOAD、SELECT 权限。对 Client 赋予对任意或指定数据库表 SELECT 权限。同时，也可以通过创建不同的角色，来简化对多个用户的授权操作。</p></li><li><p>场景二</p><p>一个集群内有多个业务，每个业务可能使用一个或多个数据。每个业务需要管理自己的用户。在这种场景下。管理员用户可以为每个数据库创建一个拥有 DATABASE 层级 GRANT 权限的用户。该用户仅可以对用户进行指定的数据库的授权。</p></li><li><p>黑名单</p><p>Doris 本身不支持黑名单，只有白名单功能，但我们可以通过某些方式来模拟黑名单。假设先创建了名为 <code>user@&#39;192.%&#39;</code> 的用户，表示允许来自 <code>192.*</code> 的用户登录。此时如果想禁止来自 <code>192.168.10.1</code> 的用户登录。则可以再创建一个用户 <code>cmy@&#39;192.168.10.1&#39;</code> 的用户，并设置一个新的密码。因为 <code>192.168.10.1</code> 的优先级高于 <code>192.%</code>，所以来自 <code>192.168.10.1</code> 将不能再使用旧密码进行登录。</p></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;一、概述&quot;&gt;&lt;a href=&quot;#一、概述&quot; class=&quot;headerlink&quot; title=&quot;一、概述&quot;&gt;&lt;/a&gt;一、概述&lt;/h2&gt;
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
      <category term="Apache Doris" scheme="http://chenzhonzhou.github.io/tags/Apache-Doris/"/>
    
      <category term="OLAP" scheme="http://chenzhonzhou.github.io/tags/OLAP/"/>
    
  </entry>
  
  <entry>
    <title>Superset 一款轻量级BI工具</title>
    <link href="http://chenzhonzhou.github.io/2021/12/02/superset-yi-kuan-qing-liang-ji-bi-gong-ju/"/>
    <id>http://chenzhonzhou.github.io/2021/12/02/superset-yi-kuan-qing-liang-ji-bi-gong-ju/</id>
    <published>2021-12-02T02:41:26.000Z</published>
    <updated>2021-12-03T08:57:00.543Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p><a href="https://superset.apache.org/" target="_blank" rel="noopener">Apache Superset</a> 是Airbnb开源的数据挖掘和可视化平台。支持丰富的数据源连接，多种可视化方式，并能够对用户实现细粒度的权限控制。该工具主要特点是可自助分析、自定义仪表盘、分析结果可视化（导出）、用户/角色权限控制，还集成了一个SQL编辑器，可以进行SQL编辑查询等。</p><h2 id="二、支持功能"><a href="#二、支持功能" class="headerlink" title="二、支持功能"></a>二、支持功能</h2><ul><li>丰富的数据可视化集；</li><li>易于使用的界面，用于浏览和可视化数据；</li><li>创建和共享仪表板；</li><li>与主要身份验证提供程序（数据库，OpenID，LDAP，OAuth和REMOTE_USER通过Flask AppBuilder集成）集成的企业就绪身份验证；</li><li>可扩展的高粒度安全性/权限模型，允许有关谁可以访问单个要素和数据集的复杂规则；</li><li>一个简单的语义层，允许用户通过定义哪些字段应显示在哪个下拉列表中以及哪些聚合和功能度量可供用户使用来控制如何在UI中显示数据源；</li><li>通过 SQLAlchemy 连接到任何基于 SQL 的数据源，包括 PB 级的现代云原生数据库和引擎；</li><li>与Druid.io的深度集成。</li></ul><h2 id="三、Superset-部署"><a href="#三、Superset-部署" class="headerlink" title="三、Superset 部署"></a>三、Superset 部署</h2><p>Superset 的部署可以使用本地Python环境、Docker Compose、K8s等方式，这里基于前再者进行示范</p><h3 id="3-1-基于本地-Pthon-部署"><a href="#3-1-基于本地-Pthon-部署" class="headerlink" title="3.1 基于本地 Pthon 部署"></a>3.1 基于本地 Pthon 部署</h3><h4 id="3-1-1-安装依赖包"><a href="#3-1-1-安装依赖包" class="headerlink" title="3.1.1 安装依赖包"></a>3.1.1 安装依赖包</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sudo yum install gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel bzip2-devel</span><br></pre></td></tr></table></figure><p>编译依赖sqlite3，后面编译python的时候，缺失sqlite3的库，并不会直接报错，直到运行Superset时就会报<code>ModuleNotFoundError: No module named &#39;_sqlite3&#39;</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~/downloads]$ wget http://www.sqlite.org/sqlite-3.5.6.tar.gz</span><br><span class="line">[hadoop@hadoop3 ~/downloads]$ tar xf sqlite-3.5.6.tar.gz</span><br><span class="line">[hadoop@hadoop3 ~/downloads]$ cd sqlite-3.5.6/</span><br><span class="line">[hadoop@hadoop3 ~/downloads/sqlite-3.5.6]$ ./configure --disable-tcl</span><br><span class="line">[hadoop@hadoop3 ~/downloads/sqlite-3.5.6]$ make &amp;&amp; sudo make install</span><br></pre></td></tr></table></figure><h4 id="3-1-2-安装-python3-环境"><a href="#3-1-2-安装-python3-环境" class="headerlink" title="3.1.2 安装 python3 环境"></a>3.1.2 安装 python3 环境</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~/downloads]$ wget https://www.python.org/ftp/python/3.7.11/Python-3.7.11.tgz</span><br><span class="line">[hadoop@hadoop3 ~/downloads]$ sudo tar zxf Python-3.7.11.tgz -C /usr/src/</span><br><span class="line">[hadoop@hadoop3 ~/downloads]$ <span class="built_in">cd</span> /usr/src/Python-3.7.11/</span><br><span class="line">[hadoop@hadoop3 Python-3.7.11]$ sudo ./configure LDFLAGS=<span class="string">"-L/usr/local/lib"</span> CPPFLAGS=<span class="string">"-I/usr/local/include"</span> --<span class="built_in">enable</span>-optimizations</span><br><span class="line">[hadoop@hadoop3 Python-3.7.11]$ sudo make altinstall</span><br></pre></td></tr></table></figure><h4 id="3-1-3-开启-python-虚拟环境"><a href="#3-1-3-开启-python-虚拟环境" class="headerlink" title="3.1.3 开启 python 虚拟环境"></a>3.1.3 开启 python 虚拟环境</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sudo pip3 install --upgrade setuptools pip</span><br><span class="line">[hadoop@hadoop3 ~]$ pip3 install virtualenv -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop3 ~]$ python3 -m venv venv</span><br><span class="line">[hadoop@hadoop3 ~]$ . venv/bin/activate</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">可以通过deactivate命令退出虚拟环境。</span></span><br></pre></td></tr></table></figure><p>或者使用 pyenv-virtualenv：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Here we name the virtual env <span class="string">'superset'</span></span></span><br><span class="line">pyenv virtualenv superset</span><br><span class="line">pyenv activate superset</span><br></pre></td></tr></table></figure><h4 id="3-1-4-安装和初始化-Superset"><a href="#3-1-4-安装和初始化-Superset" class="headerlink" title="3.1.4 安装和初始化 Superset"></a>3.1.4 安装和初始化 Superset</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">安装 superset</span></span><br><span class="line">(venv) [hadoop@hadoop3 ~]$ pip3 install apache-superset -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">初始化 superset</span></span><br><span class="line">(venv) [hadoop@hadoop3 ~]$ superset db upgrade</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">创建管理员用户，初始化，启动superset</span></span><br><span class="line">(venv) [hadoop@hadoop3 ~]$ superset superset fab create-admin</span><br><span class="line">(venv) [hadoop@hadoop3 ~]$ superset load_examples</span><br><span class="line">(venv) [hadoop@hadoop3 ~]$ superset init</span><br><span class="line">(venv) [hadoop@hadoop3 ~]$ superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger</span><br></pre></td></tr></table></figure><p>如果报错 <code>ERROR: flask-appbuilder 3.4.0 has requirement Flask-WTF&lt;0.15.0,&gt;=0.14.2, but you&#39;ll have flask-wtf 1.0.0 which is incompatible.</code> 请安装 <code>pip3 install Flask-WTF==0.14.3</code></p><h3 id="3-2-基于Docker-Compose-部署"><a href="#3-2-基于Docker-Compose-部署" class="headerlink" title="3.2 基于Docker Compose 部署"></a>3.2 基于Docker Compose 部署</h3><h4 id="3-2-1-克隆-Superset"><a href="#3-2-1-克隆-Superset" class="headerlink" title="3.2.1 克隆 Superset"></a>3.2.1 克隆 Superset</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ git clone https://github.com/apache/superset.git</span><br></pre></td></tr></table></figure><h4 id="3-2-2-启动-Superset"><a href="#3-2-2-启动-Superset" class="headerlink" title="3.2.2 启动 Superset"></a>3.2.2 启动 Superset</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~/superset]$ docker-compose -f docker-compose-non-dev.yml up</span><br></pre></td></tr></table></figure><p>访问 Superset UI 页面，默认账户密码admin</p><p><img src="/2021/12/02/superset-yi-kuan-qing-liang-ji-bi-gong-ju/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><h4 id="3-2-3-Superset-汉化"><a href="#3-2-3-Superset-汉化" class="headerlink" title="3.2.3 Superset 汉化"></a>3.2.3 Superset 汉化</h4><p>在 <code>docker/pythonpath_dev/superset_config.py</code> 文件任意位置中添加以下内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">LANGUAGES  = &#123;</span><br><span class="line">     <span class="string">'en'</span> : &#123; <span class="string">'flag'</span> : <span class="string">'us'</span> , <span class="string">'name'</span> : <span class="string">'English'</span> &#125;,</span><br><span class="line">     <span class="string">'zh'</span> : &#123; <span class="string">'flag'</span> : <span class="string">'cn'</span> , <span class="string">'name'</span> : <span class="string">'Chinese'</span> &#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重启 Superset</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~/superset]$ docker-compose -f docker-compose-non-dev.yml down</span><br><span class="line">[hadoop@hadoop3 ~/superset]$ docker-compose -f docker-compose-non-dev.yml up</span><br></pre></td></tr></table></figure><p><img src="/2021/12/02/superset-yi-kuan-qing-liang-ji-bi-gong-ju/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><h4 id="3-3-添加-MySQL-数据库"><a href="#3-3-添加-MySQL-数据库" class="headerlink" title="3.3 添加 MySQL 数据库"></a>3.3 添加 MySQL 数据库</h4><p>在 <strong>Data</strong> –&gt;&gt; <strong>Databases</strong> –&gt;&gt; 选择右上解的 <strong>+数据库</strong> –&gt;&gt; <strong>MySQL</strong></p><p>然后添加数据库相关配置</p><p><img src="/2021/12/02/superset-yi-kuan-qing-liang-ji-bi-gong-ju/%E5%9B%BE%E7%89%873.png" alt="图片3"></p><p>添加完后就可以在 <strong>SQL Lab</strong> –&gt;&gt; <strong>SQL Editor</strong> 进行查询</p><p><img src="/2021/12/02/superset-yi-kuan-qing-liang-ji-bi-gong-ju/%E5%9B%BE%E7%89%874.png" alt="图片4"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;一、简介&quot;&gt;&lt;a href=&quot;#一、简介&quot; class=&quot;headerlink&quot; title=&quot;一、简介&quot;&gt;&lt;/a&gt;一、简介&lt;/h2&gt;
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
      <category term="Superset" scheme="http://chenzhonzhou.github.io/tags/Superset/"/>
    
      <category term="BI" scheme="http://chenzhonzhou.github.io/tags/BI/"/>
    
  </entry>
  
  <entry>
    <title>OLAP 如何选型</title>
    <link href="http://chenzhonzhou.github.io/2021/11/30/olap-ru-he-xuan-xing/"/>
    <id>http://chenzhonzhou.github.io/2021/11/30/olap-ru-he-xuan-xing/</id>
    <published>2021-11-30T07:22:53.000Z</published>
    <updated>2021-12-01T08:05:21.040Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --><h2 id="一、OLAP-简介"><a href="#一、OLAP-简介" class="headerlink" title="一、OLAP 简介"></a>一、OLAP 简介</h2><p><strong>OLAP</strong>，也叫<strong>联机分析处理（Online Analytical Processing）</strong>系统，有的时候也叫DSS决策支持系统，就是我们说的数据仓库。与此相对的是<strong>OLTP（on-line transaction processing）联机事务处理</strong>系统。</p><p>联机分析处理 (OLAP) 的概念最早是由关系数据库之父<strong>E.F.Codd</strong>于1993年提出的。OLAP的提出引起了很大的反响，OLAP作为一类产品同联机事务处理 (OLTP) 明显区分开来。</p><p>Codd认为联机事务处理（OLTP）已不能满足终端用户对数据库查询分析的要求，SQL对大数据库的简单查询也不能满足用户分析的需求。用户的决策分析需要对关系数据库进行大量计算才能得到结果，而查询的结果并不能满足决策者提出的需求。因此，Codd提出了多维数据库和多维分析的概念，即OLAP。</p><p>OLAP委员会对联机分析处理的定义为：从原始数据中转化出来的、能够真正为用户所理解的、并真实反映企业多维特性的数据称为信息数据，使分析人员、管理人员或执行人员能够从多种角度对信息数据进行快速、一致、交互地存取，从而获得对数据的更深入了解的一类软件技术。OLAP的目标是满足决策支持或多维环境特定的查询和报表需求，它的技术核心是”维”这个概念，因此OLAP也可以说是多维数据分析工具的集合。</p><h2 id="二、OLAP-的准则和特性"><a href="#二、OLAP-的准则和特性" class="headerlink" title="二、OLAP 的准则和特性"></a>二、OLAP 的准则和特性</h2><p><strong>E.F.Codd提出了关于OLAP的12条准则</strong></p><ol><li>准则1 OLAP模型必须提供多维概念视图</li><li>准则2 透明性准则</li><li>准则3 存取能力准则</li><li>准则4 稳定的报表能力</li><li>准则5 客户/服务器体系结构</li><li>准则6 维的等同性准则</li><li>准则7 动态的稀疏矩阵处理准则</li><li>准则8 多用户支持能力准则</li><li>准则9 非受限的跨维操作</li><li>准则10 直观的数据操纵</li><li>准则11 灵活的报表生成</li><li>准则12 不受限的维与聚集层次</li></ol><p><strong>OLAP场景的关键特征</strong></p><ol><li>大多数是读请求</li><li>数据总是以相当大的批(&gt; 1000 rows)进行写入</li><li>不修改已添加的数据</li><li>每次查询都从数据库中读取大量的行，但是同时又仅需要少量的列</li><li>宽表，即每个表包含着大量的列</li><li>较少的查询(通常每台服务器每秒数百个查询或更少)</li><li>对于简单查询，允许延迟大约50毫秒</li><li>列中的数据相对较小：数字和短字符串(例如，每个URL 60个字节)</li><li>处理单个查询时需要高吞吐量（每个服务器每秒高达数十亿行）</li><li>事务不是必须的</li><li>对数据一致性要求低</li><li>每一个查询除了一个大表外都很小</li><li>查询结果明显小于源数据，换句话说，数据被过滤或聚合后能够被盛放在单台服务器的内存中</li></ol><blockquote><p>OLTP系统强调数据库内存效率，强调内存各种指标的命令率，强调绑定变量，强调并发操作，强调事务性；<br>OLAP系统则强调数据分析，强调SQL执行时长，强调磁盘I/O，强调分区。</p></blockquote><h2 id="三、OLAP-开源引擎"><a href="#三、OLAP-开源引擎" class="headerlink" title="三、OLAP 开源引擎"></a>三、OLAP 开源引擎</h2><p>目前市面上主流的开源OLAP引擎包含不限于：Hive、Hawq、Presto、Kylin、Impala、Sparksql、Druid、Clickhouse、Greeplum等，可以说目前没有一个引擎能在数据量，灵活程度和性能上做到完美，用户需要根据自己的需求进行选型。</p><p>开源大数据OLAP组件，可以分为MOLAP和ROLAP两类。ROLAP中又可细分为MPP数据库和SQL引擎两类。对于SQL引擎又可以再细分为基于MPP架构的SQL引擎和基于通用计算框架的SQL引擎。</p><p><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><p>MOLAP一般对数据存储有优化，并且进行部分预计算，因此查询性能最高。但通常对查询灵活性有限制。</p><p>MPP数据库是个完整的数据库，通常数据需要导入其中才能完成OLAP功能。MPP数据库在数据入库时对数据分布可以做优化，虽然入库效率有一定下降，但是对后期查询性能的提高有很大帮助。MPP数据库可以提供灵活的即席查询能力，但一般对查询数据量有一定限制，无法支撑特别大的数据量的查询。</p><p>SQL引擎只提供SQL执行的能力，本身一般不负责数据存储，通常可以对接多种数据储存，如HDFS、HBase、MySQL等。有的还支持联邦查询能力，可以对多个异构数据源进行联合分析。SQL引擎中，基于MPP架构的SQL引擎，一般对在线查询场景有特殊优化，所以端到端查询性能一般要高于基于通用计算框架的SQL引擎；但是在容错性和数据量方面又会逊于基于通用计算框架的SQL引擎。</p><h3 id="3-1-Kylin"><a href="#3-1-Kylin" class="headerlink" title="3.1 Kylin"></a>3.1 Kylin</h3><p>Apache <a href="https://kylin.apache.org/cn/" target="_blank" rel="noopener">Kylin</a> 是一个开源的分布式分析引擎，提供 Hadoop/Spark 之上的 SQL 查询接口及多维分析（OLAP）能力以支持超大规模数据，它能在亚秒内查询巨大的 Hive 表。Kylin的核心思想是预计算，理论基础是：以空间换时间。即将多维分析可能用到的度量进行预计算，将计算好的结果保存成Cube并存储到HBase中，供查询时直接访问。把高复杂度的聚合运算，多表连接等操作转换成对预计算结果的查询。</p><p>Kylin的主要特点是预计算，提前计算好各个cube，这样的优点是查询快速，秒级延迟；缺点也非常明显，灵活性不足，无法做一些探索式的，关联性的数据分析。</p><p><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><p><strong>Kylin的核心模块</strong>：</p><blockquote><p><strong>REST Server：</strong>提供 Restful 接口，例如创建、构建、刷新、合并等 Cube 相关操作，Kylin 的 Projects、Tables 等元数据管理，用户访问权限控制，SQL 的查询等；<br><strong>Query Engine：</strong>使用开源的 Apache Calcite 框架来实现 SQL 解析，可以理解为 SQL 引擎层；<br><strong>Routing：</strong>负责将解析 SQL 生成的执行计划转换成 Cube 缓存的查询，这部分查询是可以在秒级甚至毫秒级完成；<br><strong>Metadata：</strong>Kylin 中有大量的元数据信息，包括 Cube 的定义、星型模型的定义、Job 和执行 Job 的输出信息、模型的维度信息等等，Kylin 的元数据和 Cube 都存储在 HBase 中，存储的格式是 json 字符串；<br><strong>Cube Build Engine：</strong>所有模块的基础，它主要负责 Kylin 预计算中创建 Cube，创建的过程是首先通过 Hive 读取原始数据，然后通过一些 MapReduce 或 Spark 计算生成 Htable，最后将数据 load 到 HBase 表中。</p></blockquote><p><strong>整个系统分为两部分：</strong></p><ol><li><p>离线构建：</p><blockquote><p>1）数据源在左侧，目前主要是 Hadoop Hive，保存着待分析的用户数据；<br>2）根据元数据的定义，下方构建引擎从数据源抽取数据，并构建 Cube；<br>3）数据以关系表的形式输入，支持星形模型和雪花模型；<br>4）2.5 开始 Spark 是主要的构建技术（以前是MapReduce）；<br>5）构建后的 Cube 保存在右侧的存储引擎中，一般选用 HBase 作为存储。</p></blockquote></li><li><p>在线查询</p><blockquote><p>1）用户可以从上方查询系统（Rest API、JDBC/ODBC）发送 SQL 进行查询分析；<br>2）无论从哪个接口进入，SQL 最终都会来到 Rest 服务层，再转交给查询引擎进行处理；<br>3）查询引擎解析 SQL，生成基于关系表的逻辑执行计划；<br>4）然后将其转译为基于 Cube 的物理执行计划；<br>5）最后查询预计算生成的 Cube 并产生结果。</p></blockquote></li></ol><p><strong>优点</strong>：</p><blockquote><p>1）亚秒级查询响应；<br>2）支持百亿、千亿甚至万亿级别交互式分析；<br>3）无缝与 BI 工具集成；<br>4）支持增量刷新；</p></blockquote><p><strong>缺点</strong>：</p><blockquote><p>1）由于 Kylin 是一个分析引擎，只读，不支持 insert, update, delete 等 SQL 操作，用户修改数据的话需要重新批量导入（构建）；<br>2）需要预先建立模型后加载数据到 Cube 后才可进行查询；<br>3）使用 Kylin 的建模人员需要了解一定的数据仓库知识。</p></blockquote><h3 id="3-2-Druid"><a href="#3-2-Druid" class="headerlink" title="3.2 Druid"></a>3.2 Druid</h3><p><a href="https://druid.apache.org/" target="_blank" rel="noopener">Druid</a> 是一个高效的数据查询系统，主要解决的是对于大量的基于时序的数据进行聚合查询。数据可以实时摄入，进入到Druid后立即可查，同时数据是几乎是不可变。</p><p><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%873.png" alt="图片3"></p><p>Druid是专为海量数据集上的做高性能 OLAP而设计的数据存储和分析系统。</p><p>Druid 采用了shared-nothing架构与lambda架构。</p><p>Druid的核心设计结合了数据仓库，时间序列数据库和搜索系统的思想，以创建一个统一的系统，用于针对各种用例的实时分析。Druid将这三个系统中每个系统的关键特征合并到其接收层，存储格式，查询层和核心体系结构中。</p><p>目前 Druid 的去重都是非精确的，Druid 适合处理星型模型的数据，不支持关联操作。也不支持数据的更新。</p><p>Druid有几种进程类型，简要描述如下：</p><blockquote><p><strong>Coordinators协调器进程：</strong>负责监控数据服务器上的Historicals进程，将Segments分配给特定的服务器，并负责确保Segments在多个Historicals之间保持平衡。<br><strong>Overlords进程：</strong>负责监控数据服务器上的MiddleManager进程，并控制数据获取任务的分配。<br><strong>Broker代理进程：</strong>处理来自外部客户端的查询，将查询转发给数据服务器去执行，并合并来自多个数据服务器的结果，返回给最终用户。<br><strong>Routers进程：</strong>是个可选进程，提供统一的API Gateway，可以将请求路由到Brokers、Overlords和Coordinators。<br><strong>Historicals进程：</strong>负责处理“历史数据”的查询。 它会从Deep Storage下载查询需要的Segments以加速查询。它不负责写入。<br><strong>MiddleManager进程：</strong>负责处理获取到新数据，从外部数据源读取数据并转换成Segments进行存储。</p></blockquote><p>Druid进程可以按照任何方式进行部署，但是为了易于部署，一般建议将它们组织为三种服务器类型：</p><blockquote><p><strong>主服务器：</strong>运行Coordinatos和Overlords进程，负责管理数据获取和数据可用性。<br><strong>查询服务器：</strong>运行Brokers和可选的Routers进程，处理来自外部客户端的查询。<br><strong>数据服务器：</strong>运行Historicals和MiddleManagers进程，负责执行数据获取任务并存储所有可查询的数据。</p></blockquote><p><strong>特点：</strong></p><blockquote><p><strong>面向列的存储：</strong>Druid 单独存储和压缩每一列，只需要读取特定查询所需的列，支持快速扫描、排名和 groupBys。<br><strong>本地搜索索引：</strong>Druid 为字符串值创建倒排索引以进行快速搜索和过滤。<br><strong>流式传输和批量摄取：</strong>适用于 Apache Kafka、HDFS、AWS S3、流处理器等的开箱即用连接器。<br><strong>灵活的模式：</strong>Druid 优雅地处理不断变化的模式和<a href="https://druid.apache.org/docs/latest/ingestion/flatten-json" target="_blank" rel="noopener">嵌套数据</a>。<br><strong>时间优化分区：</strong>Druid根据时间对数据进行智能分区，基于时间的查询速度明显快于传统数据库。<br><strong>SQL 支持：</strong>除了其原生的<a href="https://druid.apache.org/docs/latest/querying/querying" target="_blank" rel="noopener">基于 JSON 的语言外</a>，Druid 还通过 HTTP 或 JDBC 使用<a href="https://druid.apache.org/docs/latest/querying/sql" target="_blank" rel="noopener">SQL</a>。<br><strong>水平扩展性：</strong>Druid 已<a href="https://druid.apache.org/druid-powered" target="_blank" rel="noopener">在生产中</a>用于每秒摄取数百万个事件、保留多年数据并提供亚秒级查询。<br><strong>操作简单：</strong>只需添加或删除服务器即可扩大或缩小规模，Druid 会自动重新平衡。容错架构围绕服务器故障进行路由。</p></blockquote><p><strong>使用场景</strong>：</p><blockquote><p>1）需要实时分析，查询延迟为100毫秒到几秒钟；<br>2）插入率非常高，但更新不常见；<br>3）大多数查询都是聚合和报告查询；<br>4）数据有一个时间组件；<br>5）每个查询只能访问一个大的分布式表；<br>6）有高基数数据列，对它们进行快速计数和排名；<br>7）需要交互式和快速探究大量数据。</p></blockquote><p><strong>缺点：</strong></p><blockquote><p>1）灵活性适中，虽然维度之间随意组合，但不支持adhoc查询，不能自由组合查询，且丢失了明细数据（不采用roll-up情况下可以进行明细查询）；<br>2）易用性较差，不支持join，不支持更新，sql支持很弱(有些插件类似于pinot的PQL语言)，只能JSON格式查询，对于去重操作不能精准去重；<br>3）处理方式复杂，需要流处理引擎将数据join成宽表，维护相对复杂；对内存要求较高。</p></blockquote><h3 id="3-3-Greenplum"><a href="#3-3-Greenplum" class="headerlink" title="3.3 Greenplum"></a>3.3 Greenplum</h3><p><a href="https://greenplum.org/" target="_blank" rel="noopener">Greenplum</a> 是基于PostgreSQL的开源MPP数据库，具有良好的线性扩展能力，具有高效的并行运算和并行存储特性。</p><p><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%874.png" alt="图片4"></p><p>Greenplum的系统架构实际上是多台PostgreSQL数据库服务器组成的矩阵，采用无共享(no shareing)的MPP架构：</p><blockquote><p><strong>Master节点：</strong>作为数据库的入口，负责客服端连接；对客服端的请求生成查询计划，分发给某个或者所有的Segment节点；<br><strong>Standby节点 :</strong> 作为master节点的备库，提供高可用性；<br><strong>Interconnect：</strong>是GreenPlum的网络层；负责每个节点之间的通信；<br><strong>Segment节点：</strong>为数据节点；接收master分发下来的查询计划；执行返回结果给master节点；<br><strong>Mirror Segment节点：</strong> 作为Segment节点的备库，提供高可用性；通常跟对应的segment节点不在同一台机器上。</p></blockquote><p><strong>优点</strong>：</p><blockquote><p>1）支持多态数据存储，允许用户根据应用定义数据分布方式，可提高查询性能；<br>2）具有高效的SQL优化器，针对OLAP查询进行优化。</p></blockquote><p><strong>缺点：</strong></p><blockquote><p>1）存在<strong><code>木桶效应</code></strong>，单机故障会导致性能严重下降，因此集群规模不能太大；<br>2）并发性能不高，通常无法支持超过30个并发。</p></blockquote><h3 id="3-4-ClickHouse"><a href="#3-4-ClickHouse" class="headerlink" title="3.4 ClickHouse"></a>3.4 ClickHouse</h3><p><a href="https://clickhouse.yandex/" target="_blank" rel="noopener">ClickHouse</a> 的全称是<strong>Click Stream，Data WareHouse</strong>，简称ClickHouse，是俄罗斯 Yandex 公司于2016年开源的列式存储数据库（DBMS)，主要用于联机分析处理查询（OLAP），能够使用SQL 查询实时生成分析数据报告。<br>目前ClickHouse公开的资料相对匮乏，比如在架构设计层面就很难找到完整的资料，甚至连一张整体的架构图都没有。</p><p>ClickHouse为什么性能这么好？</p><blockquote><p><strong>着眼硬件：</strong>基于将硬件功效最大化的目的，ClickHouse会在内存中进行GROUP BY；与此同时，他们非常在意CPU L3级别的缓存，因为一次L3的缓存失效会带来70～100ns的延迟，意味着在单核CPU上，它会浪费4000万次/秒的运算。正因为注意了这些细节，所以ClickHouse在基准查询中能做到1.75亿次/秒的数据扫描性能。</p><p><strong>注重算法：</strong>例如，在字符串搜索方面，针对不同的场景，ClickHouse选择了多种算法：对于常量，使用Volnitsky算法；对于非常量，使用CPU的向量化执行SIMD，暴力优化；正则匹配使用re2和hyperscan算法。除了字符串之外，其余的场景也与它类似，ClickHouse会使用最合适、最快的算法。如果世面上出现了号称性能强大的新算法，ClickHouse团队会立即将其纳入并进行验证。</p><p><strong>特定场景，特殊优化：</strong>针对同一个场景的不同状况，选择使用不同的实现方式，尽可能将性能最大化。对于数据结构比较清晰的场景，会通过代码生成技术实现循环展开，以减少循环次数。</p><p><strong>向量化执行：</strong>SIMD被广泛地应用于文本转换、数据过滤、数据解压和JSON转换等场景。相较于单纯地使用CPU，利用寄存器暴力优化也算是一种降维打击了。</p></blockquote><p><strong>优点</strong>：</p><blockquote><p>1）为了高效的使用CPU，数据不仅仅按列存储，同时还按向量进行处理；<br>2）数据压缩空间大，减少IO；处理单查询高吞吐量每台服务器每秒最多数十亿行；<br>3）索引非B树结构，不需要满足最左原则；只要过滤条件在索引列中包含即可；即使在使用的数据不在索引中，由于各种并行处理机制ClickHouse全表扫描的速度也很快；<br>4）写入速度非常快，50-200M/s，对于大量的数据更新非常适用。</p></blockquote><p><strong>缺点：</strong></p><blockquote><p>1）不支持事务，不支持真正的删除/更新；<br>2）不支持高并发，官方建议qps为100，可以通过修改配置文件增加连接数，但是在服务器足够好的情况下；<br>3）SQL满足日常使用80%以上的语法，join写法比较特殊；最新版已支持类似SQL的join，但性能不好；<br>4）尽量做1000条以上批量的写入，避免逐行insert或小批量的insert，update，delete操作，因为ClickHouse底层会不断的做异步的数据合并，会影响查询性能，这个在做实时数据写入的时候要尽量避开；<br>5）Clickhouse快是因为采用了并行处理机制，即使一个查询，也会用服务器一半的CPU去执行，所以ClickHouse不能支持高并发的使用场景，默认单查询使用CPU核数为服务器核数的一半，安装时会自动识别服务器核数，可以通过配置文件修改该参数。</p></blockquote><h3 id="3-5-Impala"><a href="#3-5-Impala" class="headerlink" title="3.5 Impala"></a>3.5 Impala</h3><p><a href="https://impala.apache.org/" target="_blank" rel="noopener">Impala</a> 是Cloudera 公司推出，提供对 HDFS、Hbase 数据的高性能、低延迟的交互式 SQL 查询功能。</p><p>Impala 使用 Hive的元数据, 完全在内存中计算。是CDH 平台首选的 PB 级大数据实时查询分析引擎。</p><p><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%875.png" alt="图片5"></p><p>Impala采用MPP架构，与存储引擎解耦：</p><blockquote><p><strong>Impalad进程：</strong>是核心进程，负责接收查询请求并向多个数据节点分发任务。<br><strong>statestored进程：</strong>负责监控所有Impalad进程，并向集群中的节点报告各个Impalad进程的状态。<br><strong>catalogd进程：</strong>负责广播通知元数据的最新信息。</p></blockquote><p><strong>Impala的特性包括：</strong></p><blockquote><p>1）支持Parquet、Avro、Text、RCFile、SequenceFile等多种文件格式；<br>2）支持存储在HDFS、HBase、Amazon S3上的数据操作；<br>3）支持多种压缩编码方式：Snappy、Gzip、Deflate、Bzip2、LZO；<br>4）支持UDF和UDAF；<br>5）自动以最有效的顺序进行表连接；<br>6）允许定义查询的优先级排队策略；<br>7）支持多用户并发查询；<br>8）支持数据缓存；<br>9）提供计算统计信息（COMPUTE STATS）；<br>10）提供窗口函数（聚合 OVER PARTITION, RANK, LEAD, LAG, NTILE等等）以支持高级分析功能；<br>11）支持使用磁盘进行连接和聚合，当操作使用的内存溢出时转为磁盘操作；<br>12）允许在where子句中使用子查询；<br>13）允许增量统计——只在新数据或改变的数据上执行统计计算；<br>14）支持maps、structs、arrays上的复杂嵌套查询；<br>15）可以使用impala插入或更新HBase。</p></blockquote><p><strong>优点：</strong></p><blockquote><p>1）支持SQL查询，快速查询大数据；<br>2）可以对已有数据进行查询，减少数据的加载，转换；<br>3）多种存储格式可以选择（Parquet, Text, Avro, RCFile, SequeenceFile）；<br>4）可以与Hive配合使用。</p></blockquote><p><strong>缺点：</strong></p><blockquote><p>1）不支持用户定义函数UDF；<br>2）不支持text域的全文搜索；<br>3）不支持Transforms；<br>4）不支持查询期的容错；<br>5）对内存要求高。</p></blockquote><h3 id="3-6-Hawq"><a href="#3-6-Hawq" class="headerlink" title="3.6 Hawq"></a>3.6 Hawq</h3><p><a href="http://hawq.apache.org" target="_blank" rel="noopener">HAWQ</a> 是Pivotal公司开源的一个Hadoop原生大规模并行SQL分析引擎，针对的是分析型应用。Apache HAWQ 采用主从（Master-Slave）的改进MPP架构，通过将MPP与批处理系统有效的结合，克服了MPP的一些关键的限制问题，如短板效应、并发限制、扩展性等。其整体架构与Pivotal另一开源MPP数据库Greenplum比较相似。</p><p>下图提供了典型 HAWQ 部署的高级架构视图<br><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%877.png" alt="图片7"></p><p>下图提供了构成 HAWQ 的软件组件的另一种视图<br><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%876.png" alt="图片6"></p><p>HAWQ Master节点内部有以下几个重要组件：</p><blockquote><p>1）<strong>查询解析器（Parser/Analyzer）</strong>，负责解析查询，并检查语法及语义。最终生成查询树传递给优化器。<br>2）<strong>优化器（Optimizer）</strong>，负责接受查询树，生成查询计划。针对一个查询，可能有数亿个可能的等价的查询计划，但执行性能差异很大。优化器的做用是找出优化的查询计划。<br>3）<strong>资源管理器（Resource Manager）</strong>，资源管理器经过资源代理向全局资源管理器（好比YARN）动态申请资源。并缓存资源。在不须要的时候返回资源。<br>4）<strong>HDFS元数据缓存（HDFS Catalog Cache）</strong>，用于HAWQ确定哪些Segment扫描表的哪些部分。HAWQ是把计算派发到数据所在的地方。因此要匹配计算和数据的局部性。如果每一个查询都访问HDFS NameNode会形成NameNode的瓶颈。因此在HAWQ Master节点上创建了HDFS元数据缓存。<br>5）<strong>容错服务（Fault Tolerance Service）</strong>，负责检测哪些节点可用，哪些节点不可用。不可用的机器会被排除出资源池。<br>6）<strong>查询派遣器（Dispatcher）</strong>，优化器优化完查询之后，查询派遣器派遣计划到各个节点上执行，并协调查询执行的整个过程。查询派遣器是整个并行系统的粘合剂。<br>7）<strong>元数据服务（Catalog Service）</strong>，负责存储HAWQ的各类元数据，包括数据库和表信息，以及访问权限信息等。另外，元数据服务也是实现分布式事务的关键。<br>8）其余节点为Slave节点，每一个Slave节点上部署有HDFS DataNode，YARN NodeManager以及一个HAWQ Segment。HAWQ Segment在执行查询的时候会启动多个QE (Query Executor, 查询执行器)。查询执行器运行在资源容器里面。节点间数据交换经过Interconnect（高速互联网络）进行。</p></blockquote><p><strong>优点：</strong></p><blockquote><p>1）对SQL标准的完善支持：ANSI SQL标准，OLAP扩展，标准JDBC/ODBC支持；<br>2）支持ACID事务特性：这是很多现有基于Hadoop的SQL引擎做不到的，对保证数据一致性很重要；<br>3）动态数据流引擎：基于UDP的高速互联网络；<br>4）多种UDF（用户自定义函数）语言支持：java, python, c/c++, perl, R等；<br>5）动态扩容：动态按需扩容，按照存储大小或者计算需求，秒级添加节点；<br>6）支持MADlib机器学习。</p></blockquote><p><strong>缺点：</strong></p><blockquote><p>1）基于GreenPlum实现，技术实现复杂，包含多个组件。比如对于外部数据源，需要通过PXF单独进行处理；<br>2）C++实现，对内存的控制比较复杂，如果出现segmentfault直接导致当前node挂掉；<br>3）安装配置复杂。</p></blockquote><h3 id="3-7-Presto"><a href="#3-7-Presto" class="headerlink" title="3.7 Presto"></a>3.7 Presto</h3><p><a href="https://prestodb.io/" target="_blank" rel="noopener">Presto</a> 是由 Facebook 推出的一个基于Java开发的开源分布式SQL查询引擎，数据量支持GB到TB字节，presto本身不存数据，但是可以接入很多数据源，它使得用SQL访问任何数据源成为可能，而且支持跨数据源的级联查询。你可以使用Presto通过水平扩展查询处理的方式来查询大型数据集。</p><p>Presto相比ClickHouse优点主要是多表join效果好。相比ClickHouse的支持功能简单，场景支持单一，Presto支持复杂的查询，应用范围更广。</p><p><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%878.png" alt="图片8"></p><p>Presto采用典型的Master-Slave架构：</p><blockquote><p>1）<strong>coordinator</strong>：是presto集群的master节点。负责解析SQL语句，生成执行计划，分发执行任务给Worker节点执行；<br>2）<strong>worker</strong>：是执行任务的节点。负责实际查询任务的计算和读写；<br>3）<strong>discovery service</strong>：是将coordinator和worker结合在一起服务。worker节点启动后向discovery service服务注册，coordinator通过discovery service获取注册的worker节点；<br>4）<strong>connector</strong>：presto以插件形式对数据存储层进行了抽象，即connector。可通过connector连接多种数据源，提取数据。</p></blockquote><p>discovery service 将coordinator和worker结合在一起服务； worker节点启动后向discovery service服务注册 coordinator通过discovery service获取注册的worker节点。</p><p><strong>优点：</strong></p><blockquote><p>1）基于内存运算，减少没必要的硬盘IO；<br>2）都能够处理PB级别的海量数据分析；（虽然能够处理PB级别的海量数据分析，但不是代表Presto把PB级别都放在内存中计算的。而是根据场景，如count，avg等聚合运算，是边读数据边计算，再清内存，再读数据再计算，这种耗的内存并不高。）<br>3）能够连接多个数据源，跨数据源关联查询；<br>4）清晰的架构，是一个能够独立运行的系统，不依赖于任何其他外部系统。部署简单。</p></blockquote><p><strong>缺点：</strong></p><blockquote><p>1）不适合多个大表的join操作，因为presto是基于内存的，太多数据内存放不下的；<br>2）Presto的一个权衡是不关心中间查询容错。如果其中一个Presto工作节点出现故障（例如，关闭），则大多数情况下正在进行的查询将中止并需要重新启动。</p></blockquote><h3 id="3-8-Drill"><a href="#3-8-Drill" class="headerlink" title="3.8 Drill"></a>3.8 Drill</h3><p><a href="https://drill.apache.org/" target="_blank" rel="noopener">Drill</a> 是MapR开源的一个低延迟的大数据集的分布式SQL查询引擎，是谷歌Dremel的开源实现。它支持对本地文件、HDFS、HBASE等数据进行数据查询，也支持对如JSON等schema-free的数据进行查询。</p><p><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%879.png" alt="图片9"></p><p>从架构上看，与同是源自Dremel的Impala比较类似。Drill的核心是DrillBit，它主要负责接收客户端的请求，处理查询，并将结果返回给客户端。 Drill的查询流程包括以下步骤：</p><blockquote><p>1）Drill客户端发起查询，任意DrilBit都可以接受来自客户端的查询；<br>2）收到请求的DrillBit成为<strong>驱动节点（Foreman）</strong>，对查询进行分析优化生成执行计划，之后将执行计划划分成各个<strong>片段（Fragment）</strong>，并确定合适的节点来执行；<br>3）各个节点执行查询片段（Fragment），并将结果返回给驱动节点；<br>4）驱动节点将结果返回给客户端。</p></blockquote><p><strong>优点：</strong></p><blockquote><p>1）几乎可以查询任何类型的NoSQL数据库（包含 Hbase、MongoDB、ElasticSearch、Cassandra、Druid、Kudu、Kafka、OpenTSDB、HDFS、Amazon S3、Azure Blob Storage、Google Cloud Storage、Swift、NAS和本地文件。可以在单次查询中组合多个数据源（联邦查询）。）；<br>2）在处理数据之前，无需加载数据、创建和维护模式或转换数据。相反，只需在 SQL 查询中包含 Hadoop 目录、MongoDB 集合或 S3 存储桶的路径；<br>3）Drill 是唯一支持复杂数据的列式查询引擎。它具有用于复杂数据的内存切碎柱状表示，这使 Drill 能够通过内部 JSON 文档模型的灵活性实现柱状速度；<br>4）可以使用 Tableau、Qlik、MicroStrategy、Spotfire、SAS 和 Excel 等标准 BI/分析工具；<br>5）Drill 的对称架构（所有节点都相同）和简单的安装使得部署和扩展集群变得容易；<br>6）Drill 不是世界上第一个查询引擎，但它是第一个兼具灵活性和速度的查询引擎。为了实现这一点，Drill 具有完全不同的架构，可以在不牺牲 JSON 文档模型提供的灵活性的情况下实现破纪录的性能。</p></blockquote><p><strong>缺点：</strong></p><blockquote><p>1） drill语法和常规sql有区别,一般是如 <code>select * from 插件名.表名</code> 的形式。主要是因为drill查询不同数据源时需要切换不同的插件；<br>2）技术线太长，不容易切合到实际生产线上去；<br>3）国内使用较少，没有大型成功案例，不够大众化，出现问题可能维护起来比较困难。资料比较少。</p></blockquote><h3 id="3-9-Hive"><a href="#3-9-Hive" class="headerlink" title="3.9 Hive"></a>3.9 Hive</h3><p><a href="https://hive.apache.org/" target="_blank" rel="noopener">Hive</a> 是一个构建于Hadoop顶层的数据仓库工具。定义了简单的类似SQL 的查询语言——HiveQL，可以将HiveQL查询转换为MapReduce 的任务在Hadoop集群上执行。</p><p><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%8710.png" alt="图片10"></p><p>hdfs可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p><p>对于Hive主要针对的是OLAP应用，其底层是HDFS分布式文件系统，HDFS一般只用于查询分析统计，而不能是常见的CUD操作，Hive需要从已有的数据库或日志进行同步最终入到HDFS文件系统中，当前要做到增量实时同步都相当困难。</p><p><strong>优点：</strong></p><blockquote><p>1）<strong>高可靠、高容错：</strong>HiveServer采用集群模式，双MetaStor，超时重试机制；<br>2）<strong>类SQL：</strong>类似SQL语法，内置大量函数；<br>3）<strong>可扩展：</strong>自定义存储格式，自定义函数；<br>4）<strong>多接口：</strong>Beeline，JDBC，ODBC，Python，Thrift。</p></blockquote><p><strong>缺点：</strong></p><blockquote><p>1）<strong>延迟较高：</strong>默认MR为执行引擎，MR延迟较高；<br>2）<strong>不支持物化视图：</strong>Hive支持普通视图，不支持物化视图。Hive不能再视图上更新、插入、删除数据；<br>3）<strong>不适用OLTP：</strong>暂不支持列级别的数据添加、更新、删除操作。</p></blockquote><h3 id="3-10-Spark-SQL"><a href="#3-10-Spark-SQL" class="headerlink" title="3.10 Spark SQL"></a>3.10 Spark SQL</h3><p><a href="https://spark.apache.org/sql/" target="_blank" rel="noopener">Spark SQL</a> 的前身是Shark，它将 SQL 查询与 Spark 程序无缝集成，可以将结构化数据作为 Spark 的 RDD 进行查询。Spark SQL作为Spark生态的一员继续发展，而不再受限于Hive，只是兼容Hive。</p><p>Spark SQL提供了sql访问和API访问的接口。支持访问各式各样的数据源，包括Hive, Avro, Parquet, ORC, JSON 和 JDBC。</p><p>Spark SQL与传统 DBMS 的查询优化器 + 执行器的架构较为类似，只不过其执行器是在分布式环境中实现，并采用的 Spark 作为执行引擎：</p><p><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%8711.png" alt="图片11"></p><p>SparkSQL的架构图如下：</p><p><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%8712.png" alt="图片12"></p><p><strong>优点：</strong></p><blockquote><p>1）支持多种数据源；<br>2）易整合，有各种API；<br>3）兼容HiveQL。</p></blockquote><p><strong>缺点：</strong></p><blockquote><p>1）查询性能不高；<br>2）以thrift server方式提供的SparkSQL服务不支持多种数据源，必须使用DataFrame API。</p></blockquote><h2 id="四、各组件性能对比"><a href="#四、各组件性能对比" class="headerlink" title="四、各组件性能对比"></a>四、各组件性能对比</h2><p>测试数据来源于：<a href="http://www.clickhouse.com.cn/topic/5c453371389ad55f127768ea" target="_blank" rel="noopener">开源OLAP引擎测评报告</a>。通过测试以及相关调研编写了各组件各个方面的综合对比分析表，这里采用5分为满分来比较，如下表所示：</p><p><img src="/2021/11/30/olap-ru-he-xuan-xing/%E5%9B%BE%E7%89%8713.png" alt="图片13"></p><ol><li>SparkSQL是Hadoop中另一个著名的SQL引擎，它以Spark作为底层计算框架，Spark使用RDD作为分布式程序的工作集合，它提供一种分布式共享内存的受限形式。在分布式共享内存系统中，应用可以向全局地址空间的任意位置进行读写作，而RDD是只读的，对其只能进行创建、转化和求值等作。这种内存操作大大提高了计算速度。SparkSql的性能相对其他的组件要差一些，多表单表查询性能都不突出。</li><li>Impala官方宣传其计算速度是一大优点，在实际测试中我们也发现它的多表查询性能和presto差不多，但是单表查询方面却不如presto好。而且Impala有很多不支持的地方，例如：不支持update、delete操作，不支持Date数据类型，不支持ORC文件格式等等，所以我们查询时采用parquet格式进行查询，而且Impala在查询时占用的内存很大。</li><li>Presto综合性能比起来要比其余组件好一些，无论是查询性能还是支持的数据源和数据格式方面都要突出一些，在单表查询时性能靠前，多表查询方面性能也很突出。由于Presto是完全基于内存的并行计算，所以presto在查询时占用的内存也不少，但是发现要比Impala少一些，比如多表join需要很大的内存，Impala占用的内存比presto要多。</li><li>HAWQ 吸收了先进的基于成本的 SQL 查询优化器，自动生成执行计划，可优化使用hadoop 集群资源。HAWQ 采用 Dynamic pipelining 技术解决这一关键问题。Dynamic pipelining 是一种并行数据流框架，利用线性可扩展加速Hadoop查询，数据直接存储在HDFS上，并且其SQL查询优化器已经为基于HDFS的文件系统性能特征进行过细致的优化。但是我们发现HAWQ在多表查询时比Presto、Impala差一些；而且不适合单表的复杂聚合操作，单表测试性能方面要比其余四种组件差很多，hawq环境搭建也遇到了诸多问题。</li><li>ClickHouse 作为目前所有开源MPP计算框架中计算速度最快的，它在做多列的表，同时行数很多的表的查询时，性能是很让人兴奋的，但是在做多表的join时，它的性能是不如单宽表查询的。性能测试结果表明ClickHouse在单表查询方面表现出很大的性能优势，但是在多表查询中性能却比较差，不如presto、impala、hawq的效果好。</li><li>GreenPlum作为关系型数据库产品，它的特点主要就是查询速度快，数据装载速度快，批量DML处理快。而且性能可以随着硬件的添加，呈线性增加，拥有非常良好的可扩展性。因此，它主要适用于面向分析的应用。比如构建企业级ODS/EDW，或者数据集市等，GREENPLUM都是不错的选择。</li></ol><p><strong>OLAP大混战对比分析：</strong> 仅供参考</p><table><thead><tr><th>引擎/对比项目</th><th>Kylin</th><th>Presto</th><th>Impala</th><th>Druid</th><th>SpaqkSql</th><th>ElasticSearch</th><th>Kudu</th><th>ClickHouse</th><th>Doris</th><th>TiDB</th><th>Hive</th><th>Greenplum</th><th>SnappyData</th><th>Hawq</th></tr></thead><tbody><tr><td>亚秒级响应</td><td>Y</td><td>N</td><td>Y</td><td>Y</td><td>N</td><td>N</td><td>Y（据说可以到达秒级）</td><td>单表查询Y，多表查询N</td><td>Y（ms-s性能高于Kylin‘）</td><td>N</td><td>N</td><td>Y</td><td>Y</td><td>N</td></tr><tr><td>高并发</td><td>Y</td><td>N</td><td>Y</td><td>Y</td><td>N</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>Y</td><td>Y</td><td>N</td><td>Y</td><td>Y</td></tr><tr><td>百亿数据集</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td></td><td>Y</td><td>Y</td><td>N</td><td>N</td><td>Y</td><td></td></tr><tr><td>SQL支持</td><td>Y</td><td>Y</td><td>Y</td><td>N（开发中）</td><td>Y</td><td>N</td><td>Y（结合Impala可支持SQL查询）</td><td>Y（支持Sql中基本语法对于开窗函数还不支持）</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>离线</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>实时</td><td>N（开发中，目前主要支持Kafka 流构建 Cube）</td><td>N</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>Y（Spark Streaming）</td><td>Y（实时数据分析领域的黑马）</td><td>Y</td><td>Y</td><td>N</td><td>N</td><td>Y</td><td>N</td></tr><tr><td>精准去重能力</td><td>Y</td><td>Y</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>是否支持明细查询</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>多表join</td><td>Y</td><td>Y</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>能否更改模型</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>Y（更换表引擎）</td><td>N</td><td>Y</td><td>Y</td><td>N</td><td>Y</td><td>未知</td></tr><tr><td>JDBC/ODBC for BI集成</td><td>Y</td><td>Y</td><td>N</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>Y</td><td>Y</td><td>N</td><td>Y</td><td>Y</td></tr><tr><td>WEB GUI</td><td>Y</td><td>Y</td><td>Y</td><td>N</td><td>N</td><td>N</td><td>Y</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>未知</td><td>未知</td><td>Y</td></tr><tr><td>REST API</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td><td>N</td><td>N</td><td>未知</td><td>N</td><td>N</td></tr><tr><td>社区活跃度</td><td>活跃</td><td>活跃</td><td>活跃</td><td>活跃</td><td>活跃</td><td>活跃</td><td>较活跃</td><td>不太活跃</td><td>Doris社区刚刚起步，目前核心用户只有Baidu；</td><td>较活跃</td><td>活跃</td><td>活跃</td><td>较活跃</td><td>较活跃</td></tr><tr><td>存储能力</td><td>shared nothing</td><td>shared nothing</td><td>纯计算</td><td>shared nothing</td><td>无</td><td>计算+存储</td><td>列式存储</td><td>计算+存储</td><td>计算+存储</td><td>计算+存储</td><td>存储到HDFS</td><td>计算+存储</td><td>shared</td><td>noting</td></tr><tr><td>成本</td><td>中</td><td>中</td><td>中（CDH据说要收费收费以后可能会提高）</td><td>中</td><td>低</td><td>中</td><td>中</td><td>高</td><td>中</td><td>中</td><td>低</td><td>未知</td><td>高</td><td>中</td></tr><tr><td>易用性</td><td>安装简单快捷，轻量级，简单，选择维度表和度量即可构建Cube</td><td>安装简单，基于内存，查询代价较大</td><td>部署简单，基于内存，查询代价较大</td><td>部署简单，基于内存</td><td>部署简单</td><td>部署较简单，但是难用目前发展趋向于专人专岗</td><td>部署简单，需要开发门槛较高，基于磁盘</td><td>简单易用</td><td>按装复杂</td><td>安装部署较为复杂</td><td>部署较简单</td><td>未知</td><td>未知</td><td>部署比较复杂</td></tr><tr><td>监控成本</td><td>自带监控组件、运维成本低</td><td>有公共web监控管理</td><td>自带监控组件，运维成本低</td><td>自带监控可配置，有web页面</td><td>自带监控</td><td>可以使用Kibana实现，成本较高</td><td>已经集成到CDH中，便于监控</td><td>自身具有简单的监控组件，可以联合其他的监控工具进行监控</td><td>监控的集中到Prometheus</td><td>使用开源的 metric 分析及可视化系统Grafana 进行监控</td><td>没有自带的监控，但是在阿里云有配合hive的监控，成本高</td><td>未知</td><td>运维成本高</td><td>支持多种工具监控</td></tr></tbody></table><h2 id="五、选型的一些建设"><a href="#五、选型的一些建设" class="headerlink" title="五、选型的一些建设"></a>五、选型的一些建设</h2><p>上面给出了常用的一些OLAP引擎，它们各自有各自的特点，我们将其分组：</p><ul><li>Hive，Hawq，Impala - 基于SQL on Hadoop；</li><li>Presto和Spark SQL类似 - 基于内存解析SQL生成执行计划；</li><li>Kylin 用空间换时间，预计算；</li><li>Druid 一个支持数据的实时摄入；</li><li>ClickHouse OLAP领域的Hbase，单表查询性能优势巨大；</li><li>Greenpulm OLAP领域的Postgresql。</li></ul><p>如果你的场景是基于HDFS的离线计算任务，那么Hive，Hawq和Imapla就是你的调研目标；<br>如果你的场景解决分布式查询问题，有一定的实时性要求，那么Presto和Spark SQL可能更符合你的期望；<br>如果你的汇总维度比较固定，实时性要求较高，可以通过用户配置的维度+指标进行预计算，那么不妨尝试Kylin和Druid；<br>ClickHouse则在单表查询性能上独领风骚，远超过其他的OLAP数据库；<br>Greenpulm作为关系型数据库产品，性能可以随着集群的扩展线性增长，更加适合进行数据分析。</p><p>就像美团在调研Kylin的报告中所说的：</p><p>目前还没有一个OLAP系统能够满足各种场景的查询需求。<br>其本质原因是，没有一个系统能同时在数据量、性能、和灵活性三个方面做到完美，每个系统在设计时都需要在这三者间做出取舍。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;一、OLAP-简介&quot;&gt;&lt;a href=&quot;#一、OLAP-简介&quot; class=&quot;headerlink&quot; title=&quot;一、OLAP 简介
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
      <category term="OLAP" scheme="http://chenzhonzhou.github.io/tags/OLAP/"/>
    
  </entry>
  
  <entry>
    <title>Apache Druid 联机分析处理(OLAP)</title>
    <link href="http://chenzhonzhou.github.io/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/"/>
    <id>http://chenzhonzhou.github.io/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/</id>
    <published>2021-11-24T08:25:33.000Z</published>
    <updated>2021-12-09T06:41:11.802Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p><a href="https://druid.apache.org/" target="_blank" rel="noopener">Apache Druid</a> 是一个实时分析数据库，专为大型数据集进行快速的查询分析（OLAP 查询）而设计。Druid最常被当做数据库来用以支持实时摄取、高性能查询和高稳定运行的应用场景。Druid也通常被用来助力分析型应用的图形化界面，或者当做需要快速聚合的高并发后端API，Druid最适合应用于面向事件类型的数据。</p><h3 id="1-1-Druid-常见应用领域"><a href="#1-1-Druid-常见应用领域" class="headerlink" title="1.1 Druid 常见应用领域"></a>1.1 Druid 常见应用领域</h3><p>Druid的常见应用领域包括：</p><ul><li>点击流分析（网络和移动分析）</li><li>网络遥测分析（网络性能监控）</li><li>服务器指标存储</li><li>供应链分析（制造指标）</li><li>应用程序性能指标</li><li>数字营销/广告分析</li><li>商业智能/OLAP</li></ul><h3 id="1-2-Druid-的三个设计原则"><a href="#1-2-Druid-的三个设计原则" class="headerlink" title="1.2 Druid 的三个设计原则"></a>1.2 Druid 的三个设计原则</h3><h4 id="1-2-1-快速查询"><a href="#1-2-1-快速查询" class="headerlink" title="1.2.1 快速查询"></a>1.2.1 快速查询</h4><p>对于数据分析场景，大部分情况下，我们只关心一定粒度聚合的数据，而非每一行原始数据的细节情况。因此，数据聚合粒度可以是1分钟、5分钟、1小时或1天等。部分数据聚合（Partial Aggregate）给Druid争取了很大的性能优化空间。<br>数据内存化也是提高查询速度的杀手锏。内存和硬盘的访问速度相差近百倍，但内存的大小是非常有限的，因此在内存使用方面要精细设计，比如Druid里面使用了Bitmap和各种压缩技术。<br>另外，为了支持Drill-Down某些维度，Druid维护了一些倒排索引。这种方式可以加快AND和OR等计算操作。</p><h4 id="1-2-2-水平扩展能力"><a href="#1-2-2-水平扩展能力" class="headerlink" title="1.2.2 水平扩展能力"></a>1.2.2 水平扩展能力</h4><p>Druid 查询性能在很大程度上依赖于内存的优化使用。数据可以分布在多个节点的内存中，因此当数据增长的时候，可以通过简单增加机器的方式进行扩容。为了保持平衡，Druid按照时间范围把聚合数据进行分区处理。对于高基数的维度，只按照时间切分有时候是不够的（Druid的每个Segment不超过2000万行），故Druid还支持对Segment进一步分区。<br>历史Segment数据可以保存在深度存储系统中，存储系统可以是本地磁盘、HDFS或远程的云服务。如果某些节点出现故障，则可借助Zookeeper协调其他节点重新构造数据。Druid内置了容易并行化的集合操作，在直方图方面和去重查询方面采用近似算法保证性能，如HyperLoglog，DataSketches等。</p><h4 id="1-2-3-实时分析"><a href="#1-2-3-实时分析" class="headerlink" title="1.2.3 实时分析"></a>1.2.3 实时分析</h4><p>Druid提供了包含基于时间维度数据的存储服务，并且任何一行数据都是历史真实发生的事件，因此在设计之初就约定事件一但进入系统，就不能再改变。<br>对于历史数据Druid以Segment数据文件的方式组织，并且将它们存储到深度存储系统中，例如文件系统或亚马逊的S3等。当需要查询这些数据的时候，Druid再从深度存储系统中将它们装载到内存供查询使用。</p><h3 id="1-3-特点"><a href="#1-3-特点" class="headerlink" title="1.3 特点"></a>1.3 特点</h3><p>Druid的核心架构吸收和结合了<strong>数据仓库</strong>、<strong>时序数据库</strong>以及<strong>检索系统</strong>的优势，其主要特征如下：</p><ul><li><strong>面向列的存储：</strong>Druid 单独存储和压缩每一列，只需要读取特定查询所需的列，支持快速扫描、排名和 groupBys。</li><li><strong>本地搜索索引：</strong>Druid 为字符串值创建倒排索引以进行快速搜索和过滤。</li><li><strong>流式传输和批量摄取：</strong>适用于 Apache Kafka、HDFS、AWS S3、流处理器等的开箱即用连接器。</li><li><strong>灵活的模式：</strong>Druid 优雅地处理不断变化的模式和<a href="https://druid.apache.org/docs/latest/ingestion/flatten-json" target="_blank" rel="noopener">嵌套数据</a>。</li><li><strong>时间优化分区：</strong>Druid根据时间对数据进行智能分区，基于时间的查询速度明显快于传统数据库。</li><li><strong>SQL 支持：</strong>除了其原生的<a href="https://druid.apache.org/docs/latest/querying/querying" target="_blank" rel="noopener">基于 JSON 的语言外</a>，Druid 还通过 HTTP 或 JDBC 使用<a href="https://druid.apache.org/docs/latest/querying/sql" target="_blank" rel="noopener">SQL</a>。</li><li><strong>水平扩展性：</strong>Druid 已<a href="https://druid.apache.org/druid-powered" target="_blank" rel="noopener">在生产中</a>用于每秒摄取数百万个事件、保留多年数据并提供亚秒级查询。</li><li><strong>操作简单：</strong>只需添加或删除服务器即可扩大或缩小规模，Druid 会自动重新平衡。容错架构围绕服务器故障进行路由。</li></ul><h3 id="1-4-数据格式"><a href="#1-4-数据格式" class="headerlink" title="1.4 数据格式"></a>1.4 数据格式</h3><ul><li><p>数据源（类似数据库中表的概念，存放一类数据）</p><ul><li>时间列：每个数据源都需要有的事件时间，是预聚合的主要依据；</li><li>维度列：用于标识事件和属性，用于聚合；</li><li>指标列：用于聚合计算的列，通常是关键量化指标；</li></ul><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%873.png" alt="图片3"></p></li><li><p>数据摄入</p><ul><li>实时摄入：Kafka；</li><li>批量摄入：HDFS、CSV等；</li></ul><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%874.png" alt="图片4"></p></li><li><p>数据查询</p><ul><li>原生Json查询，Http接口；</li><li><a href="https://druid.apache.org/docs/latest/querying/sql" target="_blank" rel="noopener">SQL</a>查询，除了标准 SQL 运算符之外，Druid 还支持独特的运算符，这些运算符利用其近似算法套件来提供快速计数、排名和分位数。</li></ul><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%875.png" alt="图片5"></p></li></ul><h3 id="1-5-数据保障"><a href="#1-5-数据保障" class="headerlink" title="1.5 数据保障"></a>1.5 数据保障</h3><p>Druid 旨在为需要每周 7 天、每天 24 小时不间断运行的应用程序提供支持。因此，德鲁伊拥有多项功能，可确保正常运行且不会丢失数据。</p><p><strong>数据复制</strong></p><blockquote><p>Druid 中的所有数据都复制了可配置的次数，因此单个服务器故障不会对查询产生影响。</p></blockquote><p><strong>独立服务</strong></p><blockquote><p>Druid 明确命名其所有主要服务，每个服务都可以根据用例进行微调。服务可以独立失败而不影响其他服务。例如，如果摄取服务失败，系统中不会加载新数据，但现有数据仍可查询。</p></blockquote><p><strong>自动数据备份</strong></p><blockquote><p>Druid 自动将所有索引数据备份到 HDFS 等文件系统。您可能会丢失整个 Druid 集群并从这些备份数据中快速恢复它。</p></blockquote><p><strong>滚动更新</strong></p><blockquote><p>您可以通过滚动更新在不停机的情况下更新 Druid 集群，也不会对最终用户产生影响。所有德鲁伊版本都向后兼容以前的版本。</p></blockquote><h3 id="1-6-优缺点"><a href="#1-6-优缺点" class="headerlink" title="1.6 优缺点"></a>1.6 优缺点</h3><p>优点：<br>1、支持的数据规模大(本地存储+DeepStorage–HDFS)；<br>2、性能高，列存压缩，预聚合加上倒排索引以及位图索引，秒级查询；<br>3、实时性高，可以通过kafka实时导入数据。</p><p>缺点：<br>1、灵活性适中，虽然维度之间随意组合，但不支持adhoc查询，不能自由组合查询，且丢失了明细数据（不采用roll-up情况下可以进行明细查询）；<br>2、易用性较差，不支持join，不支持更新，sql支持很弱(有些插件类似于pinot的PQL语言)，只能JSON格式查询，对于去重操作不能精准去重；<br>3、处理方式复杂，需要流处理引擎将数据join成宽表，维护相对复杂；对内存要求较高。</p><h3 id="1-7-适合的业务"><a href="#1-7-适合的业务" class="headerlink" title="1.7 适合的业务"></a>1.7 适合的业务</h3><ul><li>时序化数据：Druid 可以理解为时序数据库，所有的数据必须有时间字段；</li><li>实时数据接入可容忍丢数据(tranquility)：目前 tranquility 有丢数据的风险，所以建议实时和离线一起用，实时接当天数据，离线第二天把今天的数据全部覆盖，保证数据完备性；</li><li>OLAP 查询而不是 OLTP 查询：Druid 查询并发有限，不适合 OLTP 查询；</li><li>非精确的去重计算：目前 Druid 的去重都是非精确的；</li><li>无 Join 操作：Druid 适合处理星型模型的数据，不支持关联操作；</li><li>数据没有 update 更新操作，只对 segment 粒度进行覆盖：由于时序化数据的特点，Druid 不支持数据的更新。</li></ul><h3 id="1-8-高可用性"><a href="#1-8-高可用性" class="headerlink" title="1.8 高可用性"></a>1.8 高可用性</h3><ul><li><strong>MetaStore挂掉：</strong>无法感知新的Segment生成，不影响老数据；</li><li><strong>Indexing Service挂掉：</strong>无法执行新的任务，新数据无法摄入，不影响查询；</li><li><strong>Broker挂掉：</strong>本Broker节点不能查询，其他节点Broker继续服务，不影响数据摄入；</li><li><strong>Historical挂掉：</strong>Coordinator Node重分配该节点上segment到其它节点；</li><li><strong>Coordinator挂掉：</strong>Segment不会被加载和删除，选举新leader；</li><li><strong>Zookeeper挂掉：</strong>无法执行新的任务，新数据进不来；Broker有缓存。</li></ul><h2 id="二、基本架构"><a href="#二、基本架构" class="headerlink" title="二、基本架构"></a>二、基本架构</h2><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><h3 id="2-1-内部进程"><a href="#2-1-内部进程" class="headerlink" title="2.1 内部进程"></a>2.1 内部进程</h3><h4 id="2-1-1-Coordinator"><a href="#2-1-1-Coordinator" class="headerlink" title="2.1.1 Coordinator"></a>2.1.1 Coordinator</h4><p><strong>协调节点（Coordinator）</strong>主要负责历史节点的数据负载均衡，以及通过规则（Rule） 管理数据的生命周期。协调节点告诉历史节点加载新数据、卸载过期数据、复制数据、 和为了负载均衡移动数据。</p><p>Coordinator 是周期性运行的（由 druid.coordinator.period 配置指定，默认执行间隔为 60s）。因为需要评估集群的当前状态，才能决定应用哪种策略，所以，Coordinator 需要维护和 ZooKeeper 的连接，以获取集群的信息。而关于 Segment 和 Rule 的信息保存在了元数据库中，所以也需要维护与元数据库的连接。</p><h4 id="2-1-2-Overlord"><a href="#2-1-2-Overlord" class="headerlink" title="2.1.2 Overlord"></a>2.1.2 Overlord</h4><p><strong>统治者(Overlord）</strong>进程监视 MiddleManager 进程，并且是数据摄入 Druid 的控制器。他们负责将提取任务分配给 MiddleManagers 并协调 Segement 发布，包括接受、拆解、分配 Task，以及创建 Task 相关的锁，并返回 Task 的状态。</p><h4 id="2-1-3-Broker"><a href="#2-1-3-Broker" class="headerlink" title="2.1.3 Broker"></a>2.1.3 Broker</h4><p><strong>查询节点（Broker）</strong>接收客户端查询请求，并将这些查询转发给 Historicals 和 MiddleManagers。当 Brokers 从这些子查询中收到结果时，它们会合并这些结果并将它们返回给调用者。</p><h4 id="2-1-4-Router"><a href="#2-1-4-Router" class="headerlink" title="2.1.4 Router"></a>2.1.4 Router</h4><p>Router 将请求路由到Broker, Coordinators和Overlords，进程可以在 Brokers、Overlords 和 Coordinators 进程之上，提供一层统一的 API网关。Router 进程是<strong>可选</strong>进程，不过如果集群的数据规模已经达到了 TB级别，还是需要考虑启用的（<code>druid.router.managementProxy.enabled=true</code>）。因为一旦集群规模达到一定的数量级，那么发生故障的概率就会变得不容忽视，而 Router 支持将请求只发送给健康的节点，避免请求失败。同时，查询的响应时间和资源消耗，也会随着数据量的增长而变高，而 Router 支持设置查询的优先级和负载均衡策略，避免了大查询造成的队列堆积或查询热点等问题。</p><p>另外，Router 节点还可用于将查询路由到不同的 Broker 节点，便于实现冷热分层，以更好地应对超大规模数据集。默认情况下，Router 会根据设置的 Rule 规则，来路由查询请求。例如，如果将最近一个月的数据加载到热集群中，则最近一个月内的查询可以路由到一组专用 Broker，超出该时间范围的查询将被路由到另一组 Broker，如此便实现了查询的冷热隔离。</p><h4 id="2-1-5-Historical"><a href="#2-1-5-Historical" class="headerlink" title="2.1.5 Historical"></a>2.1.5 Historical</h4><p><strong>历史节点（Historical）</strong>加载已生成好的数据文件，以供数据查询。historical 节点是整个集群查询性能的核心所在，因为 historical 会承担绝大部分的 segment 查询。<br>Historical 进程从 Deep Storage 中下载 Segment，并响应有关这些 Segment 的查询请求（这些请求来自Broker 进程）。另外，Historical 进程不处理写入请求 。<br>Historical 进程采用了无共享架构设计，它知道如何去加载和删除 Segment，以及如何基于 Segment 来响应查询。因此，即便底层的 Deep Storage无法正常工作，Historical 进程还是能针对其已同步的 Segments，正常提供查询服务。</p><h4 id="2-1-6-MiddleManager"><a href="#2-1-6-MiddleManager" class="headerlink" title="2.1.6 MiddleManager"></a>2.1.6 MiddleManager</h4><p><strong>中间管理节点（MiddleManager）</strong>及时摄入实时数据，已生成 Segment 数据文件。<br>MiddleManager 进程是执行提交的任务的工作节点。Middle Managers 将任务转发给在不同 JVM 中运行的 Peon进程（如此，可以做到资源和日志的隔离）。MiddleManager、Peon、Task 的对应关系是，每个 Peon 进程一次只能运行一个Task 任务，但一个 MiddleManager 却可以管理多个 Peon 进程。</p><p>Druid进程可以按照您喜欢的任何方式进行部署，但是为了便于部署，我们建议将其组织为三种服务器类型：</p><ul><li><strong>Master</strong>: <code>Coordinator</code>, <code>Overload</code> 运行协调器和领主进程，管理数据可用性和接收；</li><li><strong>Query</strong>: <code>Broker</code> and <code>Router</code>，行代理和可选的路由器进程，处理来自外部客户端的查询；</li><li><strong>Data</strong>: <code>Historical</code> and <code>MiddleManager</code>，运行Historical和MiddleManager进程，执行提取工作负载并存储所有可查询的数据。</li></ul><h3 id="2-2-外部依赖"><a href="#2-2-外部依赖" class="headerlink" title="2.2 外部依赖"></a>2.2 外部依赖</h3><h4 id="2-2-1-深度储存"><a href="#2-2-1-深度储存" class="headerlink" title="2.2.1 深度储存"></a>2.2.1 深度储存</h4><p><strong>深度储存（Deep Storage）</strong>：存放生成的 Segment 数据文件，并供历史服务器下载， 对于单节点集群可以是本地磁盘，而对于分布式集群一般是 HDFS。</p><h4 id="2-2-2-元数据库"><a href="#2-2-2-元数据库" class="headerlink" title="2.2.2 元数据库"></a>2.2.2 元数据库</h4><p><strong>元数据库（Metadata Storage）</strong>，存储 Druid 集群的元数据信息，比如 Segment 的相关信息，一 般用 MySQL 或 PostgreSQL。</p><h4 id="2-2-3-Zookeeper"><a href="#2-2-3-Zookeeper" class="headerlink" title="2.2.3 Zookeeper"></a>2.2.3 Zookeeper</h4><p><strong>Zookeeper</strong>：为 Druid 集群提供以执行协调服务。如内部服务的监控，协调和领导者选举。</p><p>涵盖了以下的几个主要特性：</p><blockquote><p>Coordinator 节点的 Leader 选举；<br>Historical 节点发布 Segment 的协议；<br>Coordinator 和 Historical 之间 load/drop Segment 的协议；<br>Overlord 节点的 Leader 选举；<br>Overlord 和 MiddleManager 之间的 Task 管理。</p></blockquote><h3 id="2-3-Druid-主要功能特性"><a href="#2-3-Druid-主要功能特性" class="headerlink" title="2.3 Druid 主要功能特性"></a>2.3 Druid 主要功能特性</h3><p>Druid的核心体系结构结合了<strong>数据仓库</strong>，<strong>时间序列数据库</strong>和<strong>日志搜索系统</strong>的思想。Druid的一些主要功能是：</p><ol><li>列式存储格式。<br>Druid使用面向列的存储，这意味着它仅需要加载特定查询所需的确切列。这极大地提高了仅命中几列的查询的速度。此外，每列都针对其特定数据类型进行了优化存储，从而支持快速扫描和聚合。</li><li>可扩展的分布式系统。<br>Druid通常部署在数十到数百台服务器的群集中，并且可以提供每秒数百万条记录的接收速率，数万亿条记录的保留以及亚秒级到几秒的查询延迟。</li><li>大规模并行处理。<br>Druid可以在整个集群中并行处理查询。</li><li>实时或批量摄取。<br>Druid可以实时（批量获取被查询的数据）或批量提取数据。</li><li>自我修复，自我平衡，易于操作。<br>作为操作员，要扩展或扩展集群，只需添加或删除服务器，集群就会在后台自动重新平衡自身，而不会造成任何停机。如果任何Druid服务器出现故障，系统将自动绕过损坏，直到可以更换这些服务器为止。Druid设计为24*7全天候运行，而无需出于任何原因而导致计划内停机，包括配置更改和软件更新。</li><li>云原生的容错架构，不会丢失数据。<br>一旦Druid摄取了数据，副本就安全地存储在深度存储（通常是云存储，HDFS或共享文件系统）中。即使每台Druid服务器发生故障，也可以从深度存储中恢复数据。对于仅影响少数Druid服务器的有限故障，复制可确保在系统恢复时仍可进行查询。</li><li>用于快速过滤的索引。<br>Druid使用CONCISE或 Roaring压缩的位图索引来创建索引，以支持快速过滤和跨多列搜索。</li><li>基于时间的分区。<br>Druid首先按时间对数据进行分区，然后可以根据其他字段进行分区。这意味着基于时间的查询将仅访问与查询时间范围匹配的分区。这将大大提高基于时间的数据的性能。</li><li>近似算法。<br>Druid包括用于近似计数区别，近似排名以及近似直方图和分位数计算的算法。这些算法提供有限的内存使用量，通常比精确计算要快得多。对于精度比速度更重要的情况，Druid还提供了精确的计数区别和精确的排名。</li><li>摄取时自动汇总。<br>Druid可选地在摄取时支持数据汇总。这种汇总会部分地预先聚合您的数据，并可以节省大量成本并提高性能。</li></ol><h2 id="三、架构设计"><a href="#三、架构设计" class="headerlink" title="三、架构设计"></a>三、架构设计</h2><h3 id="3-1-存储设计"><a href="#3-1-存储设计" class="headerlink" title="3.1 存储设计"></a>3.1 存储设计</h3><h4 id="3-1-1-数据源和段"><a href="#3-1-1-数据源和段" class="headerlink" title="3.1.1 数据源和段"></a>3.1.1 数据源和段</h4><p>Datasources and segments</p><p>Druid 数据存储在 <strong>datasources</strong> 中，类似于传统RDBMS中的表。每个数据源都按时间分区，并且可以选择按其他属性进一步分区。每个时间范围都称为 <code>块</code>chunk（例如，如果您的数据源按天划分，则为一天）。在一个chunk内，数据被划分为一个或多个 <code>段</code>segments。每个段都是单个文件，通常包含多达几百万行的数据。由于细分是按时间块组织的，因此有时将段视为存在于如下时间线上是有帮助的：</p><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><p>数据源可能具有从几个段到数十万甚至数百万个段的任何位置。每个段都是从在MiddleManager上创建开始的，并且时段是可变的且未提交的。段构建过程包括以下步骤，旨在生成紧凑且支持快速查询的数据文件：</p><ul><li>转换为列格式</li><li>使用位图索引编制索引</li><li>使用各种算法进行压缩<ul><li>字符串列的ID存储最小化的字典编码</li><li>位图索引的位图压缩</li><li>所有列的类型感知压缩</li></ul></li></ul><p>分段会定期提交和发布。此时，它们被写入深度存储 <a href="https://druid.apache.org/docs/latest/design/architecture.html#deep-storage" target="_blank" rel="noopener">deep storage</a>，变得不可变，并从MiddleManagers迁移到Historical进程。有关该段的条目也将写入到元数据存储 <a href="https://druid.apache.org/docs/latest/design/architecture.html#metadata-storage" target="_blank" rel="noopener">metadata store</a>中。该条目是有关该段的元数据的自描述位，包括诸如段的模式，其大小以及其在深度存储上的位置之类的信息。这些条目是协调器用来了解集群上应该有哪些数据的内容。</p><p>有关段文件格式的详细信息，请参阅<a href="https://druid.apache.org/docs/latest/design/segments.html" target="_blank" rel="noopener">段文件</a>。<br>有关在Druid中对数据建模的详细信息，请参见<a href="https://druid.apache.org/docs/latest/ingestion/schema-design.html" target="_blank" rel="noopener">模式设计</a>。</p><h4 id="3-1-2-索引和切换"><a href="#3-1-2-索引和切换" class="headerlink" title="3.1.2 索引和切换"></a>3.1.2 索引和切换</h4><p>Indexing and handoff</p><p><strong>索引</strong> 是创建新段的机制，<strong>切换</strong> 是它们被发布并开始由历史进程提供服务的机制。该机制在索引端的工作方式如下：</p><p>1）索引任务开始运行并生成新段。必须先在索引任务构建段之前确定段的标识符，对于一个追加数据类型的任务（例如Kafka任务或者其他追加模式的索引任务），这将通过调用Overlord的<code>allocate</code> API来在现有的段集合中添加一个新的分区。对于一个重写类型的任务（例如Hadoop任务，或者一个非追加模式的索引任务）这是通过锁定间隔并创建新的版本号和新的段集来完成的。<br>2）如果一个索引任务是实时任务（像kafka任务），那么段在此刻可以被立即查询，它是可用的，但是未发布。<br>3）索引任务完成对段的数据读取后，会将其推入深度存储，然后通过将记录写入元数据存储来发布。<br>4）如果索引任务是实时任务，则此时它等待<code>Historical</code>进程加载段。如果索引任务不是实时任务，它将立即退出。</p><p>在<code>Coordinator</code>和<code>Historical</code>方面：</p><p>1）Coordinator定期（默认情况下，每1分钟）摘取元数据存储以查找新发布的段。<br>2）当Coordinator发现一个段是发布且可以被使用的、但是不可用的状态时，它会选个一个Historical进程来加载这个段；<br>3）Historical加载这个段并开始为其服务；<br>4）此时，如果索引任务正在等待切换，它将退出。</p><h4 id="3-1-3-段标识符"><a href="#3-1-3-段标识符" class="headerlink" title="3.1.3 段标识符"></a>3.1.3 段标识符</h4><p>Segment identifiers</p><p><strong>段</strong> 包含一个由四部分组成的标识符，包含以下组件：</p><ul><li>数据源名称；</li><li>时间间隔（包含该段的时间块，这与摄取时指定的 <code>segmentGranularity</code> 有关）；</li><li>版本号（通常是与段集首次启动时间相对应的 ISO8601 时间戳）；</li><li>分区号（一个整数，在<code>数据源+间隔+版本</code>中唯一；可能不一定是连续的）。</li></ul><p>例如这个一个段标识符，数据源为 <code>clarity-cloud0</code>, 时间块为 <code>2018-05-21T16:00:00.000Z/2018-05-21T17:00:00.000Z</code>, 版本为 <code>2018-05-21T15:56:09.909Z</code> 以及分区编号为 <code>1</code>:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1</span><br><span class="line">Copy</span><br></pre></td></tr></table></figure><p>分区号为0的段（块中的第一个分区）忽略分区号，如下例所示，该段与上一个时间块位于同一时间块中，但分区号为0而不是1：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909</span><br></pre></td></tr></table></figure><h4 id="3-1-4-段版本"><a href="#3-1-4-段版本" class="headerlink" title="3.1.4 段版本"></a>3.1.4 段版本</h4><p>Segment versioning</p><p>您可能想知道上一节中描述的<strong>版本号</strong>是用来做什么的。或者，你可能不想知道，在这种情况下对你有好处，你可以跳过这一节！</p><p>它支持批处理模式覆盖。在Druid中，如果您所做的只是附加数据，那么每个时间块只有一个版本。但是当您覆盖数据时，幕后发生的事情是，使用相同的数据源、相同的时间间隔、但更高的版本号创建一组新的段。这向Druid系统的其他部分发出了一个信号：旧版本应该从集群中删除，新版本应该替换它。</p><p>这个切换对用户来说似乎是瞬间发生的，因为Druid通过首先加载新数据（但不允许查询它）来处理这个问题，然后在新数据全部加载后，将所有新查询切换为使用这些新段。然后它会在几分钟后丢弃旧的段。</p><h4 id="3-1-5-段生命周期"><a href="#3-1-5-段生命周期" class="headerlink" title="3.1.5 段生命周期"></a>3.1.5 段生命周期</h4><p>Segment lifecycle</p><p>每个段都有一个生命周期，涉及以下三个主要领域：</p><p>1）<strong>元数据存储</strong>：段的元数据（一个小的JSON，通常不超过几个KB）在段构建完成后存储在元数据存储中，将段的记录插入到元数据存储中称为<strong>发布(Publishing)</strong>。这些元数据记录中有一个 <code>used</code> 的布尔标识，控制着段是否可查询。被实时任务创建的段在发布之前是可用的，因为它们仅在完成之时发布，并且不再接受额外的数据行。</p><p>2）<strong>深度存储</strong>：一旦构建了一个段，在将元数据发布到元数据存储之前就立刻将段数据文件推送到深度存储。</p><p>3）<strong>可查询性</strong>：在某些Druid数据服务器上段是可以进行查询的，如实时任务或Historical进程。</p><p>可以使用Druid SQL查询 <code>sys.segments</code> 表检查当前活动段的状态，它包括以下标志：</p><ul><li><code>is_published</code>: 如果段元数据已发布到元数据存储且 <code>used</code> 是true的话，则为true；</li><li><code>is_available</code>: 如果段当前可用于查询（实时任务或Historical进程），则为true；</li><li><code>is_realtime</code>: 如果段仅在实时任务上可用，则为true。对于使用实时摄取的数据源，这通常从true开始，然后在发布和切换段时变为false；</li><li><code>is_overshadowed</code>: 如果段已发布（ <code>used</code> 设置为true），并且被某些其他已发布段完全覆盖，则为true。一般来说，这是一个过渡状态，处于该状态的段很快将其 <code>used</code> 标志自动设置为false；</li></ul><h4 id="3-2-6-查询处理"><a href="#3-2-6-查询处理" class="headerlink" title="3.2.6 查询处理"></a>3.2.6 查询处理</h4><p>查询分布在 Druid 集群中，并由 Broker 管理。查询首先进入<a href="http://www.apache-druid.cn/Design/Broker.html" target="_blank" rel="noopener">Broker</a>，Broker首先鉴别哪些段可能与本次查询有关。 段的列表总是按照时间进行筛选和修剪的，当然也可能由其他属性，具体取决于数据源的分区方式。然后，Broker将确定哪些<a href="http://www.apache-druid.cn/Design/Historical.html" target="_blank" rel="noopener">Historical</a>和<a href="http://www.apache-druid.cn/Design/MiddleManager.html" target="_blank" rel="noopener">MiddleManager</a>为这些段提供服务、并向每个进程发送一个子查询。 Historical和MiddleManager进程接收查询、处理查询并返回结果，Broker将接收到的结果合并到一起形成最后的结果集返回给调用者。</p><p>时间和属性精简是Druid限制每个查询扫描数据量的一个重要方法，但不是唯一的方法。对于比Broker更细粒度级别的精简筛选器，每个段中的索引结构允许Druid在查看任何数据行之前，找出哪些行（如果有的话）与筛选器集匹配。一旦Druid知道哪些行与特定查询匹配，它就只访问该查询所需的特定列。在这些列中，Druid可以从一行跳到另一行，避免读取与查询过滤器不匹配的数据。</p><p>因此，Druid使用三种不同的技术来最大化查询性能：</p><ul><li>精简每个查询访问的段；</li><li>在每个段中，使用索引标识必须访问哪些行；</li><li>在每个段中，只读取与特定查询相关的特定行和列。</li></ul><p>有关 Druid 如何执行查询的更多详细信息，请参阅<a href="https://druid.apache.org/docs/latest/querying/query-execution.html" target="_blank" rel="noopener">查询执行</a>文档。</p><h3 id="3-2-段设计"><a href="#3-2-段设计" class="headerlink" title="3.2 段设计"></a>3.2 段设计</h3><p>Apache Druid 将其索引存储在按时间分区的<strong>段文件中</strong>。在基本设置中，通常为每个时间间隔创建一个段文件，其中时间间隔可在 <code>granularitySpec</code> 的<code>segmentGranularity</code> 参数中配置。为了让 Druid 在繁重的查询负载下运行良好，段文件大小在 300MB-700MB 的推荐范围内很重要。如果你的段文件大于该范围，请考虑更改时间间隔的粒度，或者对数据进行分区，并在 <code>partitionsSpec</code>中调整 <code>targetPartitionSize</code>（此参数的建议起点是500万行）。有关更多信息，请参阅下面的<strong>分片部分</strong>和<a href="https://druid.apache.org/docs/latest/ingestion/hadoop.html#partitionsspec" target="_blank" rel="noopener">批处理摄取</a>文档的<strong>分区规范</strong>部分。</p><h4 id="3-2-1-段文件的核心数据结构"><a href="#3-2-1-段文件的核心数据结构" class="headerlink" title="3.2.1 段文件的核心数据结构"></a>3.2.1 段文件的核心数据结构</h4><p>在这里，我们描述段文件的内部结构，它本质上是<strong>列式</strong>的：每列的数据在单独的数据结构中。通过分别存储每一列，Druid可以通过只扫描查询实际需要的列来减少查询延迟。有三种基本列类型：<strong>时间戳列</strong>、<strong>维度列</strong>和<strong>指标列</strong>，如下图所示：</p><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%873.png" alt="图片3"></p><p>timestamp和metric列很简单：在底层，每个列都是用LZ4压缩的整数或浮点值数组。一旦查询知道需要选择哪些行，它只需解压缩这些行，提取相关行，然后使用所需的聚合运算符进行计算。与所有列一样，如果不查询一个列，则跳过该列的数据。</p><p>dimension列是不同的，因为它们支持过滤和聚合操作，所以每一个维度都需要以下三种数据结构：</p><p>1）一个将值（通常被当做字符串）映射到整数id的字典；<br>2）一个使用第1步的字典进行编码的列值的列表；<br>3）对于列中每一个不同的值，标识哪些行包含该值的位图。</p><p>为什么需要这三种数据结构？字典简单地将字符串值映射到整数id，以便于在（2）和（3）中可以紧凑的表示。（3）中的位图（也称<strong>倒排索引</strong>）可以进行快速过滤操作（特别是，位图便于快速进行AND和OR操作）。 最后，<code>GroupBy</code> 和 <code>TopN</code> 查询需要（2）中的值列表。换句话说，仅基于过滤器的聚合指标是不需要（2）中存储的维度值列表的。</p><p>要具体了解这些数据结构，请考虑上面示例数据中的<code>page</code>列,表示此维度的三个数据结构如下图所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1: Dictionary that encodes column values</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;Justin Bieber&quot;: 0,</span><br><span class="line">    &quot;Ke$ha&quot;:         1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">2: Column data</span><br><span class="line">  [0,</span><br><span class="line">   0,</span><br><span class="line">   1,</span><br><span class="line">   1]</span><br><span class="line"></span><br><span class="line">3: Bitmaps - one for each unique value of the column</span><br><span class="line">  value=&quot;Justin Bieber&quot;: [1,1,0,0]</span><br><span class="line">  value=&quot;Ke$ha&quot;:         [0,0,1,1]</span><br></pre></td></tr></table></figure><p><strong>多值列</strong></p><p>如果数据源使用多值列，那么段文件中的数据结构看起来有点不同。让我们假设在上面的例子中，第二行同时标记了<code>Ke$ha</code>和<code>Justin Bieber</code>主题。在这种情况下，这三种数据结构现在看起来如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1: Dictionary that encodes column values</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;Justin Bieber&quot;: 0,</span><br><span class="line">    &quot;Ke$ha&quot;:         1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">2: Column data</span><br><span class="line">  [0,</span><br><span class="line">   [0,1],  &lt;--Row value of multi-value column can have array of values</span><br><span class="line">   1,</span><br><span class="line">   1]</span><br><span class="line"></span><br><span class="line">3: Bitmaps - one for each unique value</span><br><span class="line">  value=&quot;Justin Bieber&quot;: [1,1,0,0]</span><br><span class="line">  value=&quot;Ke$ha&quot;:         [0,1,1,1]</span><br><span class="line">                            ^</span><br><span class="line">                            |</span><br><span class="line">                            |</span><br><span class="line">    Multi-value column has multiple non-zero entries</span><br></pre></td></tr></table></figure><p>请注意列数据和<code>Ke$ha</code>位图中第二行的更改。如果一行中的某一列有多个值，则其在<strong>列数据</strong>中的条目是一个数组。此外，在<strong>列数据</strong>中有n个值的行在位图中将有n个非零值项。</p><h4 id="3-2-2-SQL-兼容的空值处理"><a href="#3-2-2-SQL-兼容的空值处理" class="headerlink" title="3.2.2 SQL 兼容的空值处理"></a>3.2.2 SQL 兼容的空值处理</h4><p>默认情况下，Druid字符串维度列可以使用 <code>&quot;&quot;</code> 或者 <code>null</code> ，数值列和指标列则不能表示为 <code>null</code>，而是将 <code>null</code> 强制为 <code>0</code>。但是，Druid还提供了一个与SQL兼容的空值处理模式，必须在系统级别通过 <code>Druid.generic.useDefaultValueForNull</code> 启用，当设置为 <code>false</code> 时，此设置将允许Druid在接收数据时创建的段中：字符串列区分 <code>&quot;&quot;</code> 和 <code>null</code> ，数值列区分 <code>null</code> 和 <code>0</code>。</p><p>在这种模式下，字符串维度列不包含额外的列结构，只是为 <code>null</code> 保留额外的字典条目。但是，数值列将与一个附加位图一起存储在段中，该位图标识哪些行是 <code>null</code> 值。除了略微增加段大小之外，由于需要检查 <code>null</code> 的位图，SQL兼容的空值处理在查询时也会导致性能损失，此性能开销仅对实际包含<code>null</code>列的场景中存在。</p><h4 id="3-2-3-命名规则"><a href="#3-2-3-命名规则" class="headerlink" title="3.2.3 命名规则"></a>3.2.3 命名规则</h4><p>段标识符通常使用数据源、时间区间的开始时间（ISO 8601格式）、时间区间的结束时间（ISO 8601格式）和版本来构造。如果数据被额外的分片后超出了时间范围，则段标识符还将包含分区号。</p><p>一个示例段标识符可以是： <code>数据源名称_开始时间_结束时间_版本号_分区号</code></p><h4 id="3-2-4-段的组成"><a href="#3-2-4-段的组成" class="headerlink" title="3.2.4 段的组成"></a>3.2.4 段的组成</h4><p>在底层，一个段由以下几个文件组成：</p><ul><li><p><code>version.bin</code></p><p>4个字节，以整数表示当前段版本。 例如，对于v9段，版本为0x0、0x0、0x0、0x9</p></li><li><p><code>meta.smoosh</code></p><p>一个包含其他 <code>smoosh</code> 文件内容的元数据（文件名以及偏移量）文件</p></li><li><p><code>XXXXX.smoosh</code></p><p>这些文件中有一些是串联的二进制数据</p><p><code>smoosh</code> 文件代表 <strong>smooshed</strong> 在一起的多个文件，以减少必须打开用来容纳数据的文件描述符的数量，它们是最大为2GB的文件（以匹配Java中内存映射的ByteBuffer的限制）。<code>smoosh</code> 文件为数据中的每个列提供单独的文件，并在 <code>index.drd</code> 文件提供有关该段的额外元数据。</p><p>还有一个称为 <code>__time</code> 的特殊列，它表示该段的时间列。</p></li></ul><p>在代码库中，段具有内部格式版本。当前的句段格式版本为 <code>v9</code>。</p><h4 id="3-2-5-列的格式"><a href="#3-2-5-列的格式" class="headerlink" title="3.2.5 列的格式"></a>3.2.5 列的格式</h4><p>每列存储为两部分：</p><ul><li>Jackson序列化的列描述符（ColumnDescriptor）；</li><li>列二进制文件的其余部分。</li></ul><p><strong>列描述符</strong>本质上是一个对象，它允许我们使用Jackson的多态反序列化来添加新的有趣的序列化方法，并且对代码的影响最小。它由关于列的一些元数据（它是什么类型的，它是多值的，等等）和一列序列化/反序列化逻辑组成，这些逻辑可以反序列化二进制文件的其余部分。</p><h4 id="3-2-6-压缩"><a href="#3-2-6-压缩" class="headerlink" title="3.2.6 压缩"></a>3.2.6 压缩</h4><p>Druid 压缩字符串、long、float 和 double 列的值块，默认使用<a href="https://github.com/lz4/lz4-java" target="_blank" rel="noopener">LZ4</a>，字符串列和数字空值的位图使用<a href="https://github.com/RoaringBitmap/RoaringBitmap" target="_blank" rel="noopener">Roaring</a>压缩. 我们建议坚持使用这些默认值，除非使用您自己的数据和查询模式进行实验验证表明非默认选项在您的特定情况下会表现得更好。例如，对于字符串列中的位图，使用 Roaring 和 CONCISE 之间的差异对于高基数列最为明显。在这种情况下，Roaring 在匹配大量值的过滤器上要快得多，但在某些情况下，由于 Roaring 格式的开销，CONCISE 的占用空间较小（但在匹配大量值时仍然较慢）。目前，压缩配置在段级别而不是单个列，有关更多详细信息，请参阅<a href="https://druid.apache.org/docs/latest/ingestion/ingestion-spec.html#indexspec" target="_blank" rel="noopener">IndexSpec</a>。</p><h4 id="3-2-7-切分数据以创建段"><a href="#3-2-7-切分数据以创建段" class="headerlink" title="3.2.7 切分数据以创建段"></a>3.2.7 切分数据以创建段</h4><p><strong>数据分片</strong></p><p>对于同一数据源，同一时间间隔内可能存在多个段。这些段在一段时间内形成一个 <code>块</code> 。根据用于切分数据的 <code>shardSpec</code> 的类型，只有当一个 <code>块</code> 完成时，Druid查询才能完成。也就是说，如果一个块由3个段组成，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sampleData_2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z_v1_0</span><br><span class="line">sampleData_2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z_v1_1</span><br><span class="line">sampleData_2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z_v1_2</span><br></pre></td></tr></table></figure><p>在完成对间隔 <code>2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z</code> 的查询之前，必须加载所有3个段。</p><p>此规则的例外是使用<strong>线性切片规范</strong>。线性切片规范不会强制<strong>完整性</strong>，即使系统中没有加载切片，查询也可以完成。例如，如果您的实时摄取任务创建了3个使用线性切片规范进行分段的段，并且系统中只加载了其中的两个段，那么查询将只返回这两个段的结果。</p><h4 id="3-2-8-Schema更改"><a href="#3-2-8-Schema更改" class="headerlink" title="3.2.8 Schema更改"></a>3.2.8 Schema更改</h4><p><strong>替换段</strong></p><p>Druid使用数据源、间隔、版本和分区号唯一地标识段。只有在为某个时间粒度创建多个段时，分区号才在段id中可见。例如，如果有小时段，但一小时内的数据量超过单个段的容量，则可以为同一小时创建多个段。这些段将共享相同的数据源、间隔和版本，但具有线性增加的分区号。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">foo_2015-01-01/2015-01-02_v1_0</span><br><span class="line">foo_2015-01-01/2015-01-02_v1_1</span><br><span class="line">foo_2015-01-01/2015-01-02_v1_2</span><br></pre></td></tr></table></figure><p>在上述段的实例中，<code>dataSource = foo</code>, <code>interval = 2015-01-01/2015-01-02</code>, <code>version = v1</code>, and <code>partitionNum = 0</code>。 如果在以后的某个时间点，使用新的schema重新索引数据，则新创建的段将具有更高的版本id。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">foo_2015-01-01/2015-01-02_v2_0</span><br><span class="line">foo_2015-01-01/2015-01-02_v2_1</span><br><span class="line">foo_2015-01-01/2015-01-02_v2_2</span><br></pre></td></tr></table></figure><p>Druid批量索引任务（基于Hadoop或基于IndexTask）保证了间隔内的的原子更新。在我们的例子中，在 <code>2015-01-01/2015-01-02</code> 的所有 <code>v2</code> 段加载到Druid集群之前，查询只使用 <code>v1</code> 段。加载完所有 <code>v2</code> 段并可查询后，所有查询都将忽略 <code>v1</code> 段并切换到 <code>v2</code> 段。不久之后，<code>v1</code>段将从集群中卸载。</p><p>请注意，跨越多个段间隔的更新在每个间隔内都是原子的，但是在整个更新过程中它们不是原子的。例如，您有如下段：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">foo_2015-01-01/2015-01-02_v1_0</span><br><span class="line">foo_2015-01-02/2015-01-03_v1_1</span><br><span class="line">foo_2015-01-03/2015-01-04_v1_2</span><br></pre></td></tr></table></figure><p><code>v2</code> 段将在构建后立即加载到集群中，并在段重叠的时间段内替换 <code>v1</code> 段。在完全加载 <code>v2</code>段之前，集群可能混合了 <code>v1</code> 和 <code>v2</code> 段。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">foo_2015-01-01/2015-01-02_v1_0</span><br><span class="line">foo_2015-01-02/2015-01-03_v2_1</span><br><span class="line">foo_2015-01-03/2015-01-04_v1_2</span><br></pre></td></tr></table></figure><p>在这种情况下，查询可能会命中 <code>v1</code> 和 <code>v2</code> 段的混合.</p><h4 id="3-2-9-段之间的不同Schema"><a href="#3-2-9-段之间的不同Schema" class="headerlink" title="3.2.9 段之间的不同Schema"></a>3.2.9 段之间的不同Schema</h4><p>同一数据源的Druid段可能有不同的schema。如果一个字符串列（维度列）存在于一个段中而不是另一个段中，则涉及这两个段的查询仍然有效。对缺少维度的段的查询将表现为该维度只有空值。类似地，如果一个段有一个数值列（指标列），而另一个没有，那么查询缺少指标列的段通常会<strong>做正确的事情</strong>，在缺失的指标上做聚合操作也就是缺失的。</p><h3 id="3-3-进程和服务"><a href="#3-3-进程和服务" class="headerlink" title="3.3 进程和服务"></a>3.3 进程和服务</h3><h4 id="3-3-1-进程类型"><a href="#3-3-1-进程类型" class="headerlink" title="3.3.1 进程类型"></a>3.3.1 进程类型</h4><p>Druid有以下几种进程类型：</p><ul><li><a href="http://www.apache-druid.cn/Design/Coordinator.html" target="_blank" rel="noopener">Coordinator</a></li><li><a href="http://www.apache-druid.cn/Design/Overlord.html" target="_blank" rel="noopener">Overlord</a></li><li><a href="http://www.apache-druid.cn/Design/Broker.html" target="_blank" rel="noopener">Broker</a></li><li><a href="http://www.apache-druid.cn/Design/Historical.html" target="_blank" rel="noopener">Historical</a></li><li><a href="http://www.apache-druid.cn/Design/MiddleManager.html" target="_blank" rel="noopener">MiddleManager</a> 和 <a href="http://www.apache-druid.cn/Design/Peons.html" target="_blank" rel="noopener">Peons</a></li><li><a href="http://www.apache-druid.cn/Design/Indexer.html" target="_blank" rel="noopener">Indexer(可选)</a></li><li><a href="http://www.apache-druid.cn/Design/Router.html" target="_blank" rel="noopener">Router(可选)</a></li></ul><h4 id="3-3-2-服务类型"><a href="#3-3-2-服务类型" class="headerlink" title="3.3.2 服务类型"></a>3.3.2 服务类型</h4><p>Druid进程可以按照您喜欢的任何方式部署，但是为了便于部署，我们建议将它们组织成三种服务器类型：</p><ul><li><strong>Master</strong></li><li><strong>Query</strong></li><li><strong>Data</strong></li></ul><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><h5 id="3-3-2-1-Master服务"><a href="#3-3-2-1-Master服务" class="headerlink" title="3.3.2.1 Master服务"></a>3.3.2.1 Master服务</h5><p>Master服务管理数据的摄取和可用性：它负责启动新的摄取作业并协调下面描述的”Data服务”上数据的可用性。</p><p>在Master服务中，功能分为两个进程：Coordinator和Overlord。</p><p><strong>Coordinator进程</strong></p><p><a href="http://www.apache-druid.cn/Design/Coordinator.html" target="_blank" rel="noopener">Coordinator</a> 监视Data服务中的Historical进程，它们负责将数据段分配给特定的服务器，并确保数据段在各个Historical之间保持良好的平衡。</p><p><strong>Overlord进程</strong></p><p><a href="http://www.apache-druid.cn/Design/Overlord.html" target="_blank" rel="noopener">Overlord</a> 监视Data服务中的MiddleManager进程，并且是Druid数据接收的控制器。它们负责将接收任务分配给MiddleManager，并协调数据段的发布。</p><h5 id="3-3-2-2-Query服务"><a href="#3-3-2-2-Query服务" class="headerlink" title="3.3.2.2 Query服务"></a>3.3.2.2 Query服务</h5><p>Query服务提供用户和客户端应用程序交互，将查询路由到Data服务或其他Query服务（以及可选的代理Master服务请求）。</p><p>在Query服务中，功能上分为两个进程：Broker和Router。</p><p><strong>Broker进程</strong></p><p><a href="http://www.apache-druid.cn/Design/Broker.html" target="_blank" rel="noopener">Broker</a>从外部客户端接收查询并将这些查询转发到Data服务器, 当Broker接收到子查询的结果时，它们会合并这些结果并将其返回给调用者。用户通常查询Broker，而不是直接查询Data服务中的Historical或MiddleManager进程。</p><p><strong>Router进程（可选）</strong></p><p>Router进程是<em>可选</em>的进程，相当于是为Druid Broker、Overlord和Coordinator提供一个统一的API网关。Router是可选的，因为也可以直接与Druid的Broker、Overlord和Coordinator。</p><p>Router还运行着<a href="http://www.apache-druid.cn/Operations/manageui.md" target="_blank" rel="noopener">Druid控制台</a>，一个用于数据源、段、任务、数据进程（Historical和MiddleManager）和Coordinator动态配置的管理UI。用户还可以在控制台中运行SQL和本地Druid查询。</p><h5 id="3-3-2-3-Data服务"><a href="#3-3-2-3-Data服务" class="headerlink" title="3.3.2.3 Data服务"></a>3.3.2.3 Data服务</h5><p>Data服务执行摄取作业并存储可查询数据。</p><p>在Data服务中，根据功能被分为两个进程：Historical和MiddleManager。</p><h5 id="3-3-2-4-Historical进程"><a href="#3-3-2-4-Historical进程" class="headerlink" title="3.3.2.4 Historical进程"></a>3.3.2.4 Historical进程</h5><p><a href="http://www.apache-druid.cn/Design/Historical.html" target="_blank" rel="noopener">Historical</a> 进程是处理存储和查询”Historical”数据（包括系统中已提交足够长时间的任何流数据）的工作程序。Historical进程从深层存储下载段并响应有关这些段的查询，他们不接受写操作。</p><h5 id="3-3-2-5-MiddleManager进程"><a href="#3-3-2-5-MiddleManager进程" class="headerlink" title="3.3.2.5 MiddleManager进程"></a>3.3.2.5 MiddleManager进程</h5><p><a href="http://www.apache-druid.cn/Design/MiddleManager.html" target="_blank" rel="noopener">MiddleManager</a> 进程处理将新数据摄取到集群中的操作, 他们负责读取外部数据源并发布新的Druid段。</p><p><strong>Peon进程</strong></p><p><a href="http://www.apache-druid.cn/Design/Peons.html" target="_blank" rel="noopener">Peon</a> 进程是由MiddleManagers生成的任务执行引擎。每个Peon运行一个单独的JVM，负责执行一个任务。Peon总是和运行它们的MiddleManager在同一个主机上运行。</p><h5 id="3-3-2-6-Indexer进程（可选）"><a href="#3-3-2-6-Indexer进程（可选）" class="headerlink" title="3.3.2.6 Indexer进程（可选）"></a>3.3.2.6 Indexer进程（可选）</h5><p><a href="http://www.apache-druid.cn/Design/Indexer.html" target="_blank" rel="noopener">Indexer</a> 进程是MiddleManager和Peon的替代方法。Indexer在单个JVM进程中作为单个线程运行任务，而不是为每个任务派生单独的JVM进程。</p><p>与MiddleManager + Peon系统相比，Indexer的设计更易于配置和部署，并且能够更好地实现跨任务的资源共享。Indexer是一种较新的功能，由于其内存管理系统仍在开发中，因此目前被指定为<a href="http://www.apache-druid.cn/Development/experimental.md" target="_blank" rel="noopener">实验性的特性</a>。它将在Druid的未来版本中继续成熟。</p><p>通常，您可以部署MiddleManagers或indexer，但不能同时部署两者。</p><h4 id="3-3-3-服务混合部署的利弊"><a href="#3-3-3-服务混合部署的利弊" class="headerlink" title="3.3.3 服务混合部署的利弊"></a>3.3.3 服务混合部署的利弊</h4><p>Druid进程可以基于上面描述的Master/Data/Query服务组织进行混合部署，这种部署方式通常会使大多数集群更好地利用硬件资源。</p><p>但是，对于非常大规模的集群，可以分割Druid进程，使它们在单独的服务器上运行，以避免资源争用。</p><p>本节介绍与进程混合部署相关的指南和配置参数。</p><h5 id="3-3-3-1-Coordinator和Overlord"><a href="#3-3-3-1-Coordinator和Overlord" class="headerlink" title="3.3.3.1 Coordinator和Overlord"></a>3.3.3.1 Coordinator和Overlord</h5><p>Coordinator进程的工作负载往往随着集群中段的数量而增加。Overlord的工作量也会根据集群中的分段数而增加，但程度要比Coordinator小。</p><p>在具有非常大量的段的集群中，可以将Coordinator进程和Overlord进程分开，以便为Coordinator进程的分段平衡工作负载提供更多资源。</p><p><strong>统一进程</strong></p><p>通过设置 <code>druid.Coordinator.asOverlord.enabled</code> 属性，Coordinator进程和Overlord进程可以作为单个组合进程运行。</p><p>有关详细信息，请参阅<a href="http://www.apache-druid.cn/Configuration/configuration.html#Coordinator" target="_blank" rel="noopener">Coordinator配置</a>。</p><h5 id="3-3-3-2-Historical和MiddleManager"><a href="#3-3-3-2-Historical和MiddleManager" class="headerlink" title="3.3.3.2 Historical和MiddleManager"></a>3.3.3.2 Historical和MiddleManager</h5><p>对于更高级别的数据摄取或查询负载，将Historical进程和MiddleManager进程部署在不同的主机上以避免CPU和内存争用。</p><p>Historical还受益于为<strong>内存映射段</strong>提供可用内存，这也是分别部署Historical和MiddleManager进程的另一个原因。</p><h3 id="3-4-深度存储"><a href="#3-4-深度存储" class="headerlink" title="3.4 深度存储"></a>3.4 深度存储</h3><p>Apache Druid不提供的存储机制，深度存储是存储段的地方。深度存储基础结构定义了数据的持久性级别，只要Druid进程能够看到这个存储基础结构并获得存储在上面的段，那么无论丢失多少Druid节点，都不会丢失数据。如果段在深度存储层消失了，则这些段中存储的任何数据都将丢失。</p><h4 id="3-4-1-本地挂载"><a href="#3-4-1-本地挂载" class="headerlink" title="3.4.1 本地挂载"></a>3.4.1 本地挂载</h4><p>本地装载也可用于存储段。这使得您可以使用本地文件系统或任何可以在本地挂载的东西，如NFS、Ceph等来存储段。这是默认的深度存储实现。</p><p>为了使用本地装载进行深层存储，需要在公共配置中设置以下配置：</p><table><thead><tr><th>属性</th><th>可能的取值</th><th>描述</th><th>默认值</th></tr></thead><tbody><tr><td><code>druid.storage.type</code></td><td>local</td><td></td><td>必须设置</td></tr><tr><td><code>druid.storage.storageDirectory</code></td><td></td><td>存储段的目录</td><td>必须设置</td></tr></tbody></table><p>注意，通常应该将 <code>druid.storage.storageDirectory</code> 设置为与 <code>druid.segmentCache.locations</code> 和 <code>druid.segmentCache.infoDir</code> 不同的目录。</p><p>如果在本地模式下使用Hadoop Indexer，那么只需给它一个本地目录作为输出目录，它就可以工作了。</p><h4 id="3-4-2-S3适配"><a href="#3-4-2-S3适配" class="headerlink" title="3.4.2 S3适配"></a>3.4.2 S3适配</h4><p>请看<a href="http://www.apache-druid.cn/Configuration/core-ext/s3.md" target="_blank" rel="noopener">druid-s3-extensions</a>扩展文档</p><h4 id="3-4-3-HDFS"><a href="#3-4-3-HDFS" class="headerlink" title="3.4.3 HDFS"></a>3.4.3 HDFS</h4><p>请看<a href="http://www.apache-druid.cn/Configuration/core-ext/hdfs.md" target="_blank" rel="noopener">druid-hdfs-extensions</a>扩展文档</p><h4 id="3-4-4-其他深度存储"><a href="#3-4-4-其他深度存储" class="headerlink" title="3.4.4 其他深度存储"></a>3.4.4 其他深度存储</h4><p>对于另外的深度存储等，可以参见<a href="http://www.apache-druid.cn/Configuration/extensions.md" target="_blank" rel="noopener">扩展列表</a></p><h3 id="3-5-元数据存储"><a href="#3-5-元数据存储" class="headerlink" title="3.5 元数据存储"></a>3.5 元数据存储</h3><p>元数据存储是Apache Druid的一个外部依赖。Druid使用它来存储系统的各种元数据，但不存储实际的数据。下面有许多用于各种目的的表。</p><p>Derby是Druid的默认元数据存储，但是它不适合生产环境。<a href="http://www.apache-druid.cn/Configuration/core-ext/mysql.md" target="_blank" rel="noopener">MySQL</a>和<a href="http://www.apache-druid.cn/Configuration/core-ext/postgresql.md" target="_blank" rel="noopener">PostgreSQL</a>是更适合生产的元数据存储。</p><blockquote><p>元数据存储存储了Druid集群工作所必需的整个元数据。对于生产集群，考虑使用MySQL或PostgreSQL而不是Derby。此外，强烈建议设置数据库的高可用，因为如果丢失任何元数据，将无法恢复。</p></blockquote><h4 id="3-5-1-使用Derby"><a href="#3-5-1-使用Derby" class="headerlink" title="3.5.1 使用Derby"></a>3.5.1 使用Derby</h4><p>将以下内容添加到您的Druid配置中：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">druid.metadata.storage.type=derby</span><br><span class="line">druid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527//opt/var/druid_state/derby;create=true</span><br><span class="line">Copy</span><br></pre></td></tr></table></figure><h4 id="3-5-2-MySQL"><a href="#3-5-2-MySQL" class="headerlink" title="3.5.2 MySQL"></a>3.5.2 MySQL</h4><p>参见<a href="http://www.apache-druid.cn/Configuration/core-ext/mysql.md" target="_blank" rel="noopener">mysql-metadata-storage</a>扩展文档</p><h4 id="5-3-3-PostgreSQL"><a href="#5-3-3-PostgreSQL" class="headerlink" title="5.3.3 PostgreSQL"></a>5.3.3 PostgreSQL</h4><p>参见<a href="http://www.apache-druid.cn/Configuration/core-ext/postgresql.md" target="_blank" rel="noopener">postgresql-metadata-storage</a>扩展文档</p><h4 id="5-3-4-添加自定义的数据库连接池属性"><a href="#5-3-4-添加自定义的数据库连接池属性" class="headerlink" title="5.3.4 添加自定义的数据库连接池属性"></a>5.3.4 添加自定义的数据库连接池属性</h4><p>注意：<code>username</code>、<code>password</code>、<code>connectURI</code>、<code>validationQuery</code>、<code>testOnBorrow</code> 这些属性不能通过 <code>druid.metadata.storage.connector.dbcp</code> 属性设置，这些必须通过 <code>druid.metadata.storage.connector</code> 属性设置。</p><p>支持的属性示例：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">druid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000</span><br><span class="line">druid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000</span><br><span class="line">Copy</span><br></pre></td></tr></table></figure><p>全部列表请查看 <a href="https://commons.apache.org/proper/commons-dbcp/configuration.html" target="_blank" rel="noopener">基本数据源配置</a></p><h4 id="5-3-5-元数据存储表"><a href="#5-3-5-元数据存储表" class="headerlink" title="5.3.5 元数据存储表"></a>5.3.5 元数据存储表</h4><h5 id="5-3-5-1-段表"><a href="#5-3-5-1-段表" class="headerlink" title="5.3.5.1 段表"></a>5.3.5.1 段表</h5><p>这是由 <code>druid.metadata.storage.tables.segments</code> 属性决定的。</p><p>此表存储了系统中可用段的元数据。<a href="http://www.apache-druid.cn/Design/Coordinator.html" target="_blank" rel="noopener">Coordinator</a> 对表进行轮询，以确定可用于在系统中查询的段集。该表有两个主功能列，其他列用于索引。</p><p><code>used</code> 列是布尔型标识。1表示集群应”使用”该段(即，应加载该段并可用于请求), 0表示不应将段主动加载到集群中。我们这样做是为了从集群中删除段，而不实际删除它们的元数据（这允许在出现问题时更简单地回滚）。</p><p><code>payload</code> 列存储一个JSON blob，该blob包含该段的所有元数据（存储在该payload中的某些数据与表中的某些列是冗余的，这是有意的）, 信息如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> <span class="attr">"dataSource"</span>:<span class="string">"wikipedia"</span>,</span><br><span class="line"> <span class="attr">"interval"</span>:<span class="string">"2012-05-23T00:00:00.000Z/2012-05-24T00:00:00.000Z"</span>,</span><br><span class="line"> <span class="attr">"version"</span>:<span class="string">"2012-05-24T00:10:00.046Z"</span>,</span><br><span class="line"> <span class="attr">"loadSpec"</span>:&#123;</span><br><span class="line">    <span class="attr">"type"</span>:<span class="string">"s3_zip"</span>,</span><br><span class="line">    <span class="attr">"bucket"</span>:<span class="string">"bucket_for_segment"</span>,</span><br><span class="line">    <span class="attr">"key"</span>:<span class="string">"path/to/segment/on/s3"</span></span><br><span class="line"> &#125;,</span><br><span class="line"> <span class="attr">"dimensions"</span>:<span class="string">"comma-delimited-list-of-dimension-names"</span>,</span><br><span class="line"> <span class="attr">"metrics"</span>:<span class="string">"comma-delimited-list-of-metric-names"</span>,</span><br><span class="line"> <span class="attr">"shardSpec"</span>:&#123;<span class="attr">"type"</span>:<span class="string">"none"</span>&#125;,</span><br><span class="line"> <span class="attr">"binaryVersion"</span>:<span class="number">9</span>,</span><br><span class="line"> <span class="attr">"size"</span>:size_of_segment,</span><br><span class="line"> <span class="attr">"identifier"</span>:<span class="string">"wikipedia_2012-05-23T00:00:00.000Z_2012-05-24T00:00:00.000Z_2012-05-23T00:10:00.046Z"</span></span><br><span class="line">&#125;</span><br><span class="line">Copy</span><br></pre></td></tr></table></figure><p>请注意，此blob的格式可以而且将不时地更改。</p><h5 id="5-3-5-2-规则表"><a href="#5-3-5-2-规则表" class="headerlink" title="5.3.5.2 规则表"></a>5.3.5.2 规则表</h5><p>规则表用于存储有关段应在何处着陆的各种规则。<a href="http://www.apache-druid.cn/Design/Coordinator.html" target="_blank" rel="noopener">Coordinator</a> 在对集群进行段（重）分配决策时使用这些规则。</p><h5 id="5-3-5-3-配置表"><a href="#5-3-5-3-配置表" class="headerlink" title="5.3.5.3 配置表"></a>5.3.5.3 配置表</h5><p>配置表用于存储运行时的配置对象。我们还没有很多这样的机制，我们也不确定是否会继续使用这种机制，但这是一种在运行时跨集群更改一些配置参数的方法的开始。</p><h5 id="5-3-5-4-任务相关的表"><a href="#5-3-5-4-任务相关的表" class="headerlink" title="5.3.5.4 任务相关的表"></a>5.3.5.4 任务相关的表</h5><p>在管理任务时，<a href="http://www.apache-druid.cn/Design/Overlord.html" target="_blank" rel="noopener">Overlord</a> 和 <a href="http://www.apache-druid.cn/Design/MiddleManager.html" target="_blank" rel="noopener">MiddleManager</a> 还创建和使用了许多表。</p><h5 id="5-3-5-5-审计表"><a href="#5-3-5-5-审计表" class="headerlink" title="5.3.5.5 审计表"></a>5.3.5.5 审计表</h5><p>审核表用于存储配置更改的历史记录，例如<a href="http://www.apache-druid.cn/Design/Coordinator.html" target="_blank" rel="noopener">Coordinator</a> 所做的规则更改和其他配置更改。</p><p>只有以下角色才能访问元数据存储：</p><ul><li>索引服务进程（如果有）</li><li>实时进程（如果有）</li><li>协调程序</li></ul><p>因此，您只需要为这些计算机授予访问元数据存储的权限（例如，在AWS安全组中）。</p><h3 id="3-6-ZooKeeper"><a href="#3-6-ZooKeeper" class="headerlink" title="3.6 ZooKeeper"></a>3.6 ZooKeeper</h3><p>Apache Druid使用<a href="http://zookeeper.apache.org/" target="_blank" rel="noopener">Apache ZooKeeper</a> 来管理整个集群状态。通过ZK来进行的操作有：</p><ul><li><a href="http://www.apache-druid.cn/Design/Coordinator.html" target="_blank" rel="noopener">Coordinator</a> Leader选举</li><li><a href="http://www.apache-druid.cn/Design/Historical.html" target="_blank" rel="noopener">Historical</a> 段发布协议</li><li><a href="http://www.apache-druid.cn/Design/Coordinator.html" target="_blank" rel="noopener">Coordinator</a> 和 <a href="http://www.apache-druid.cn/Design/Historical.html" target="_blank" rel="noopener">Historical</a> 之间的段加载/删除</li><li><a href="http://www.apache-druid.cn/Design/Overlord.html" target="_blank" rel="noopener">Overlord</a> Leader选举</li><li><a href="http://www.apache-druid.cn/Design/Overlord.html" target="_blank" rel="noopener">Overlord</a>和<a href="http://www.apache-druid.cn/Design/MiddleManager.html" target="_blank" rel="noopener">MiddleManager</a>任务管理</li></ul><h4 id="3-6-1-Coordinator-Leader选举"><a href="#3-6-1-Coordinator-Leader选举" class="headerlink" title="3.6.1 Coordinator Leader选举"></a>3.6.1 Coordinator Leader选举</h4><p>我们使用 <strong>Curator LeadershipLatch</strong> 进行Leader选举：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$&#123;druid.zk.paths.coordinatorPath&#125;/_COORDINATOR</span><br></pre></td></tr></table></figure><h4 id="3-6-2-Historical和Realtime之间的段发布"><a href="#3-6-2-Historical和Realtime之间的段发布" class="headerlink" title="3.6.2 Historical和Realtime之间的段发布"></a>3.6.2 Historical和Realtime之间的段发布</h4><p><code>announcementsPath</code> 和 <code>servedSegmentsPath</code> 这两个参数用于这个功能。</p><p>所有的 <a href="http://www.apache-druid.cn/Design/Historical.html" target="_blank" rel="noopener">Historical</a> 进程都将它们自身发布到 <code>announcementsPath</code>, 具体来说它们将在以下路径创建一个临时的ZNODE：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$&#123;druid.zk.paths.announcementsPath&#125;/$&#123;druid.host&#125;</span><br></pre></td></tr></table></figure><p>这意味着Historical节点可用。它们也将随后创建一个ZNODE:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$&#123;druid.zk.paths.servedSegmentsPath&#125;/$&#123;druid.host&#125;</span><br></pre></td></tr></table></figure><p>当它们加载段时，它们将在以下路径附着一个临时的ZNODE：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$&#123;druid.zk.paths.servedSegmentsPath&#125;/$&#123;druid.host&#125;/_segment_identifier_</span><br></pre></td></tr></table></figure><p>然后，<a href="http://www.apache-druid.cn/Design/Coordinator.html" target="_blank" rel="noopener">Coordinator</a> 和 <a href="http://www.apache-druid.cn/Design/Broker.html" target="_blank" rel="noopener">Broker</a> 之类的进程可以监视这些路径，以查看哪些进程当前正在为哪些段提供服务。</p><h4 id="3-6-3-Coordinator和Historical之间的段加载-删除"><a href="#3-6-3-Coordinator和Historical之间的段加载-删除" class="headerlink" title="3.6.3 Coordinator和Historical之间的段加载/删除"></a>3.6.3 Coordinator和Historical之间的段加载/删除</h4><p><code>loadQueuePath</code> 参数用于这个功能。</p><p>当 <a href="http://www.apache-druid.cn/Design/Coordinator.html" target="_blank" rel="noopener">Coordiantor</a> 决定一个 <a href="http://www.apache-druid.cn/Design/Historical.html" target="_blank" rel="noopener">Historical</a> 进程应该加载或删除一个段时，它会将一个临时znode写到:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&#123;druid.zk.paths.loadQueuePath&#125;/_host_of_historical_process/_segment_identifier</span></span><br></pre></td></tr></table></figure><p>这个znode将包含一个payload，它向Historical进程指示它应该如何处理给定的段。当Historical进程完成任务时，它将删除znode，以便向Coordinator表示它已经完成处理。</p><h2 id="四、Druid-部署"><a href="#四、Druid-部署" class="headerlink" title="四、Druid 部署"></a>四、Druid 部署</h2><h3 id="4-1-单机模式部署"><a href="#4-1-单机模式部署" class="headerlink" title="4.1 单机模式部署"></a>4.1 单机模式部署</h3><p>Druid 包含一组用于单机部署的参考配置和启动脚本，位于<code>conf/druid/single-server/</code>目录下</p><ul><li><code>nano-quickstart</code></li><li><code>micro-quickstart</code></li><li><code>small</code></li><li><code>medium</code></li><li><code>large</code></li><li><code>xlarge</code></li></ul><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Nano-Quickstart：1个CPU，4GB RAM</span><br><span class="line">    启动命令： bin/start-nano-quickstart</span><br><span class="line">    配置目录： conf/druid/single-server/nano-quickstart/*</span><br><span class="line">微型快速入门：4个CPU，16GB RAM</span><br><span class="line">    启动命令： bin/start-micro-quickstart</span><br><span class="line">    配置目录： conf/druid/single-server/micro-quickstart/*</span><br><span class="line">小型：8 CPU，64GB RAM（〜i3.2xlarge）</span><br><span class="line">    启动命令： bin/start-small</span><br><span class="line">    配置目录： conf/druid/single-server/small/*</span><br><span class="line">中：16 CPU，128GB RAM（〜i3.4xlarge）</span><br><span class="line">    启动命令： bin/start-medium</span><br><span class="line">    配置目录： conf/druid/single-server/medium/*</span><br><span class="line">大型：32 CPU，256GB RAM（〜i3.8xlarge）</span><br><span class="line">    启动命令： bin/start-large</span><br><span class="line">    配置目录： conf/druid/single-server/large/*</span><br><span class="line">超大型：64 CPU，512GB RAM（〜i3.16xlarge）</span><br><span class="line">    启动命令： bin/start-xlarge</span><br><span class="line">    配置目录： conf/druid/single-server/xlarge/*</span><br></pre></td></tr></table></figure></blockquote><h4 id="4-1-1-安装-jdk"><a href="#4-1-1-安装-jdk" class="headerlink" title="4.1.1 安装 jdk"></a>4.1.1 安装 jdk</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ tar xf downloads/jdk-8u301-linux-x64.tar.gz</span><br><span class="line">[hadoop@hadoop3 ~]$ vim .bash_profile</span><br><span class="line">export JAVA_HOME=/home/hadoop/jdk1.8.0_301</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~]$ source .bash_profile</span><br></pre></td></tr></table></figure><h4 id="4-1-2-准备-druid"><a href="#4-1-2-准备-druid" class="headerlink" title="4.1.2 准备 druid"></a>4.1.2 准备 druid</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~/downloads]$ wget https://dlcdn.apache.org/druid/0.21.1/apache-druid-0.21.1-bin.tar.gz</span><br><span class="line">https://dlcdn.apache.org/druid/0.21.1/apache-druid-0.21.1-bin.tar.gz</span><br><span class="line">https://downloads.apache.org/druid/0.21.1/apache-druid-0.21.1-bin.tar.gz</span><br><span class="line">[hadoop@hadoop3 ~]$ tar xf apache-druid-0.21.1-bin.tar.gz</span><br><span class="line">[hadoop@hadoop3 ~]$ cd apache-druid-0.21.1/</span><br></pre></td></tr></table></figure><h4 id="4-1-3-启动-druid"><a href="#4-1-3-启动-druid" class="headerlink" title="4.1.3 启动 druid"></a>4.1.3 启动 druid</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~/apache-druid-0.21.1]$ ./bin/start-nano-quickstart start</span><br></pre></td></tr></table></figure><p>如果端口冲突，可通过以下配置修改各服务端口信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cd conf/druid/single-server</span><br><span class="line"></span><br><span class="line">./nano-quickstart/</span><br><span class="line">├── _common</span><br><span class="line">│   ├── common.runtime.properties</span><br><span class="line">│   └── log4j2.xml</span><br><span class="line">├── broker</span><br><span class="line">│   └── runtime.properties      druid.plaintextPort=端口号</span><br><span class="line">├── coordinator-overlord</span><br><span class="line">│   └── runtime.properties      druid.plaintextPort=端口号</span><br><span class="line">├── historical</span><br><span class="line">│   └── runtime.properties      druid.plaintextPort=端口号</span><br><span class="line">├── middleManager</span><br><span class="line">│   └── runtime.properties      druid.plaintextPort=端口号</span><br><span class="line">└── router</span><br><span class="line">    └── runtime.properties      druid.plaintextPort=端口号</span><br></pre></td></tr></table></figure><p><strong>访问 WEB UI</strong></p><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%876.png" alt="图片6"></p><h3 id="4-2-集群模式部署"><a href="#4-2-集群模式部署" class="headerlink" title="4.2 集群模式部署"></a>4.2 集群模式部署</h3><p>Apache Druid 旨在部署为可扩展、容错的集群。</p><p>在这里，我们将设置一个简单的集群，这个简单的集群将具有：</p><ul><li>一个Master用于托管 Coordinator 和 Overlord 进程的主服务器；</li><li>两个Data运行Historical和 MiddleManager 进程的服务器；</li><li>一个Query服务器，托管 Broker 和 Router 进程。</li></ul><p><strong>服务规划</strong></p><table><thead><tr><th>主机名</th><th>druid角色</th><th>依赖服务</th></tr></thead><tbody><tr><td>hadoop1</td><td>Master（Coordinator、Overlord）</td><td>JDK1.8、zookeeper、Mysql、HDFS</td></tr><tr><td>hadoop2</td><td>Data（Historical、MiddleManager）</td><td>JDK1.8、zookeeper</td></tr><tr><td>hadoop3</td><td>Query（Broker、Router）、Data</td><td>JDK1.8、zookeeper</td></tr></tbody></table><p>在生产环境，推荐署多个Master和多个Query服务来满足容错。但现在可以快速的使用一个Master、一个Query服务器的方式先完成集群部署，后续添加Master、Query服务器。</p><p><strong>前置条件</strong></p><blockquote><p>HDFS、zookeeper和Mysql是可用的</p></blockquote><h4 id="4-2-1-安装-jdk"><a href="#4-2-1-安装-jdk" class="headerlink" title="4.2.1 安装 jdk"></a>4.2.1 安装 jdk</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/jdk-8u301-linux-x64.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ vim .bash_profile</span><br><span class="line">export JAVA_HOME=/home/hadoop/jdk1.8.0_301</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~]$ source .bash_profile</span><br></pre></td></tr></table></figure><h4 id="4-2-2-准备-druid"><a href="#4-2-2-准备-druid" class="headerlink" title="4.2.2 准备 druid"></a>4.2.2 准备 druid</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://dlcdn.apache.org/druid/0.21.1/apache-druid-0.21.1-bin.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ tar xf apache-druid-0.21.1-bin.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ cd apache-druid-0.21.1/</span><br></pre></td></tr></table></figure><h4 id="4-2-3-准备-mysql-账户"><a href="#4-2-3-准备-mysql-账户" class="headerlink" title="4.2.3 准备 mysql 账户"></a>4.2.3 准备 mysql 账户</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#创建druid数据库和账户</span><br><span class="line">mysql&gt; CREATE DATABASE druid DEFAULT CHARACTER SET utf8mb4;</span><br><span class="line">mysql&gt; CREATE USER &apos;druid&apos;@&apos;%&apos; IDENTIFIED BY &apos;druid&apos;;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON druid.* TO &apos;druid&apos;@&apos;%&apos;;</span><br></pre></td></tr></table></figure><h4 id="4-2-3-druid-配置"><a href="#4-2-3-druid-配置" class="headerlink" title="4.2.3 druid 配置"></a>4.2.3 druid 配置</h4><h5 id="4-2-3-1-元数据存储配置"><a href="#4-2-3-1-元数据存储配置" class="headerlink" title="4.2.3.1 元数据存储配置"></a>4.2.3.1 元数据存储配置</h5><p>修改 <code>conf/druid/cluster/_common/common.runtime.properties</code> 配置文件，添加mysql相关信息，记得关闭默认的derby配置</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">druid.extensions.loadList</span>=<span class="string">["mysql-metadata-storage"]</span></span><br><span class="line"><span class="meta">druid.metadata.storage.type</span>=<span class="string">mysql</span></span><br><span class="line"><span class="meta">druid.metadata.storage.connector.connectURI</span>=<span class="string">jdbc:mysql://&lt;host&gt;/druid</span></span><br><span class="line"><span class="meta">druid.metadata.storage.connector.user</span>=<span class="string">druid</span></span><br><span class="line"><span class="meta">druid.metadata.storage.connector.password</span>=<span class="string">diurd</span></span><br></pre></td></tr></table></figure><p>下载mysql驱动，存储到 <code>extensions/mysql-metadata-storage</code> 目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/apache-druid-0.21.1/extensions/mysql-metadata-storage]$ wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/5.1.48/mysql-connector-java-5.1.48.jar</span><br></pre></td></tr></table></figure><h5 id="4-2-3-2-深度存储配置"><a href="#4-2-3-2-深度存储配置" class="headerlink" title="4.2.3.2 深度存储配置"></a>4.2.3.2 深度存储配置</h5><p>Druid 依赖分布式文件系统或大对象 (blob) 存储来存储数据。最常用的深度存储实现是 S3 和 HDFS 。这里配置为HDFS，同样修改 <code>conf/druid/cluster/_common/common.runtime.properties</code> 配置文件</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">druid.extensions.loadList</span>=<span class="string">["mysql-metadata-storage", "druid-hdfs-storage"]</span></span><br><span class="line"></span><br><span class="line"><span class="meta">druid.host</span>=<span class="string">hadoop3</span></span><br><span class="line"><span class="comment">#druid.storage.type=local</span></span><br><span class="line"><span class="comment">#druid.storage.storageDirectory=var/druid/segments</span></span><br><span class="line"></span><br><span class="line"><span class="meta">druid.storage.type</span>=<span class="string">hdfs</span></span><br><span class="line"><span class="meta">druid.storage.storageDirectory</span>=<span class="string">/druid/segments</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#druid.indexer.logs.type=file</span></span><br><span class="line"><span class="comment">#druid.indexer.logs.directory=var/druid/indexing-logs</span></span><br><span class="line"></span><br><span class="line"><span class="meta">druid.indexer.logs.type</span>=<span class="string">hdfs</span></span><br><span class="line"><span class="meta">druid.indexer.logs.directory</span>=<span class="string">/druid/indexing-logs</span></span><br></pre></td></tr></table></figure><h5 id="4-2-3-3-配置-Zookeeper-连接"><a href="#4-2-3-3-配置-Zookeeper-连接" class="headerlink" title="4.2.3.3 配置 Zookeeper 连接"></a>4.2.3.3 配置 Zookeeper 连接</h5><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">druid.zk.service.host</span>=<span class="string">hadoop1:2181,hadoop2:2181,hadoop3:2181</span></span><br><span class="line"><span class="meta">druid.zk.paths.base</span>=<span class="string">/druid</span></span><br></pre></td></tr></table></figure><h5 id="4-2-3-4-配置连接Hadoop"><a href="#4-2-3-4-配置连接Hadoop" class="headerlink" title="4.2.3.4 配置连接Hadoop"></a>4.2.3.4 配置连接Hadoop</h5><p>因为前面深度存储配置为了HDFS，那么此时应该配置 Druid 以了解您的Hadoop集群：</p><ul><li>将 Hadoop 配置 XML（core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml）放在 Druid 进程的类路径上。您可以通过将它们复制到<code>conf/druid/cluster/_common/</code> 目录下来做到这一点。</li></ul><blockquote><p>请注意，若不需要使用 HDFS 深度存储来从 Hadoop 加载数据。例如，如果您的集群在 Amazon Web Services 上运行，我们建议使用 S3 进行深度存储，即使您使用 Hadoop 或 Elastic MapReduce 加载数据。</p></blockquote><h4 id="4-2-4-分发配置"><a href="#4-2-4-分发配置" class="headerlink" title="4.2.4 分发配置"></a>4.2.4 分发配置</h4><p>将修改好的druid配置分发到各节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ rsync -av apache-druid-0.21.1 hadoop@hadoop2:~/</span><br><span class="line">[hadoop@hadoop1 ~]$ rsync -av apache-druid-0.21.1 hadoop@hadoop3:~/</span><br></pre></td></tr></table></figure><blockquote><p>注意各节点的 <code>druid.host</code> 主机名，配置为可以通过该主机名访问的节点名称</p></blockquote><h4 id="4-2-5-启动各节点服务"><a href="#4-2-5-启动各节点服务" class="headerlink" title="4.2.5 启动各节点服务"></a>4.2.5 启动各节点服务</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">启动Master</span></span><br><span class="line">[hadoop@hadoop1 ~/apache-druid-0.21.1]$ ./bin/start-cluster-master-no-zk-server</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">启动Data</span></span><br><span class="line">[hadoop@hadoop2 ~/apache-druid-0.21.1]$ ./bin/start-cluster-data-server</span><br><span class="line">[hadoop@hadoop3 ~/apache-druid-0.21.1]$ ./bin/start-cluster-data-server</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">启动Query</span></span><br><span class="line">[hadoop@hadoop3 ~/apache-druid-0.21.1]$ ./bin/start-cluster-query-server</span><br></pre></td></tr></table></figure><p><strong>访问 WEB UI</strong>，Query节点8888端口</p><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%877.png" alt="图片7"></p><h4 id="4-2-6-druid-各服务默认资源占用"><a href="#4-2-6-druid-各服务默认资源占用" class="headerlink" title="4.2.6 druid 各服务默认资源占用"></a>4.2.6 druid 各服务默认资源占用</h4><table><thead><tr><th>角色</th><th>默认资源占用</th></tr></thead><tbody><tr><td>Coordinator</td><td>-Xms15g -Xmx15g</td></tr><tr><td>Overlord</td><td>Xms15g -Xmx15g</td></tr><tr><td>middleManager</td><td>Xms128m -Xmx128m</td></tr><tr><td>historical</td><td>-Xms8g -Xmx8g -XX:MaxDirectMemorySize=13g</td></tr><tr><td>broker</td><td>-Xms12g -Xmx12g -XX:MaxDirectMemorySize=6g</td></tr><tr><td>routers</td><td>-Xms1g -Xmx1g -XX:MaxDirectMemorySize=128m</td></tr></tbody></table><h4 id="4-2-7-druid-各服务默认端口"><a href="#4-2-7-druid-各服务默认端口" class="headerlink" title="4.2.7  druid 各服务默认端口"></a>4.2.7 druid 各服务默认端口</h4><p><strong>Master Server</strong></p><ul><li>1527 (Derby 元数据存储；如果您使用单独的元数据存储，如 MySQL 或 PostgreSQL，则不需要)</li><li>2181 (ZooKeeper，如果您使用单独的 ZooKeeper 集群，则不需要)</li><li>8081 (Coordinator)</li><li>8090 (Overlord)</li></ul><p><strong>Data Server</strong></p><ul><li>8083 (Historical)</li><li>8091, 8100–8199 (Druid Middle Manager)</li></ul><p><strong>Query Server</strong></p><ul><li>8082 (Broker)</li><li>8088 (Router, 如果使用)</li></ul><h3 id="4-3-加载示例数据"><a href="#4-3-加载示例数据" class="headerlink" title="4.3 加载示例数据"></a>4.3 加载示例数据</h3><p>这里演示一下从本地加载官方的示例数据，大概有以下几种方式</p><h4 id="4-3-1-通过控制台"><a href="#4-3-1-通过控制台" class="headerlink" title="4.3.1 通过控制台"></a>4.3.1 通过控制台</h4><p>Druid 包包含以下示例本机批处理摄取任务规范<code>quickstart/tutorial/wikipedia-index.json</code>，为方便起见，此处显示，已配置为读取<code>quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz</code>输入文件：</p><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%878.png" alt="图片8"></p><p>或者，在<code>Ingestion</code>视图中，单击<code>Tasks</code>旁边的省略号并选择<code>Submit JSON task</code>，粘贴以下内容。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"type"</span> : <span class="string">"index_parallel"</span>,</span><br><span class="line">  <span class="attr">"spec"</span> : &#123;</span><br><span class="line">    <span class="attr">"dataSchema"</span> : &#123;</span><br><span class="line">      <span class="attr">"dataSource"</span> : <span class="string">"wikipedia"</span>,</span><br><span class="line">      <span class="attr">"dimensionsSpec"</span> : &#123;</span><br><span class="line">        <span class="attr">"dimensions"</span> : [</span><br><span class="line">          <span class="string">"channel"</span>,</span><br><span class="line">          <span class="string">"cityName"</span>,</span><br><span class="line">          <span class="string">"comment"</span>,</span><br><span class="line">          <span class="string">"countryIsoCode"</span>,</span><br><span class="line">          <span class="string">"countryName"</span>,</span><br><span class="line">          <span class="string">"isAnonymous"</span>,</span><br><span class="line">          <span class="string">"isMinor"</span>,</span><br><span class="line">          <span class="string">"isNew"</span>,</span><br><span class="line">          <span class="string">"isRobot"</span>,</span><br><span class="line">          <span class="string">"isUnpatrolled"</span>,</span><br><span class="line">          <span class="string">"metroCode"</span>,</span><br><span class="line">          <span class="string">"namespace"</span>,</span><br><span class="line">          <span class="string">"page"</span>,</span><br><span class="line">          <span class="string">"regionIsoCode"</span>,</span><br><span class="line">          <span class="string">"regionName"</span>,</span><br><span class="line">          <span class="string">"user"</span>,</span><br><span class="line">          &#123; <span class="attr">"name"</span>: <span class="string">"added"</span>, <span class="attr">"type"</span>: <span class="string">"long"</span> &#125;,</span><br><span class="line">          &#123; <span class="attr">"name"</span>: <span class="string">"deleted"</span>, <span class="attr">"type"</span>: <span class="string">"long"</span> &#125;,</span><br><span class="line">          &#123; <span class="attr">"name"</span>: <span class="string">"delta"</span>, <span class="attr">"type"</span>: <span class="string">"long"</span> &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"timestampSpec"</span>: &#123;</span><br><span class="line">        <span class="attr">"column"</span>: <span class="string">"time"</span>,</span><br><span class="line">        <span class="attr">"format"</span>: <span class="string">"iso"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"metricsSpec"</span> : [],</span><br><span class="line">      <span class="attr">"granularitySpec"</span> : &#123;</span><br><span class="line">        <span class="attr">"type"</span> : <span class="string">"uniform"</span>,</span><br><span class="line">        <span class="attr">"segmentGranularity"</span> : <span class="string">"day"</span>,</span><br><span class="line">        <span class="attr">"queryGranularity"</span> : <span class="string">"none"</span>,</span><br><span class="line">        <span class="attr">"intervals"</span> : [<span class="string">"2015-09-12/2015-09-13"</span>],</span><br><span class="line">        <span class="attr">"rollup"</span> : <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"ioConfig"</span> : &#123;</span><br><span class="line">      <span class="attr">"type"</span> : <span class="string">"index_parallel"</span>,</span><br><span class="line">      <span class="attr">"inputSource"</span> : &#123;</span><br><span class="line">        <span class="attr">"type"</span> : <span class="string">"local"</span>,</span><br><span class="line">        <span class="attr">"baseDir"</span> : <span class="string">"quickstart/tutorial/"</span>,</span><br><span class="line">        <span class="attr">"filter"</span> : <span class="string">"wikiticker-2015-09-12-sampled.json.gz"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"inputFormat"</span> :  &#123;</span><br><span class="line">        <span class="attr">"type"</span>: <span class="string">"json"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"appendToExisting"</span> : <span class="literal">false</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"tuningConfig"</span> : &#123;</span><br><span class="line">      <span class="attr">"type"</span> : <span class="string">"index_parallel"</span>,</span><br><span class="line">      <span class="attr">"maxRowsPerSegment"</span> : <span class="number">5000000</span>,</span><br><span class="line">      <span class="attr">"maxRowsInMemory"</span> : <span class="number">25000</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该规范创建了一个名为<code>wikipedia</code>的数据源。</p><h4 id="4-3-2-通过命令行"><a href="#4-3-2-通过命令行" class="headerlink" title="4.3.2 通过命令行"></a>4.3.2 通过命令行</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/post-index-task --file quickstart/tutorial/wikipedia-index.json --url http://localhost:8081</span></span><br></pre></td></tr></table></figure><h4 id="4-3-3-HTTP-客户端"><a href="#4-3-3-HTTP-客户端" class="headerlink" title="4.3.3 HTTP 客户端"></a>4.3.3 HTTP 客户端</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -X <span class="string">'POST'</span> -H <span class="string">'Content-Type:application/json'</span> -d @quickstart/tutorial/wikipedia-index.json http://localhost:8081/druid/indexer/v1/task</span></span><br></pre></td></tr></table></figure><h3 id="4-4-数据查询方式"><a href="#4-4-数据查询方式" class="headerlink" title="4.4 数据查询方式"></a>4.4 数据查询方式</h3><h4 id="4-4-1-控制台查询"><a href="#4-4-1-控制台查询" class="headerlink" title="4.4.1 控制台查询"></a>4.4.1 控制台查询</h4><p>Druid 控制台包含一个视图，可以更轻松地构建和测试查询以及查看其结果。</p><p>控制台进入<code>Query</code>查询视图，在左则选择需要查询的数据源</p><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%879.png" alt="图片9"></p><p>展开左窗格中的<code>wikipedia</code>数据源树。我们将为页面维度创建一个查询。</p><p>单击<code>page</code>，然后从菜单中单击<strong>Show:page</strong>：</p><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%8710.png" alt="图片10"></p><p>SELECT 查询出现在查询编辑窗格中并立即运行。但是，在这种情况下，查询不返回任何数据，因为默认情况下查询会过滤最后一天的数据，而我们的数据远比这更旧。让我们移除过滤器。</p><p>在数据源树中，单击<code>__time</code>并<strong>删除过滤器</strong>。</p><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%8711.png" alt="图片11"></p><p>单击<strong>运行</strong>以运行查询。</p><p><img src="/2021/11/24/apache-druid-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%8712.png" alt="图片12"></p><p>默认情况下，由于<strong>智能查询限制</strong> 功能，控制台中的结果被限制在大约一百个左右。这有助于用户避免无意中运行返回过多数据的查询，这可能会使他们的系统不堪重负。</p><h4 id="4-4-2-dsql-查询-SQL"><a href="#4-4-2-dsql-查询-SQL" class="headerlink" title="4.4.2 dsql 查询 SQL"></a>4.4.2 dsql 查询 SQL</h4><p>Druid 包包含一个 SQL 命令行客户端，位于<code>bin/dsql</code>Druid 包根目录。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~/apache-druid-0.21.1]$ ./bin/dsql</span><br><span class="line">Welcome to dsql, the command-line client for Druid SQL.</span><br><span class="line">Connected to [http://localhost:8082/].</span><br><span class="line"></span><br><span class="line">Type "\h" for help.</span><br><span class="line">dsql&gt; <span class="keyword">SELECT</span> page, <span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> Edits <span class="keyword">FROM</span> wikipedia <span class="keyword">WHERE</span> <span class="string">"__time"</span> <span class="keyword">BETWEEN</span> <span class="built_in">TIMESTAMP</span> <span class="string">'2015-09-12 00:00:00'</span> <span class="keyword">AND</span> <span class="built_in">TIMESTAMP</span> <span class="string">'2015-09-13 00:00:00'</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> page <span class="keyword">ORDER</span> <span class="keyword">BY</span> Edits <span class="keyword">DESC</span> <span class="keyword">LIMIT</span> <span class="number">10</span>;</span><br><span class="line">┌──────────────────────────────────────────────────────────┬───────┐</span><br><span class="line">│ page                                                     │ Edits │</span><br><span class="line">├──────────────────────────────────────────────────────────┼───────┤</span><br><span class="line">│ Wikipedia:Vandalismusmeldung                             │    33 │</span><br><span class="line">│ User:Cyde/List of candidates for speedy deletion/Subpage │    28 │</span><br><span class="line">│ Jeremy Corbyn                                            │    27 │</span><br><span class="line">│ Wikipedia:Administrators' noticeboard/Incidents          │    21 │</span><br><span class="line">│ Flavia Pennetta                                          │    20 │</span><br><span class="line">│ Total Drama Presents: The Ridonculous Race               │    18 │</span><br><span class="line">│ User talk:Dudeperson176123                               │    18 │</span><br><span class="line">│ Wikipédia:Le Bistro/12 septembre 2015                    │    18 │</span><br><span class="line">│ Wikipedia:In the news/Candidates                         │    17 │</span><br><span class="line">│ Wikipedia:Requests for page protection                   │    17 │</span><br><span class="line">└──────────────────────────────────────────────────────────┴───────┘</span><br><span class="line">Retrieved 10 rows in 0.09s.</span><br><span class="line"></span><br><span class="line">dsql&gt;</span><br></pre></td></tr></table></figure><h4 id="4-4-3-HTTP-查询-SQL"><a href="#4-4-3-HTTP-查询-SQL" class="headerlink" title="4.4.3 HTTP 查询 SQL"></a>4.4.3 HTTP 查询 SQL</h4><p>可以通过 HTTP 直接向 Druid Broker 提交查询。</p><p>Druid 包包括一个示例文件，其中包含上面显示的 SQL 查询<code>quickstart/tutorial/wikipedia-top-pages-sql.json</code>。让我们将该查询提交给 Druid Broker：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -X <span class="string">'POST'</span> -H <span class="string">'Content-Type:application/json'</span> -d @quickstart/tutorial/wikipedia-top-pages-sql.json http://localhost:8888/druid/v2/sql | jq .</span></span><br></pre></td></tr></table></figure><p>应该返回以下结果</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"page"</span>: <span class="string">"Wikipedia:Vandalismusmeldung"</span>,</span><br><span class="line">    <span class="attr">"Edits"</span>: <span class="number">33</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"page"</span>: <span class="string">"User:Cyde/List of candidates for speedy deletion/Subpage"</span>,</span><br><span class="line">    <span class="attr">"Edits"</span>: <span class="number">28</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"page"</span>: <span class="string">"Jeremy Corbyn"</span>,</span><br><span class="line">    <span class="attr">"Edits"</span>: <span class="number">27</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"page"</span>: <span class="string">"Wikipedia:Administrators' noticeboard/Incidents"</span>,</span><br><span class="line">    <span class="attr">"Edits"</span>: <span class="number">21</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"page"</span>: <span class="string">"Flavia Pennetta"</span>,</span><br><span class="line">    <span class="attr">"Edits"</span>: <span class="number">20</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"page"</span>: <span class="string">"Total Drama Presents: The Ridonculous Race"</span>,</span><br><span class="line">    <span class="attr">"Edits"</span>: <span class="number">18</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"page"</span>: <span class="string">"User talk:Dudeperson176123"</span>,</span><br><span class="line">    <span class="attr">"Edits"</span>: <span class="number">18</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"page"</span>: <span class="string">"Wikipédia:Le Bistro/12 septembre 2015"</span>,</span><br><span class="line">    <span class="attr">"Edits"</span>: <span class="number">18</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"page"</span>: <span class="string">"Wikipedia:In the news/Candidates"</span>,</span><br><span class="line">    <span class="attr">"Edits"</span>: <span class="number">17</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"page"</span>: <span class="string">"Wikipedia:Requests for page protection"</span>,</span><br><span class="line">    <span class="attr">"Edits"</span>: <span class="number">17</span></span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;一、简介&quot;&gt;&lt;a href=&quot;#一、简介&quot; class=&quot;headerlink&quot; title=&quot;一、简介&quot;&gt;&lt;/a&gt;一、简介&lt;/h2&gt;
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
      <category term="OLAP" scheme="http://chenzhonzhou.github.io/tags/OLAP/"/>
    
      <category term="Druid" scheme="http://chenzhonzhou.github.io/tags/Druid/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse 联机分析处理(OLAP)</title>
    <link href="http://chenzhonzhou.github.io/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/"/>
    <id>http://chenzhonzhou.github.io/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/</id>
    <published>2021-11-19T02:55:01.000Z</published>
    <updated>2021-12-21T10:04:52.953Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><p><strong><a href="https://clickhouse.com/" target="_blank" rel="noopener">ClickHouse</a></strong> 的全称是<strong>Click Stream，Data WareHouse</strong>，简称ClickHouse，是俄罗斯 Yandex 公司于2016年开源的列式存储数据库（DBMS)，主要用于联机分析处理查询（OLAP），能够使用SQL 查询实时生成分析数据报告。</p><h2 id="一、OLAP和列式存储"><a href="#一、OLAP和列式存储" class="headerlink" title="一、OLAP和列式存储"></a>一、OLAP和列式存储</h2><h3 id="1-1-什么是OLAP"><a href="#1-1-什么是OLAP" class="headerlink" title="1.1 什么是OLAP"></a>1.1 什么是OLAP</h3><p><strong>OLAP（On-line Analytical Processing，联机分析处理）</strong>是在基于数据仓库多维模型的基础上实现的面向分析的各类操作的集合。可以比较下其与传统的<strong>OLTP（On-line Transaction Processing，联机事务处理）</strong>的区别来看一下它的特点：</p><table><thead><tr><th>对比项</th><th>OLAP</th><th>OLTP</th></tr></thead><tbody><tr><td>主要使用场景</td><td>数据分析，挖掘，机器学习</td><td>在线业务服务</td></tr><tr><td>涉及数据量</td><td>历史存档数据，可能时间跨度比较大</td><td>当前正在发生的业务数据</td></tr><tr><td>事务和数据完整性</td><td>对事务能力没有要求，数据不一致也可以重建数据</td><td>对事务和数据一致性要求很高</td></tr><tr><td>功能使用需求</td><td>复杂的聚积和多数据源关联；<br>查询执行时间可到分钟，小时，天级别</td><td>简单的增删改查，要求响应时间极端(ms)</td></tr><tr><td>并发要求</td><td>低并发</td><td>高并发</td></tr><tr><td>技术实现方案</td><td>大量SCAN；列式存储；存储计算可以分离</td><td>事务；索引；存储计算在一起</td></tr><tr><td>可用性要求</td><td>要求不高</td><td>非常高</td></tr><tr><td>数据模型/规约</td><td>纬度模型，关系模型，对范式要求很低</td><td>关系模型，3NF范式</td></tr><tr><td>技术典范</td><td>SQL-On-Hadoop</td><td>MySQL，Oracle</td></tr></tbody></table><p>OLAP的优势是基于数据仓库面向主题、集成的、保留历史及不可变更的数据存储，以及多维模型多视角多层次的数据组织形式，如果脱离的这两点，OLAP将不复存在，也就没有优势可言。</p><p><strong>OLAP场景的关键特征</strong></p><ul><li>绝大多数是读请求；</li><li>数据以相当大的批次(&gt; 1000行)更新，而不是单行更新;或者根本没有更新；</li><li>已添加到数据库的数据不能修改；</li><li>对于读取，从数据库中提取相当多的行，但只提取列的一小部分；</li><li>宽表，即每个表包含着大量的列；</li><li>查询相对较少(通常每台服务器每秒查询数百次或更少)；</li><li>对于简单查询，允许延迟大约50毫秒；</li><li>列中的数据相对较小：数字和短字符串(例如，每个URL 60个字节)；</li><li>处理单个查询时需要高吞吐量(每台服务器每秒可达数十亿行)；</li><li>事务不是必须的；</li><li>对数据一致性要求低；</li><li>每个查询有一个大表。除了他以外，其他的都很小；</li><li>查询结果明显小于源数据。换句话说，数据经过过滤或聚合，因此结果适合于单个服务器的RAM中。</li></ul><p>很容易可以看出，OLAP场景与其他通常业务场景(例如,OLTP或K/V)有很大的不同， 因此想要使用OLTP或Key-Value数据库去高效的处理分析查询场景，并不是非常完美的适用方案。例如，使用OLAP数据库去处理分析请求通常要优于使用MongoDB或Redis去处理分析请求。</p><h3 id="1-2-什么是列式存储"><a href="#1-2-什么是列式存储" class="headerlink" title="1.2 什么是列式存储"></a>1.2 什么是列式存储</h3><p>在传统的行式数据库系统中，数据按如下顺序存储：</p><table><thead><tr><th>Row</th><th>WatchID</th><th>JavaEnable</th><th>Title</th><th>GoodEvent</th><th>EventTime</th></tr></thead><tbody><tr><td>#0</td><td>89354350662</td><td>1</td><td>Investor Relations</td><td>1</td><td>2016-05-18 05:19:20</td></tr><tr><td>#1</td><td>90329509958</td><td>0</td><td>Contact us</td><td>1</td><td>2016-05-18 08:10:20</td></tr><tr><td>#2</td><td>89953706054</td><td>1</td><td>Mission</td><td>1</td><td>2016-05-18 07:38:00</td></tr><tr><td>#N</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table><p>处于同一行中的数据总是被物理的存储在一起。</p><p>常见的行式数据库系统有：<code>MySQL</code>、<code>Postgres</code>和<code>MS SQL Server</code>。</p><p>在列式数据库系统中，数据按如下的顺序存储：</p><table><thead><tr><th>Row:</th><th>#0</th><th>#1</th><th>#2</th><th>#N</th></tr></thead><tbody><tr><td>WatchID:</td><td>89354350662</td><td>90329509958</td><td>89953706054</td><td>…</td></tr><tr><td>JavaEnable:</td><td>1</td><td>0</td><td>1</td><td>…</td></tr><tr><td>Title:</td><td>Investor Relations</td><td>Contact us</td><td>Mission</td><td>…</td></tr><tr><td>GoodEvent:</td><td>1</td><td>1</td><td>1</td><td>…</td></tr><tr><td>EventTime:</td><td>2016-05-18 05:19:20</td><td>2016-05-18 08:10:20</td><td>2016-05-18 07:38:00</td><td>…</td></tr></tbody></table><p>这些示例只显示了数据的排列顺序。来自不同列的值被单独存储，来自同一列的数据被存储在一起。</p><p>常见的列式数据库有： Vertica、 Paraccel (Actian Matrix，Amazon Redshift)、 Sybase IQ、 Exasol、 Infobright、 InfiniDB、 MonetDB (VectorWise， Actian Vector)、 LucidDB、 SAP HANA、 Google Dremel、 Google PowerDrill、 Druid、 kdb+。</p><p>下图更好的展示了行式存储和列式存储的存储特点</p><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><p><strong>传统行式数据库的特性如下：</strong><br>1）数据是按行存储的；<br>2）没有索引的查询使用大量I/O。比如一般的数据库表都会建立索引，通过索引加快查询效率；<br>3）建立索引和物化视图需要花费大量的时间和资源；<br>4）面对查询需求，数据库必须被大量膨胀才能满足需求。</p><p><strong>列式数据库的特性如下：</strong><br>1）数据按列存储，即每一列单独存放；<br>2）数据即索引；<br>3）只访问查询涉及的列，可以大量降低系统I/O；<br>4）每一列由一个线程来处理，即查询的并发处理性能高；<br>5）数据类型一致，数据特征相似，可以高效压缩。比如有增量压缩、前缀压缩算法都是基于列存储的类型定制的，所以可以大幅度提高压缩比，有利于存储和网络输出数据带宽的消耗。</p><p><strong>相比于行式存储，列式存储在分析场景下有着许多优良的特性：</strong><br>1）分析场景中往往需要读大量行但是少数几个列。在行存模式下，数据按行连续存储，所有列的数据都存储在一个block中，不参与计算的列在IO时也要全部读出，读取操作被严重放大。而列存模式下，只需要读取参与计算的列即可，极大的减低了IO cost，加速了查询；<br>2）同一列中的数据属于同一类型，压缩效果显著。列存往往有着高达十倍甚至更高的压缩比，节省了大量的存储空间，降低了存储成本；<br>3）更高的压缩比意味着更小的data size，从磁盘中读取相应数据耗时更短；<br>4）自由的压缩算法选择。不同列的数据具有不同的数据类型，适用的压缩算法也就不尽相同。可以针对不同列类型，选择最合适的压缩算法；<br>5）高压缩比，意味着同等大小的内存能够存放更多数据，系统cache效果更好。</p><h2 id="二、ClickHouse-架构概述"><a href="#二、ClickHouse-架构概述" class="headerlink" title="二、ClickHouse 架构概述"></a>二、ClickHouse 架构概述</h2><h3 id="2-1-ClickHouse-的核心特性"><a href="#2-1-ClickHouse-的核心特性" class="headerlink" title="2.1 ClickHouse 的核心特性"></a>2.1 ClickHouse 的核心特性</h3><h4 id="2-1-1-完备的DBMS功能"><a href="#2-1-1-完备的DBMS功能" class="headerlink" title="2.1.1 完备的DBMS功能"></a>2.1.1 完备的DBMS功能</h4><p>ClickHouse拥有完备的数据库管理功能，所以它称得上是一个DBMS（Database Management System，数据库管理系统），而不仅是一个数据库。作为一个DBMS，它具备了一些基本功能，如下所示。<br>1）DDL（数据定义语言）：可以动态地创建、修改或删除数据库、表和视图，而无须重启服务。<br>2）DML（数据操作语言）：可以动态查询、插入、修改或删除数据。<br>3）权限控制：可以按照用户粒度设置数据库或者表的操作权限，保障数据的安全性。<br>4）数据备份与恢复：提供了数据备份导出与导入恢复机制，满足生产环境的要求。<br>5）分布式管理：提供集群模式，能够自动管理多个数据库节点。<br>这里只列举了一些最具代表性的功能，但已然足以表明为什么Click House称得上是DBMS了。</p><h4 id="2-1-2-列式存储与数据压缩"><a href="#2-1-2-列式存储与数据压缩" class="headerlink" title="2.1.2 列式存储与数据压缩"></a>2.1.2 列式存储与数据压缩</h4><p>列式存储和数据压缩，对于一款高性能数据库来说是必不可少的特性。一个非常流行的观点认为，如果你想让查询变得更快，最简单且有效的方法是减少数据扫描范围和数据传输时的大小，而列式存储和数据压缩就可以帮助我们实现上述两点。列式存储和数据压缩通常是伴生的，因为一般来说列式存储是数据压缩的前提。<br>按列存储与按行存储相比，前者可以有效减少查询时所需扫描的数据量，这一点可以用一个示例简单说明。假设一张数据表A拥有50个字段A1～A50，以及100行数据。现在需要查询前5个字段并进行数据分析，则可以用如下SQL实现：</p><blockquote><p>SELECT A1，A2，A3，A4，A5 FROM A</p></blockquote><p>如果数据按行存储，数据库首先会逐行扫描，并获取每行数据的所有50个字段，再从每一行数据中返回A1～A5这5个字段。不难发现，尽管只需要前面的5个字段，但由于数据是按行进行组织的，实际上还是扫描了所有的字段。如果数据按列存储，就不会发生这样的问题。由于数据按列组织，数据库可以直接获取A1～A5这5列的数据，从而避免了多余的数据扫描。<br>按列存储相比按行存储的另一个优势是对数据压缩的友好性。同样可以用一个示例简单说明压缩的本质是什么。假设有两个字符串abcdefghi和bcdefghi，现在对它们进行压缩，如下所示：</p><blockquote><p>压缩前：abcdefghi_bcdefghi<br>压缩后：abcdefghi_(9,8)</p></blockquote><p>可以看到，压缩的本质是按照一定步长对数据进行匹配扫描，当发现重复部分的时候就进行编码转换。例如上述示例中的(9,8)，表示如果从下划线开始向前移动9个字节，会匹配到8个字节长度的重复项，即这里的bcdefghi。<br>真实的压缩算法自然比这个示例更为复杂，但压缩的实质就是如此。数据中的重复项越多，则压缩率越高；压缩率越高，则数据体量越小；而数据体量越小，则数据在网络中的传输越快，对网络带宽和磁盘IO的压力也就越小。既然如此，那怎样的数据最可能具备重复的特性呢？答案是属于同一个列字段的数据，因为它们拥有相同的数据类型和现实语义，重复项的可能性自然就更高。<br>ClickHouse就是一款使用列式存储的数据库，数据按列进行组织，属于同一列的数据会被保存在一起，列与列之间也会由不同的文件分别保存（这里主要指MergeTree表引擎）。数据默认使用LZ4算法压缩，在Yandex.Metrica的生产环境中，数据总体的压缩比可以达到8:1（未压缩前17PB，压缩后2PB）。列式存储除了降低IO和存储的压力之外，还为向量化执行做好了铺垫。</p><h4 id="2-1-3-向量化执行引擎"><a href="#2-1-3-向量化执行引擎" class="headerlink" title="2.1.3 向量化执行引擎"></a>2.1.3 向量化执行引擎</h4><p>坊间有句玩笑，即<em>能用钱解决的问题，千万别花时间</em>。而业界也有种调侃如出一辙，即<em>能升级硬件解决的问题，千万别优化程序</em>。有时候，你千辛万苦优化程序逻辑带来的性能提升，还不如直接升级硬件来得简单直接。这虽然只是一句玩笑不能当真，但硬件层面的优化确实是最直接、最高效的提升途径之一。向量化执行就是这种方式的典型代表，这项寄存器硬件层面的特性，为上层应用程序的性能带来了指数级的提升。<br>向量化执行，可以简单地看作一项消除程序中循环的优化。这里用一个形象的例子比喻。小胡经营了一家果汁店，虽然店里的鲜榨苹果汁深受大家喜爱，但客户总是抱怨制作果汁的速度太慢。小胡的店里只有一台榨汁机，每次他都会从篮子里拿出一个苹果，放到榨汁机内等待出汁。如果有8个客户，每个客户都点了一杯苹果汁，那么小胡需要重复循环8次上述的榨汁流程，才能榨出8杯苹果汁。如果制作一杯果汁需要5分钟，那么全部制作完毕则需要40分钟。为了提升果汁的制作速度，小胡想出了一个办法。他将榨汁机的数量从1台增加到了8台，这么一来，他就可以从篮子里一次性拿出8个苹果，分别放入8台榨汁机同时榨汁。此时，小胡只需要5分钟就能够制作出8杯苹果汁。为了制作n杯果汁，非向量化执行的方式是用1台榨汁机重复循环制作n次，而向量化执行的方式是用n台榨汁机只执行1次。<br>为了实现向量化执行，需要利用CPU的SIMD指令。SIMD的全称是Single Instruction Multiple Data，即用单条指令操作多条数据。现代计算机系统概念中，它是通过数据并行以提高性能的一种实现方式（其他的还有指令级并行和线程级并行），它的原理是在CPU寄存器层面实现数据的并行操作。<br>在计算机系统的体系结构中，存储系统是一种层次结构。典型服务器计算机的存储层次结构如下图所示。一个实用的经验告诉我们，存储媒介距离CPU越近，则访问数据的速度越快。</p><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><p>从上图中可以看到，从左向右，距离CPU越远，则数据的访问速度越慢。从寄存器中访问数据的速度，是从内存访问数据速度的300倍，是从磁盘中访问数据速度的3000万倍。所以利用CPU向量化执行的特性，对于程序的性能提升意义非凡。<br>ClickHouse目前利用SSE4.2指令集实现向量化执行。</p><h4 id="2-1-4-关系模型与SQL查询"><a href="#2-1-4-关系模型与SQL查询" class="headerlink" title="2.1.4 关系模型与SQL查询"></a>2.1.4 关系模型与SQL查询</h4><p>相比HBase和Redis这类NoSQL数据库，ClickHouse使用关系模型描述数据并提供了传统数据库的概念（数据库、表、视图和函数等）。与此同时，ClickHouse完全使用SQL作为查询语言（支持GROUP BY、ORDER BY、JOIN、IN等大部分标准SQL），这使得它平易近人，容易理解和学习。因为关系型数据库和SQL语言，可以说是软件领域发展至今应用最为广泛的技术之一，拥有极高的<strong>群众基础</strong>。也正因为ClickHouse提供了标准协议的SQL查询接口，使得现有的第三方分析可视化系统可以轻松与它集成对接。在SQL解析方面，ClickHouse是大小写敏感的，这意味着<code>SELECT a</code>和<code>SELECT A</code>所代表的语义是不同的。<br>关系模型相比文档和键值对等其他模型，拥有更好的描述能力，也能够更加清晰地表述实体间的关系。更重要的是，在OLAP领域，已有的大量数据建模工作都是基于关系模型展开的（星型模型、雪花模型乃至宽表模型）。ClickHouse使用了关系模型，所以将构建在传统关系型数据库或数据仓库之上的系统迁移到ClickHouse的成本会变得更低，可以直接沿用之前的经验成果。</p><h4 id="2-1-5-多样化的表引擎"><a href="#2-1-5-多样化的表引擎" class="headerlink" title="2.1.5 多样化的表引擎"></a>2.1.5 多样化的表引擎</h4><p>也许因为Yandex.Metrica的最初架构是基于MySQL实现的，所以在ClickHouse的设计中，能够察觉到一些MySQL的影子，表引擎的设计就是其中之一。与MySQL类似，ClickHouse也将存储部分进行了抽象，把存储引擎作为一层独立的接口。ClickHouse共拥有合并树、内存、文件、接口和其他6大类20多种表引擎。其中每一种表引擎都有着各自的特点，用户可以根据实际业务场景的要求，选择合适的表引擎使用。</p><h4 id="2-1-6-多线程与分布式"><a href="#2-1-6-多线程与分布式" class="headerlink" title="2.1.6 多线程与分布式"></a>2.1.6 多线程与分布式</h4><p>ClickHouse几乎具备现代化高性能数据库的所有典型特征，对于可以提升性能的手段可谓是一一用尽，对于多线程和分布式这类被广泛使用的技术，自然更是不在话下。<br>如果说向量化执行是通过数据级并行的方式提升了性能，那么多线程处理就是通过线程级并行的方式实现了性能的提升。相比基于底层硬件实现的向量化执行SIMD，线程级并行通常由更高层次的软件层面控制。现代计算机系统早已普及了多处理器架构，所以现今市面上的服务器都具备良好的多核心多线程处理能力。由于SIMD不适合用于带有较多分支判断的场景，ClickHouse也大量使用了多线程技术以实现提速，以此和向量化执行形成互补。<br>如果一个篮子装不下所有的鸡蛋，那么就多用几个篮子来装，这就是分布式设计中分而治之的基本思想。同理，如果一台服务器性能吃紧，那么就利用多台服务的资源协同处理。为了实现这一目标，首先需要在数据层面实现数据的分布式。因为在分布式领域，存在一条金科玉律——<strong>计算移动比数据移动更加划算</strong>。在各服务器之间，通过网络传输数据的成本是高昂的，所以相比移动数据，更为聪明的做法是预先将数据分布到各台服务器，将数据的计算查询直接下推到数据所在的服务器。ClickHouse在数据存取方面，既支持分区（纵向扩展，利用多线程原理），也支持分片（横向扩展，利用分布式原理），可以说是将多线程和分布式的技术应用到了极致。</p><h4 id="2-1-7-多主架构"><a href="#2-1-7-多主架构" class="headerlink" title="2.1.7 多主架构"></a>2.1.7 多主架构</h4><p>HDFS、Spark、HBase和Elasticsearch这类分布式系统，都采用了Master-Slave主从架构，由一个管控节点作为Leader统筹全局。而ClickHouse则采用Multi-Master多主架构，集群中的每个节点角色对等，客户端访问任意一个节点都能得到相同的效果。这种多主的架构有许多优势，例如对等的角色使系统架构变得更加简单，不用再区分主控节点、数据节点和计算节点，集群中的所有节点功能相同。所以它天然规避了单点故障的问题，非常适合用于多数据中心、异地多活的场景。</p><h4 id="2-1-8-在线查询"><a href="#2-1-8-在线查询" class="headerlink" title="2.1.8 在线查询"></a>2.1.8 在线查询</h4><p>ClickHouse经常会被拿来与其他的分析型数据库作对比，比如Vertica、SparkSQL、Hive和Elasticsearch等，它与这些数据库确实存在许多相似之处。例如，它们都可以支撑海量数据的查询场景，都拥有分布式架构，都支持列存、数据分片、计算下推等特性。这其实也侧面说明了ClickHouse在设计上确实吸取了各路奇技淫巧。与其他数据库相比，ClickHouse也拥有明显的优势。例如，Vertica这类商用软件价格高昂；SparkSQL与Hive这类系统无法保障90%的查询在1秒内返回，在大数据量下的复杂查询可能会需要分钟级的响应时间；而Elasticsearch这类搜索引擎在处理亿级数据聚合查询时则显得捉襟见肘。<br>正如ClickHouse的广告词所言，其他的开源系统太慢，商用的系统太贵，只有Clickouse在成本与性能之间做到了良好平衡，即又快又开源。ClickHouse当之无愧地阐释了<strong>在线</strong>二字的含义，即便是在复杂查询的场景下，它也能够做到极快响应，且无须对数据进行任何预处理加工。</p><h4 id="2-1-9-数据分片与分布式查询"><a href="#2-1-9-数据分片与分布式查询" class="headerlink" title="2.1.9 数据分片与分布式查询"></a>2.1.9 数据分片与分布式查询</h4><p>数据分片是将数据进行横向切分，这是一种在面对海量数据的场景下，解决存储和查询瓶颈的有效手段，是一种分治思想的体现。ClickHouse支持分片，而分片则依赖集群。每个集群由1到多个分片组成，而每个分片则对应了ClickHouse的1个服务节点。分片的数量上限取决于节点数量（1个分片只能对应1个服务节点）。<br>ClickHouse并不像其他分布式系统那样，拥有高度自动化的分片功能。ClickHouse提供了本地表（Local Table）与分布式表（Distributed Table）的概念。一张本地表等同于一份数据的分片。而分布式表本身不存储任何数据，它是本地表的访问代理，其作用类似分库中间件。借助分布式表，能够代理访问多个数据分片，从而实现分布式查询。<br>这种设计类似数据库的分库和分表，十分灵活。例如在业务系统上线的初期，数据体量并不高，此时数据表并不需要多个分片。所以使用单个节点的本地表（单个数据分片）即可满足业务需求，待到业务增长、数据量增大的时候，再通过新增数据分片的方式分流数据，并通过分布式表实现分布式查询。这就好比一辆手动挡赛车，它将所有的选择权都交到了使用者的手中。</p><h3 id="2-2-ClickHouse-的架构设计"><a href="#2-2-ClickHouse-的架构设计" class="headerlink" title="2.2 ClickHouse 的架构设计"></a>2.2 ClickHouse 的架构设计</h3><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%873.png" alt="图片3"></p><h4 id="2-2-1-Column与Field"><a href="#2-2-1-Column与Field" class="headerlink" title="2.2.1 Column与Field"></a>2.2.1 Column与Field</h4><p>Column和Field是ClickHouse数据最基础的映射单元。作为一款百分之百的列式存储数据库，ClickHouse按列存储数据，内存中的一列数据由一个Column对象表示。Column对象分为接口和实现两个部分，在IColumn接口对象中，定义了对数据进行各种关系运算的方法，例如插入数据的insertRangeFrom和insertFrom方法、用于分页的cut，以及用于过滤的filter方法等。而这些方法的具体实现对象则根据数据类型的不同，由相应的对象实现，例如ColumnString、ColumnArray和ColumnTuple等。在大多数场合，ClickHouse都会以整列的方式操作数据，但凡事也有例外。如果需要操作单个具体的数值（也就是单列中的一行数据），则需要使用Field对象，Field对象代表一个单值。与Column对象的泛化设计思路不同，Field对象使用了聚合的设计模式。在Field对象内部聚合了Null、UInt64、String和Array等13种数据类型及相应的处理逻辑。</p><h4 id="2-2-2-DataType"><a href="#2-2-2-DataType" class="headerlink" title="2.2.2 DataType"></a>2.2.2 DataType</h4><p>数据的序列化和反序列化工作由DataType负责。IDataType接口定义了许多正反序列化的方法，它们成对出现，例如serializeBinary和deserializeBinary、serializeTextJSON和deserializeTextJSON等，涵盖了常用的二进制、文本、JSON、XML、CSV和Protobuf等多种格式类型。IDataType也使用了泛化的设计模式，具体方法的实现逻辑由对应数据类型的实例承载，例如DataTypeString、DataTypeArray及DataTypeTuple等。<br>DataType虽然负责序列化相关工作，但它并不直接负责数据的读取，而是转由从Column或Field对象获取。在DataType的实现类中，聚合了相应数据类型的Column对象和Field对象。例如，DataTypeString会引用字符串类型的ColumnString，而DataTypeArray则会引用数组类型的ColumnArray，以此类推。</p><h4 id="2-2-3-Block与Block流"><a href="#2-2-3-Block与Block流" class="headerlink" title="2.2.3 Block与Block流"></a>2.2.3 Block与Block流</h4><p>ClickHouse内部的数据操作是面向Block对象进行的，并且采用了流的形式。虽然Column和Field组成了数据的基本映射单元，但对应到实际操作，它们还缺少了一些必要的信息，比如数据的类型及列的名称。于是ClickHouse设计了Block对象，Block对象可以看作数据表的子集。Block对象的本质是由数据对象、数据类型和列名称组成的三元组，即Column、DataType及列名称字符串。Column提供了数据的读取能力，而DataType知道如何正反序列化，所以Block在这些对象的基础之上实现了进一步的抽象和封装，从而简化了整个使用的过程，仅通过Block对象就能完成一系列的数据操作。在具体的实现过程中，Block并没有直接聚合Column和DataType对象，而是通过ColumnWithTypeAndName对象进行间接引用。<br>有了Block对象这一层封装之后，对Block流的设计就是水到渠成的事情了。流操作有两组顶层接口：IBlockInputStream负责数据的读取和关系运算，IBlockOutputStream负责将数据输出到下一环节。Block流也使用了泛化的设计模式，对数据的各种操作最终都会转换成其中一种流的实现。IBlockInputStream接口定义了读取数据的若干个read虚方法，而具体的实现逻辑则交由它的实现类来填充。<br>IBlockInputStream接口总共有60多个实现类，它们涵盖了ClickHouse数据摄取的方方面面。这些实现类大致可以分为三类：第一类用于处理数据定义的DDL操作，例如DDLQueryStatusInputStream等；第二类用于处理关系运算的相关操作，例如LimitBlockInput-Stream、JoinBlockInputStream及AggregatingBlockInputStream等；第三类则是与表引擎呼应，每一种表引擎都拥有与之对应的BlockInputStream实现，例如MergeTreeBaseSelect-BlockInputStream（MergeTree表引擎）、TinyLogBlockInputStream（TinyLog表引擎）及KafkaBlockInputStream（Kafka表引擎）等。<br>IBlockOutputStream的设计与IBlockInputStream如出一辙。IBlockOutputStream接口同样也定义了若干写入数据的write虚方法。它的实现类比IBlockInputStream要少许多，一共只有20多种。这些实现类基本用于表引擎的相关处理，负责将数据写入下一环节或者最终目的地，例如MergeTreeBlockOutputStream、TinyLogBlockOutputStream及StorageFileBlock-OutputStream等。</p><h4 id="2-2-4-Table"><a href="#2-2-4-Table" class="headerlink" title="2.2.4 Table"></a>2.2.4 Table</h4><p>在数据表的底层设计中并没有所谓的Table对象，它直接使用IStorage接口指代数据表。表引擎是ClickHouse的一个显著特性，不同的表引擎由不同的子类实现，例如IStorageSystemOneBlock（系统表）、StorageMergeTree（合并树表引擎）和StorageTinyLog（日志表引擎）等。IStorage接口定义了DDL（如ALTER、RENAME、OPTIMIZE和DROP等）、read和write方法，它们分别负责数据的定义、查询与写入。在数据查询时，IStorage负责根据AST查询语句的指示要求，返回指定列的原始数据。后续对数据的进一步加工、计算和过滤，则会统一交由Interpreter解释器对象处理。对Table发起的一次操作通常都会经历这样的过程，接收AST查询语句，根据AST返回指定列的数据，之后再将数据交由Interpreter做进一步处理。</p><h4 id="2-2-5-Parser与Interpreter"><a href="#2-2-5-Parser与Interpreter" class="headerlink" title="2.2.5 Parser与Interpreter"></a>2.2.5 Parser与Interpreter</h4><p>Parser和Interpreter是非常重要的两组接口：Parser分析器负责创建AST对象；而Interpreter解释器则负责解释AST，并进一步创建查询的执行管道。它们与IStorage一起，串联起了整个数据查询的过程。Parser分析器可以将一条SQL语句以递归下降的方法解析成AST语法树的形式。不同的SQL语句，会经由不同的Parser实现类解析。例如，有负责解析DDL查询语句的ParserRenameQuery、ParserDropQuery和ParserAlterQuery解析器，也有负责解析INSERT语句的ParserInsertQuery解析器，还有负责SELECT语句的ParserSelectQuery等。<br>Interpreter解释器的作用就像Service服务层一样，起到串联整个查询过程的作用，它会根据解释器的类型，聚合它所需要的资源。首先它会解析AST对象；然后执行“业务逻辑”（例如分支判断、设置参数、调用接口等）；最终返回IBlock对象，以线程的形式建立起一个查询执行管道。</p><h4 id="2-2-6-Functions与Aggregate-Functions"><a href="#2-2-6-Functions与Aggregate-Functions" class="headerlink" title="2.2.6 Functions与Aggregate Functions"></a>2.2.6 Functions与Aggregate Functions</h4><p>ClickHouse主要提供两类函数——普通函数和聚合函数。普通函数由IFunction接口定义，拥有数十种函数实现，例如FunctionFormatDateTime、FunctionSubstring等。除了一些常见的函数（诸如四则运算、日期转换等）之外，也不乏一些非常实用的函数，例如网址提取函数、IP地址脱敏函数等。普通函数是没有状态的，函数效果作用于每行数据之上。当然，在函数具体执行的过程中，并不会一行一行地运算，而是采用向量化的方式直接作用于一整列数据。<br>聚合函数由IAggregateFunction接口定义，相比无状态的普通函数，聚合函数是有状态的。以COUNT聚合函数为例，其AggregateFunctionCount的状态使用整型UInt64记录。聚合函数的状态支持序列化与反序列化，所以能够在分布式节点之间进行传输，以实现增量计算。</p><h4 id="2-2-7-Cluster与Replication"><a href="#2-2-7-Cluster与Replication" class="headerlink" title="2.2.7 Cluster与Replication"></a>2.2.7 Cluster与Replication</h4><p>ClickHouse的集群由分片（Shard）组成，而每个分片又通过副本（Replica）组成。这种分层的概念，在一些流行的分布式系统中十分普遍。例如，在Elasticsearch的概念中，一个索引由分片和副本组成，副本可以看作一种特殊的分片。如果一个索引由5个分片组成，副本的基数是1，那么这个索引一共会拥有10个分片（每1个分片对应1个副本）。<br>如果你用同样的思路来理解ClickHouse的分片，那么很可能会在这里栽个跟头。ClickHouse的某些设计总是显得独树一帜，而集群与分片就是其中之一。这里有几个与众不同的特性。<br>1）ClickHouse的1个节点只能拥有1个分片，也就是说如果要实现1分片、1副本，则至少需要部署2个服务节点。<br>2）分片只是一个逻辑概念，其物理承载还是由副本承担的。</p><p>如下所示，是ClickHouse的一份集群配置示例，从字面含义理解这份配置的语义，可以理解为自定义集群ch_cluster拥有1个shard（分片）和1个replica（副本），且该副本由10.37.129.6服务节点承载。</p><p>自定义集群ch_cluster的配置示例</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">ch_cluster</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">host</span>&gt;</span>10.37.129.6<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ch_cluster</span>&gt;</span></span><br></pre></td></tr></table></figure><p>从本质上看，这组1分片、1副本的配置在ClickHouse中只有1个物理副本，所以它正确的语义应该是1分片、0副本。分片更像是逻辑层的分组，在物理存储层面则统一使用副本代表分片和副本。所以真正表示1分片、1副本语义的配置，应该改为1个分片和2个副本，如下所示：<br>1分片、1副本的集群配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">ch_cluster</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">host</span>&gt;</span>10.37.129.6<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">host</span>&gt;</span>10.37.129.7<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ch_cluster</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-3-常见-ClickHouse-集群部署架构"><a href="#2-3-常见-ClickHouse-集群部署架构" class="headerlink" title="2.3 常见 ClickHouse 集群部署架构"></a>2.3 常见 ClickHouse 集群部署架构</h3><p>不同于Elasticsearch、HDFS这类主从架构的分布式系统，ClickHouse采用多主（无中心）架构，集群中的每个节点角色对等，客户端访问任意一个节点都能得到相同的效果。</p><p>ClickHouse借助分片将数据进行横向切分，而分片依赖集群，每个集群由1到多个分片组成，每个分片对应了ClickHouse的1个服务节点；分片数量的上限取决与节点数量（1个分片只能对应1个服务节点）。</p><p>但是ClickHouse并不像其他分布式系统那样，拥有高度自动化的分片功能；ClickHouse提供了本地表与分布式表的概念；一张本地表等同于一个数据分片。而分布式表是张逻辑表，本身不存储任何数据，它是本地表的访问代理，其作用类似分库中间件。借助分布式表，能够代理访问多个数据分片，从而实现分布式查询。当然，也可以在应用层实现数据分发。</p><p>ClickHouse同时支持数据副本，其副本概念与Elasticsearch类似，但在ClickHouse中分片其实是一种逻辑概念，其物理承载是由副本承担的。</p><p>ClickHouse的数据副本一般通过ReplicatedMergeTree复制表系列引擎实现，副本之间借助ZooKeeper实现数据的一致性。此外也可通过分布式表负责同时进行分片和副本的数据写入工作。</p><p>以四节点实现多分片和双副本为例：</p><h4 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h4><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%875.png" alt="图片5"></p><p>（上图中shard作为主副本）<br>在每个节点创建一个数据表，作为一个数据分片，使用ReplicatedMergeTree表引擎实现数据副本，而分布式表作为数据写入和查询的入口。<br>这是最常见的集群实现方式。</p><h4 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h4><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%876.png" alt="图片6"></p><p>在每个节点创建一个数据表，作为一个数据分片，分布式表同时负责分片和副本的数据写入工作。</p><p>这种实现方案下，不需要使用复制表，但分布式表节点需要同时负责分片和副本的数据写入工作，它很有可能成为写入的单点瓶颈。</p><h4 id="方案三"><a href="#方案三" class="headerlink" title="方案三"></a>方案三</h4><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%877.png" alt="图片7"></p><p>在每个节点创建一个数据表，作为一个数据分片，同时创建两个分布式表，每个分布表只纳管一半的数据。</p><p>副本的实现仍需要借助ReplicatedMergeTree表引擎。</p><h4 id="方案四"><a href="#方案四" class="headerlink" title="方案四"></a>方案四</h4><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%878.png" alt="图片8"></p><p>在每个节点创建两个数据表，同一数据分片的两个副本位于不同节点上，每个分布式表纳管一半的数据。<br>这种方案可以在更少的节点上实现数据分布与冗余，但是部署上略显繁琐。</p><p><strong>ClickHouse 部署方案总结</strong></p><ul><li>ClickHouse的分片与副本功能完全靠配置文件实现，无法自动管理，所以当集群规模较大时，集群运维成本较高；</li><li>数据副本依赖ZooKeeper实现同步，当数据量较大时，ZooKeeper可能会称为瓶颈；</li><li>如果资源充足，建议使用方案一，主副本和副副本位于不同节点，以更好地实现读写分离与负载均衡；</li><li>如果资源不够充足，可以使用方案四，每个节点承载两个副本，但部署方式上略复杂。</li></ul><h2 id="三、ClickHouse-性能"><a href="#三、ClickHouse-性能" class="headerlink" title="三、ClickHouse 性能"></a>三、ClickHouse 性能</h2><p>因为ClickHouse在诞生之初是为了服务Yandex自家的Web流量分析产品Yandex.Metrica，所以在存储数据超过20万亿行的情况下，ClickHouse做到了90%的查询都能够在1秒内返回的惊人之举。</p><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%874.png" alt="图片4"></p><p>更多对比数据前往<a href="https://clickhouse.com/benchmark/dbms/" target="_blank" rel="noopener">官网</a>查看：<a href="https://clickhouse.com/benchmark/dbms/" target="_blank" rel="noopener">https://clickhouse.com/benchmark/dbms/</a></p><h3 id="3-1-单个大查询的吞吐量"><a href="#3-1-单个大查询的吞吐量" class="headerlink" title="3.1 单个大查询的吞吐量"></a>3.1 单个大查询的吞吐量</h3><p>吞吐量可以使用每秒处理的行数或每秒处理的字节数来衡量。如果数据被放置在page cache中，则一个不太复杂的查询在单个服务器上大约能够以 <code>2-10GB/s</code>（未压缩）的速度进行处理（对于简单的查询，速度可以达到 <code>30GB/s</code>）。如果数据没有在page cache中的话，那么速度将取决于你的磁盘系统和数据的压缩率。例如，如果一个磁盘允许以 <code>400MB/s</code> 的速度读取数据，并且数据压缩率是3，则数据的处理速度为 <code>1.2GB/s</code>。这意味着，如果你是在提取一个10字节的列，那么它的处理速度大约是 <code>1-2</code>亿行每秒。</p><p>对于分布式处理，处理速度几乎是线性扩展的，但这受限于聚合或排序的结果不是那么大的情况下。</p><h3 id="3-2-处理短查询的延迟时间"><a href="#3-2-处理短查询的延迟时间" class="headerlink" title="3.2 处理短查询的延迟时间"></a>3.2 处理短查询的延迟时间</h3><p>如果一个查询使用主键并且没有太多行(几十万)进行处理，并且没有查询太多的列，那么在数据被page cache缓存的情况下，它的延迟应该小于<code>50毫秒</code>(在最佳的情况下应该小于<code>10毫秒</code>)。 否则，延迟取决于数据的查找次数。如果你当前使用的是HDD，在数据没有加载的情况下，查询所需要的延迟可以通过以下公式计算得知：<code>查找时间（10 ms） * 查询的列的数量 * 查询的数据块的数量</code>。</p><h3 id="3-3-处理大量短查询的吞吐量"><a href="#3-3-处理大量短查询的吞吐量" class="headerlink" title="3.3 处理大量短查询的吞吐量"></a>3.3 处理大量短查询的吞吐量</h3><p>在相同的情况下，ClickHouse可以在单个服务器上每秒处理数百个查询（在最佳的情况下最多可以处理数千个）。但是由于这不适用于分析型场景。因此我们建议每秒最多查询100次。</p><h3 id="3-4-数据的写入性能"><a href="#3-4-数据的写入性能" class="headerlink" title="3.4 数据的写入性能"></a>3.4 数据的写入性能</h3><p>我们建议每次写入不少于<code>1000</code>行的批量写入，或每秒不超过一个写入请求。当使用tab-separated格式将一份数据写入到MergeTree表中时，写入速度大约为<code>50</code>到<code>200MB/s</code>。如果您写入的数据每行为<code>1Kb</code>，那么写入的速度为<code>5万</code>到<code>20万</code>行每秒。如果您的行更小，那么写入速度将更高。为了提高写入性能，您可以使用多个INSERT进行并行写入，这将带来线性的性能提升。</p><h2 id="四、ClickHouse-的优缺点"><a href="#四、ClickHouse-的优缺点" class="headerlink" title="四、ClickHouse 的优缺点"></a>四、ClickHouse 的优缺点</h2><h3 id="4-1-ClickHouse-优点"><a href="#4-1-ClickHouse-优点" class="headerlink" title="4.1 ClickHouse 优点"></a>4.1 ClickHouse 优点</h3><p>1）为了高效的使用CPU，数据不仅仅按列存储，同时还按向量进行处理；<br>2）数据压缩空间大，减少IO；处理单查询高吞吐量每台服务器每秒最多数十亿行；<br>3）索引非B树结构，不需要满足最左原则；只要过滤条件在索引列中包含即可；即使在使用的数据不在索引中，由于各种并行处理机制ClickHouse全表扫描的速度也很快；<br>4）写入速度非常快，50-200M/s，对于大量的数据更新非常适用。</p><h3 id="4-2-ClickHouse-缺点"><a href="#4-2-ClickHouse-缺点" class="headerlink" title="4.2 ClickHouse 缺点"></a>4.2 ClickHouse 缺点</h3><p>1）不支持事务，不支持真正的删除/更新；<br>2）不支持高并发，官方建议qps为100，可以通过修改配置文件增加连接数，但是在服务器足够好的情况下；<br>3）SQL满足日常使用80%以上的语法，join写法比较特殊；最新版已支持类似SQL的join，但性能不好；<br>4）尽量做1000条以上批量的写入，避免逐行insert或小批量的insert，update，delete操作，因为ClickHouse底层会不断的做异步的数据合并，会影响查询性能，这个在做实时数据写入的时候要尽量避开；<br>5）Clickhouse快是因为采用了并行处理机制，即使一个查询，也会用服务器一半的CPU去执行，所以ClickHouse不能支持高并发的使用场景，默认单查询使用CPU核数为服务器核数的一半，安装时会自动识别服务器核数，可以通过配置文件修改该参数。</p><h3 id="4-3-注意事项"><a href="#4-3-注意事项" class="headerlink" title="4.3 注意事项"></a>4.3 注意事项</h3><p>1）关闭虚拟内存，物理内存和虚拟内存的数据交换，会导致查询变慢；<br>2）为每一个账户添加join_use_nulls配置，左表中的一条记录在右表中不存在，右表的相应字段会返回该字段相应数据类型的默认值，而不是标准SQL中的Null值；<br>3）JOIN操作时一定要把数据量小的表放在右边，ClickHouse中无论是Left Join 、Right Join还是Inner Join永远都是拿着右表中的每一条记录到左表中查找该记录是否存在，所以右表必须是小表；<br>4）批量写入数据时，必须控制每个批次的数据中涉及到的分区的数量，在写入之前最好对需要导入的数据进行排序。无序的数据或者涉及的分区太多，会导致ClickHouse无法及时对新导入的数据进行合并，从而影响查询性能；<br>5）尽量减少JOIN时的左右表的数据量，必要时可以提前对某张表进行聚合操作，减少数据条数。有些时候，先GROUP BY再JOIN比先JOIN再GROUP BY查询时间更短；<br>6）ClickHouse的分布式表性能性价比不如物理表高，建表分区字段值不宜过多，防止数据导入过程磁盘可能会被打满；<br>7）CPU一般在50%左右会出现查询波动，达到70%会出现大范围的查询超时，CPU是最关键的指标，要非常关注。</p><h2 id="五、ClickHouse-部署"><a href="#五、ClickHouse-部署" class="headerlink" title="五、ClickHouse 部署"></a>五、ClickHouse 部署</h2><blockquote><p><strong>系统要求</strong></p></blockquote><p>ClickHouse可以在任何具有x86_64，AArch64或PowerPC64LE CPU架构的Linux，FreeBSD或Mac OS X上运行。<br>官方预构建的二进制文件通常针对x86_64进行编译，并利用<code>SSE 4.2</code>指令集，因此，除非另有说明，支持它的CPU使用将成为额外的系统需求。下面是检查当前CPU是否支持SSE 4.2的命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ grep -q sse4_2 /proc/cpuinfo &amp;&amp; echo "SSE 4.2 supported" || echo "SSE 4.2 not supported"</span><br><span class="line">SSE 4.2 supported</span><br></pre></td></tr></table></figure><p>要在不支持<code>SSE 4.2</code>或<code>AArch64</code>，<code>PowerPC64LE</code>架构的处理器上运行ClickHouse，您应该通过适当的配置调整从<a href="https://clickhouse.com/docs/zh/getting-started/install/#from-sources" target="_blank" rel="noopener">源代码构建ClickHouse</a>。</p><h3 id="5-1-单机模式部署"><a href="#5-1-单机模式部署" class="headerlink" title="5.1 单机模式部署"></a>5.1 单机模式部署</h3><h4 id="5-1-1-下载tgz包"><a href="#5-1-1-下载tgz包" class="headerlink" title="5.1.1 下载tgz包"></a>5.1.1 下载tgz包</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ export LATEST_VERSION=21.11.4.14</span><br><span class="line">[hadoop@hadoop1 ~]$ cd downloads/</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://repo.clickhouse.tech/tgz/clickhouse-common-static-$LATEST_VERSION.tgz</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://repo.clickhouse.tech/tgz/clickhouse-common-static-dbg-$LATEST_VERSION.tgz</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://repo.clickhouse.tech/tgz/stable/clickhouse-server-$LATEST_VERSION.tgz</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://repo.clickhouse.tech/tgz/clickhouse-client-$LATEST_VERSION.tgz</span><br></pre></td></tr></table></figure><p>对于生产环境，建议使用最新的<code>stable</code>版本。可以在<a href="https://github.com/ClickHouse/ClickHouse/tags" target="_blank" rel="noopener">GitHub</a>页面找到它，它以后缀<code>-stable</code>标志。</p><h4 id="5-1-2-解压压缩包"><a href="#5-1-2-解压压缩包" class="headerlink" title="5.1.2 解压压缩包"></a>5.1.2 解压压缩包</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/downloads]$ tar zxf clickhouse-client-$LATEST_VERSION.tgz</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ tar zxf clickhouse-common-static-$LATEST_VERSION.tgz</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ tar zxf clickhouse-common-static-dbg-$LATEST_VERSION.tgz</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ tar zxf clickhouse-server-$LATEST_VERSION.tgz</span><br></pre></td></tr></table></figure><p>安装 clickhouse-server 时需要为clickhouse输入default用户默认密码</p><h4 id="5-1-3-安装-clickhouse"><a href="#5-1-3-安装-clickhouse" class="headerlink" title="5.1.3 安装 clickhouse"></a>5.1.3 安装 clickhouse</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/downloads]$ sudo ./clickhouse-client-$LATEST_VERSION/install/doinst.sh</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ sudo ./clickhouse-common-static-$LATEST_VERSION/install/doinst.sh</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ sudo ./clickhouse-common-static-dbg-$LATEST_VERSION/install/doinst.sh</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ sudo ./clickhouse-server-$LATEST_VERSION/install/doinst.sh</span><br></pre></td></tr></table></figure><p>删除创建的default密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ sudo rm -f /etc/clickhouse-server/users.d/default-password.xml</span><br></pre></td></tr></table></figure><p>修改 clickhouse 目录属主，不想更改clickhouse 启动用户，则跳过这一步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ sudo chown -R hadoop.hadoop /etc/clickhouse-*</span><br><span class="line">[hadoop@hadoop1 ~]$ sudo chown -R hadoop.hadoop /var/lib/clickhouse</span><br><span class="line">[hadoop@hadoop1 ~]$ sudo chown -R hadoop.hadoop /var/log/clickhouse-server</span><br><span class="line">[hadoop@hadoop1 ~]$ sudo chown -R hadoop.hadoop /var/run/clickhouse-server</span><br></pre></td></tr></table></figure><h4 id="5-1-4-启动-clickhouse"><a href="#5-1-4-启动-clickhouse" class="headerlink" title="5.1.4 启动 clickhouse"></a>5.1.4 启动 clickhouse</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ nohup clickhouse-server --config-file=/etc/clickhouse-server/config.xml &gt;/dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>使用clickhouse 用户启动服务</p><blockquote><p>sudo clickhouse start</p></blockquote><h4 id="5-1-5-client-连接-clickhouse"><a href="#5-1-5-client-连接-clickhouse" class="headerlink" title="5.1.5 client 连接 clickhouse"></a>5.1.5 client 连接 clickhouse</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/downloads]$ clickhouse-client -u default --password</span><br><span class="line">ClickHouse client version 21.11.4.14 (official build).</span><br><span class="line">Password <span class="keyword">for</span> user (default):</span><br><span class="line">Connecting to localhost:9000 as user default.</span><br><span class="line">Connected to ClickHouse server version 21.11.4 revision 54450.</span><br><span class="line"></span><br><span class="line">hadoop1 :)</span><br></pre></td></tr></table></figure><h3 id="5-2-分布式集群安装"><a href="#5-2-分布式集群安装" class="headerlink" title="5.2 分布式集群安装"></a>5.2 分布式集群安装</h3><p><strong>服务规划</strong></p><table><thead><tr><th>主机名</th><th>角色分配</th><th>分片与副本</th><th>依赖服务</th></tr></thead><tbody><tr><td>hadoop1</td><td>clickhouse-server、clickhouse-client</td><td>shard01、replica01</td><td>zookeeper</td></tr><tr><td>hadoop2</td><td>clickhouse-server、clickhouse-client</td><td>shard02、replica01</td><td>zookeeper</td></tr><tr><td>hadoop3</td><td>clickhouse-server、clickhouse-client</td><td>shard03、replica01</td><td>zookeeper</td></tr></tbody></table><p>前置条件</p><blockquote><p>zookeeper集群是可用的</p></blockquote><h4 id="5-2-1-安装-clickhouse"><a href="#5-2-1-安装-clickhouse" class="headerlink" title="5.2.1 安装 clickhouse"></a>5.2.1 安装 clickhouse</h4><p>安装 clickhouse 参考单机模式在集群的每台服务器上安装好clickhouse</p><h4 id="5-2-2-添加集群配置"><a href="#5-2-2-添加集群配置" class="headerlink" title="5.2.2 添加集群配置"></a>5.2.2 添加集群配置</h4><p>添加集群相关配置，在集群中所有clickhouse 服务器上操作</p><p><strong>修改 config.xml 配置</strong>，新增外部配置文件metrika.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vim /etc/clickhouse-server/config.xml</span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">clickhouse</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 日志 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">level</span>&gt;</span>trace<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">log</span>&gt;</span>/var/log/clickhouse-server/clickhouse-server.log<span class="tag">&lt;/<span class="name">log</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">errorlog</span>&gt;</span>/var/log/clickhouse-server/clickhouse-server.err.log<span class="tag">&lt;/<span class="name">errorlog</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">size</span>&gt;</span>1000M<span class="tag">&lt;/<span class="name">size</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">count</span>&gt;</span>10<span class="tag">&lt;/<span class="name">count</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 端口 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">http_port</span>&gt;</span>8123<span class="tag">&lt;/<span class="name">http_port</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tcp_port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">tcp_port</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mysql_port</span>&gt;</span>9004<span class="tag">&lt;/<span class="name">mysql_port</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">postgresql_port</span>&gt;</span>9005<span class="tag">&lt;/<span class="name">postgresql_port</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">interserver_http_port</span>&gt;</span>9009<span class="tag">&lt;/<span class="name">interserver_http_port</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 本机域名 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- &lt;interserver_http_host&gt;这里需要用域名，如果后续用到复制的话&lt;/interserver_http_host&gt; --&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 监听IP --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">listen_host</span>&gt;</span>::<span class="tag">&lt;/<span class="name">listen_host</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">listen_host</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">listen_host</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 最大连接数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">max_connections</span>&gt;</span>64<span class="tag">&lt;/<span class="name">max_connections</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ClickHouse在关闭连接之前等待传入请求时间。 默认为3秒 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">keep_alive_timeout</span>&gt;</span>3<span class="tag">&lt;/<span class="name">keep_alive_timeout</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 最大并发查询数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">max_concurrent_queries</span>&gt;</span>100<span class="tag">&lt;/<span class="name">max_concurrent_queries</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 单位是B --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">uncompressed_cache_size</span>&gt;</span>8589934592<span class="tag">&lt;/<span class="name">uncompressed_cache_size</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mark_cache_size</span>&gt;</span>10737418240<span class="tag">&lt;/<span class="name">mark_cache_size</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 存储路径 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">path</span>&gt;</span>/data1/clickhouse/<span class="tag">&lt;/<span class="name">path</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tmp_path</span>&gt;</span>/data1/clickhouse/tmp/<span class="tag">&lt;/<span class="name">tmp_path</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- user配置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">users_config</span>&gt;</span>users.xml<span class="tag">&lt;/<span class="name">users_config</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">default_profile</span>&gt;</span>default<span class="tag">&lt;/<span class="name">default_profile</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">log_queries</span>&gt;</span>1<span class="tag">&lt;/<span class="name">log_queries</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">default_database</span>&gt;</span>default<span class="tag">&lt;/<span class="name">default_database</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 新增外部配置文件metrika.xml  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">include_from</span>&gt;</span>/data1/clickhouse/metrika.xml<span class="tag">&lt;/<span class="name">include_from</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">remote_servers</span> <span class="attr">incl</span>=<span class="string">"clickhouse_remote_servers"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">zookeeper</span> <span class="attr">incl</span>=<span class="string">"zookeeper-servers"</span> <span class="attr">optional</span>=<span class="string">"true"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">macros</span> <span class="attr">incl</span>=<span class="string">"macros"</span> <span class="attr">optional</span>=<span class="string">"true"</span> /&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 重新加载内置词典的时间间隔（以秒为单位），默认3600。可以在不重新启动服务器的情况下“即时”修改词典 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">builtin_dictionaries_reload_interval</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">builtin_dictionaries_reload_interval</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 控制大表的删除 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">max_table_size_to_drop</span>&gt;</span>0<span class="tag">&lt;/<span class="name">max_table_size_to_drop</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">clickhouse</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>metrika.xml 配置</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vim /etc/clickhouse-server/metrika.xml</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">yandex</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 集群配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">clickhouse_remote_servers</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">perftest_3shards_1replicas</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 3分片1备份 --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 数据分片1  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop1<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 数据分片2  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop2<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span> 9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 数据分片3  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop3<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">perftest_3shards_1replicas</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">clickhouse_remote_servers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ZK配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">"1"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop1<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">"2"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop2<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">"3"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop3<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">networks</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ip</span>&gt;</span>::/0<span class="tag">&lt;/<span class="name">ip</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">networks</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">macros</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">shard</span>&gt;</span>01<span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">replica</span>&gt;</span>hadoop1<span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">macros</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 压缩相关配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">clickhouse_compression</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">case</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">min_part_size</span>&gt;</span>10000000000<span class="tag">&lt;/<span class="name">min_part_size</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">min_part_size_ratio</span>&gt;</span>0.01<span class="tag">&lt;/<span class="name">min_part_size_ratio</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">method</span>&gt;</span>lz4<span class="tag">&lt;/<span class="name">method</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">case</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">clickhouse_compression</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">yandex</span>&gt;</span></span><br></pre></td></tr></table></figure><p>部分配置说明</p><ul><li><p><code>cluster_3shards_1replicas</code> 集群名称,可随意定义</p></li><li><p>共设置3个分片，每个分片只有1个副本；</p></li><li><p><code>internal_replication</code> 此参数设置为«true»时，写操作只选一个正常的副本写入数据。如果分布式表的子表是复制表(*ReplicaMergeTree)，请使用此方案。换句话说，这其实是把数据的复制工作交给实际需要写入数据的表本身而不是分布式表。若此参数设置为«false»（默认值），写操作会将数据写入所有副本。实质上，这意味着要分布式表本身来复制数据。这种方式不如使用复制表的好，因为不会检查副本的一致性，并且随着时间的推移，副本数据可能会有些不一样。</p></li><li><p><code>macros</code> 是复制标识的配置，也称为宏配置，这里唯一标识一个副本名称，使用了cluster{layer}-{shard}-{replica}的表示方式（其中layer是双级分片设置，在Yandex公司的集群中用到），每个实例都要配置并且都是唯一的，这里3个节点分别配置如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hadoop1配置 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">macros</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">shard</span>&gt;</span>01<span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">replica</span>&gt;</span>hadoop1<span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">macros</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- hadoop2配置 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">macros</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">shard</span>&gt;</span>02<span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">replica</span>&gt;</span>hadoop2<span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">macros</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- hadoop3配置 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">macros</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">shard</span>&gt;</span>03<span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">replica</span>&gt;</span>hadoop3<span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">macros</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="5-2-3-启动-clickhouse-集群"><a href="#5-2-3-启动-clickhouse-集群" class="headerlink" title="5.2.3 启动 clickhouse 集群"></a>5.2.3 启动 clickhouse 集群</h4><p>启动所有节点的clickhouse服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ nohup clickhouse-server --config-file=/etc/clickhouse-server/config.xml &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line">[hadoop@hadoop2 ~]$ nohup clickhouse-server --config-file=/etc/clickhouse-server/config.xml &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line">[hadoop@hadoop3 ~]$ nohup clickhouse-server --config-file=/etc/clickhouse-server/config.xml &gt;/dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h4 id="5-2-4-client-连接-clickhouse"><a href="#5-2-4-client-连接-clickhouse" class="headerlink" title="5.2.4 client 连接 clickhouse"></a>5.2.4 client 连接 clickhouse</h4><p>在任意一台安装clickhouse client的服务器上连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ clickhouse-client -h hadoop1 --port 9000 -m -u default --password</span><br><span class="line">ClickHouse client version 21.11.4.14 (official build).</span><br><span class="line">Connecting to hadoop1:9000 as user default.</span><br><span class="line">Connected to ClickHouse server version 21.11.4 revision 54450.</span><br><span class="line"></span><br><span class="line">hadoop1 :) show databases;</span><br><span class="line"></span><br><span class="line">SHOW DATABASES</span><br><span class="line"></span><br><span class="line">Query id: 6c844db3-c555-437a-847b-5e91e67635b1</span><br><span class="line"></span><br><span class="line">┌─name───────────────┐</span><br><span class="line">│ INFORMATION_SCHEMA │</span><br><span class="line">│ default            │</span><br><span class="line">│ information_schema │</span><br><span class="line">│ system             │</span><br><span class="line">└────────────────────┘</span><br></pre></td></tr></table></figure><p><strong>查看集群信息</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> system.clusters;</span><br></pre></td></tr></table></figure><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%879.png" alt="图片9"></p><h4 id="5-2-5-创建本地表及分布式表"><a href="#5-2-5-创建本地表及分布式表" class="headerlink" title="5.2.5 创建本地表及分布式表"></a>5.2.5 创建本地表及分布式表</h4><p>在各个节点分表创建数据库test(在一个节点执行即可)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">test</span> <span class="keyword">ON</span> CLUSTER cluster_3shards_1replicas;</span><br></pre></td></tr></table></figure><p>下面给出ReplicatedMergeTree引擎的完整建表DDL语句。</p><p>创建本地表及表引擎，<strong>Replicated Table &amp; ReplicatedMergeTree Engines</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> test.events_local <span class="keyword">ON</span> CLUSTER cluster_3shards_1replicas (  ts_date <span class="built_in">Date</span>,  ts_date_time DateTime, user_id Int64,  event_type <span class="keyword">String</span>,  site_id Int64,  groupon_id Int64,  category_id Int64,  merchandise_id Int64,  search_text <span class="keyword">String</span>) <span class="keyword">ENGINE</span> = ReplicatedMergeTree(<span class="string">'/clickhouse/tables/&#123;shard&#125;/test/events_local'</span>,<span class="string">'&#123;replica&#125;'</span>) <span class="keyword">PARTITION</span> <span class="keyword">BY</span> ts_date <span class="keyword">ORDER</span> <span class="keyword">BY</span> (ts_date,toStartOfHour(ts_date_time),site_id,event_type) <span class="keyword">SETTINGS</span> index_granularity = <span class="number">8192</span>;</span><br></pre></td></tr></table></figure><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%8710.png" alt="图片10"></p><p>其中，ON CLUSTER语法表示分布式DDL，即执行一次就可在集群所有实例上创建同样的本地表。集群标识符{cluster}、分片标识符{shard}和副本标识符{replica}来自之前提到过的复制表宏配置，即<code>metrika.xml</code>中<code>&lt;macros&gt;</code>中的内容，配合ON CLUSTER语法一同使用，可以避免建表时在每个实例上反复修改这些值。</p><p>创建分布式表及分布式表引擎，<strong>Distributed Table &amp; Distributed Engine</strong></p><p>ClickHouse分布式表的本质并不是一张表，而是一些本地物理表（分片）的分布式视图，本身并不存储数据。<br>支持分布式表的引擎是Distributed，建表DDL语句示例如下，_all只是分布式表名比较通用的后缀而已。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS test.events_all ON CLUSTER cluster_3shards_1replicas AS test.events_local ENGINE = Distributed(cluster_3shards_1replicas,test,events_local,rand());</span><br></pre></td></tr></table></figure><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%8711.png" alt="图片11"></p><p><strong>在任意节点插入数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test.events_all <span class="keyword">values</span>(<span class="string">'2021-11-23'</span>,<span class="string">'2021-11-19 16:10:00'</span>,<span class="number">1</span>,<span class="string">'ceshi1'</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="string">'test1'</span>),(<span class="string">'2021-11-19'</span>,<span class="string">'2021-11-19 16:20:01'</span>,<span class="number">2</span>,<span class="string">'ceshi2'</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="string">'test2'</span>),(<span class="string">'2021-11-19'</span>,<span class="string">'2021-11-19 16:30:02'</span>,<span class="number">3</span>,<span class="string">'ceshi2'</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="string">'test3'</span>),(<span class="string">'2021-11-19'</span>,<span class="string">'2021-11-19 16:40:03'</span>,<span class="number">4</span>,<span class="string">'ceshi4'</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="string">'test4'</span>),(<span class="string">'2021-11-19'</span>,<span class="string">'2021-11-19 16:50:04'</span>,<span class="number">5</span>,<span class="string">'ceshi5'</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="string">'test5'</span>),(<span class="string">'2021-11-19'</span>,<span class="string">'2021-11-19 17:00:05'</span>,<span class="number">6</span>,<span class="string">'ceshi6'</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="string">'test6'</span>);</span><br></pre></td></tr></table></figure><p><strong>查询各分片数据</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from test.events_all;</span><br></pre></td></tr></table></figure><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%8712.png" alt="图片12"></p><p>前面说过，分布式表只是逻辑上的表，数据真正都存储在各节点的本地表中，下面通过查看各节点的本地表，看看数据的分布情况</p><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%8713.png" alt="图片13"></p><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%8714.png" alt="图片14"></p><p><img src="/2021/11/19/clickhouse-lian-ji-fen-xi-chu-li-olap/%E5%9B%BE%E7%89%8715.png" alt="图片15"></p><h3 id="5-3-ClickHouse-常用操作"><a href="#5-3-ClickHouse-常用操作" class="headerlink" title="5.3 ClickHouse 常用操作"></a>5.3 ClickHouse 常用操作</h3><p>查询clickhouse集群信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> system.clusters;</span><br></pre></td></tr></table></figure><p>查看表引擎</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">table</span> test.events_local;</span><br></pre></td></tr></table></figure><p>创建数据库命令（一个节点上执行，多个节点同时创建）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">test</span> <span class="keyword">ON</span> CLUSTER cluster_3shards_1replicas;</span><br></pre></td></tr></table></figure><p>删除数据库命令（一个节点上执行，多个节点同时删除）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">test</span> <span class="keyword">ON</span> CLUSTER cluster_3shards_1replicas;</span><br></pre></td></tr></table></figure><p>删除本地表数据（分布式表无法删除表数据）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test.events_local <span class="keyword">ON</span> CLUSTER cluster_3shards_1replicas <span class="keyword">delete</span> <span class="keyword">where</span> <span class="number">1</span>=<span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>1=1表示删除所有数据，可以接字段名删除满足某个条件的数据</p><p>查看zookeeper下目录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> system.zookeeper <span class="keyword">WHERE</span> <span class="keyword">path</span>=<span class="string">'/'</span>;</span><br></pre></td></tr></table></figure><p>clickhouse导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方式一：交互式</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tableName (c1, c2, ...) <span class="keyword">values</span> (v1, v2, ...)</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tableName (c1, c2, ...) <span class="keyword">select</span> ... </span><br><span class="line"></span><br><span class="line"><span class="comment">#方式二：批量</span></span><br><span class="line">clickhouse-<span class="keyword">client</span> <span class="comment">--query="insert into test.events_all FORMAT CSV" &lt; /tmp/events-all.csv</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#方式三：http客户端</span></span><br><span class="line">echo -ne <span class="string">'10\n11\n12\n'</span> | POST <span class="string">'http://localhost:8123/?query=insert into tableName format TabSeparated'</span></span><br></pre></td></tr></table></figure><p>clickhouse导出数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方式一：交互式</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> test.events_all <span class="keyword">into</span> <span class="keyword">outfile</span> <span class="string">'/tmp/events-all.csv'</span> <span class="keyword">FORMAT</span> CSV;</span><br><span class="line"></span><br><span class="line"><span class="comment">#方式二：非交互式</span></span><br><span class="line">clickhouse-client <span class="comment">--database bdName -u default --password password --query 'select * from tableName FORMAT CSV' &gt; abc</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#方式二：http客户端</span></span><br><span class="line">echo '<span class="keyword">select</span> <span class="number">1</span> <span class="keyword">FORMAT</span> TabSeparated<span class="string">' | curl "http://user:password@localhost:8123/" -d @- &gt; file</span></span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://clickhouse.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
      <category term="OLAP" scheme="http://chenzhonzhou.github.io/tags/OLAP/"/>
    
      <category term="ClickHouse" scheme="http://chenzhonzhou.github.io/tags/ClickHouse/"/>
    
  </entry>
  
  <entry>
    <title>Presto 分布式SQL查询引擎</title>
    <link href="http://chenzhonzhou.github.io/2021/11/16/presto-fen-bu-shi-sql-cha-xun-yin-qing/"/>
    <id>http://chenzhonzhou.github.io/2021/11/16/presto-fen-bu-shi-sql-cha-xun-yin-qing/</id>
    <published>2021-11-16T09:18:17.000Z</published>
    <updated>2021-12-24T02:30:04.340Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --><p><a href="https://prestodb.io/" target="_blank" rel="noopener">Presto</a> 是由 Facebook 推出的一个基于Java开发的开源分布式SQL查询引擎，数据量支持GB到TB字节，presto本身不存数据，但是可以接入很多数据源，它使得用SQL访问任何数据源成为可能，而且支持跨数据源的级联查询。你可以使用Presto通过水平扩展查询处理的方式来查询大型数据集。</p><p>发展历史如下：</p><ul><li>2012年秋季，Facebook启动Presto项目；</li><li>2013年冬季，Presto开源；</li><li>2017年11月，11888 commits，203 releases，198 contributors；</li><li>2019年1月，Presto分家，目前有 PrestoDB 和 PrestoSQL（Trino） 两个社区。</li></ul><h2 id="一、Presto-架构组成"><a href="#一、Presto-架构组成" class="headerlink" title="一、Presto 架构组成"></a>一、Presto 架构组成</h2><p><img src="/2021/11/16/presto-fen-bu-shi-sql-cha-xun-yin-qing/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><h3 id="1-1-Presto-服务器类型"><a href="#1-1-Presto-服务器类型" class="headerlink" title="1.1 Presto 服务器类型"></a>1.1 Presto 服务器类型</h3><h4 id="1-1-1-Coordinator"><a href="#1-1-1-Coordinator" class="headerlink" title="1.1.1 Coordinator"></a>1.1.1 Coordinator</h4><p>集群的管理节点<br>1）对外负责管理集群与客户端的连接，并接收客户端查询请求；<br>2）进行SQL的语法解析、查询计划生成和优化，并进行查询任务的调度；<br>3）Discovery Server节点，跟踪Worker节点的状态，通常内嵌于Coordinator节点中；<br>4）部署情况：一般作为单独节点部署在集群中；<br>5）通信方式：使用RESTful接口与客户端、Workers进行交互。</p><h4 id="1-1-2-Worker"><a href="#1-1-2-Worker" class="headerlink" title="1.1.2 Worker"></a>1.1.2 Worker</h4><p>集群的工作节点<br>1）用于执行被分解后的查询任务(task)及与数据的读写交互；<br>2）部署情况：一般集群中部署多个worker节点；<br>3）通信方式：使用RESTful接口与Coordinator、其他Workers进行交互。</p><h3 id="1-2-Presto-数据源"><a href="#1-2-Presto-数据源" class="headerlink" title="1.2 Presto 数据源"></a>1.2 Presto 数据源</h3><h4 id="1-2-1-Connector"><a href="#1-2-1-Connector" class="headerlink" title="1.2.1 Connector"></a>1.2.1 Connector</h4><p>Presto通过connector可以访问多种不同的数据源，connector相当于数据库访问的驱动。每种connector通过实现Presto的SPI接口实现数据源的标准接入。</p><h4 id="1-2-2-Catalog"><a href="#1-2-2-Catalog" class="headerlink" title="1.2.2 Catalog"></a>1.2.2 Catalog</h4><p>Presto Catalog目录包含架构并通过连接器引用数据源。例如，您可以配置 JMX 目录以通过 JMX 连接器提供对 JMX 信息的访问。当您在 Presto 中运行 SQL 语句时，您是在针对一个或多个目录运行它。目录的其他示例包括用于连接到 Hive 数据源的 Hive 目录。</p><p>在 Presto 中寻址表时，完全限定的表名始终以目录为根。例如，一个完全合格的表名称<code>hive.test_data.test</code>将引用<code>test</code>表中 <code>test_data</code>的架构<code>hive</code>目录。</p><p>目录在存储在 Presto 配置目录中的属性文件中定义。</p><h4 id="1-2-3-Schema"><a href="#1-2-3-Schema" class="headerlink" title="1.2.3 Schema"></a>1.2.3 Schema</h4><p>Schema是一种组织表格的方式。Catalog和Schema一起定义了一组可以查询的表。使用 Presto 访问 Hive 或关系数据库（例如 MySQL）时，Schema会转换为目标数据库中的database。其他类型的连接器可能会选择以对底层数据源有意义的方式将表组织成模式。</p><h4 id="1-2-4-Table"><a href="#1-2-4-Table" class="headerlink" title="1.2.4 Table"></a>1.2.4 Table</h4><p>数据表，与任何关系数据库中的概念类似。从源数据到表的映射由connector定义。</p><h3 id="1-3-presto-查询模型"><a href="#1-3-presto-查询模型" class="headerlink" title="1.3 presto 查询模型"></a>1.3 presto 查询模型</h3><h4 id="1-3-1-Statement"><a href="#1-3-1-Statement" class="headerlink" title="1.3.1 Statement"></a>1.3.1 Statement</h4><p><strong>Statement：</strong>语句，其实就是输入的SQL；</p><h4 id="1-3-2-Query"><a href="#1-3-2-Query" class="headerlink" title="1.3.2 Query"></a>1.3.2 Query</h4><p><strong>Query：</strong>根据SQL语句生成查询执行计划，进而生成可以执行的查询（Query），一个Query包含stages、tasks、splits、connectors等组件和相应的数据源这些概念；</p><h4 id="1-3-3-Stage"><a href="#1-3-3-Stage" class="headerlink" title="1.3.3 Stage"></a>1.3.3 Stage</h4><p><strong>Stage：</strong>当Presto执行Query时，会将query拆分成具有层次关系的多个stages。一个Query的stages之间是树形的层次结构，每一个Query都有一个Root stage，用于聚合所有其他Stages的输出数据。Stage只是coordinator用于分布式查询计划(query plan)建模的逻辑概念，本身并不会执行在Presto Workers上；</p><h4 id="1-3-4-Exchange"><a href="#1-3-4-Exchange" class="headerlink" title="1.3.4 Exchange"></a>1.3.4 Exchange</h4><p><strong>Exchange：</strong>Exchange用于不同Presto节点间查询的不同stages数据交换。Task生产数据放入输出Buffer中，也可以通过exchange客户端从其他task消费数据</p><h4 id="1-3-5-Task"><a href="#1-3-5-Task" class="headerlink" title="1.3.5 Task"></a>1.3.5 Task</h4><p><strong>Task：</strong>Presto是通过Task来运行的，一个分布式查询计划（query plan）被拆解成一些列的stage，一个stage分解成一系列并行执行的task，每个task被分解成一个或多个并行的driver，每个driver作用于一系列splist上，每个task都有对应的输入输出；</p><h4 id="1-3-6-Driver"><a href="#1-3-6-Driver" class="headerlink" title="1.3.6 Driver"></a>1.3.6 Driver</h4><p><strong>Driver：</strong>Drivers处理数据，并由task聚合后传给下游stage的一个task。一个Driver就是作用于一个split上的一系列operator的集合。Driver是Presto架构最底层的并行处理单元。每个Driver都有一个输入和一个输出。</p><h4 id="1-3-7-Operator"><a href="#1-3-7-Operator" class="headerlink" title="1.3.7 Operator"></a>1.3.7 Operator</h4><p><strong>Operator：</strong>一个operator代表对一个split的一种操作，一个Operator依次读取一个Split中的数据，将Operator所代表的计算和操作应用于该split上，并产生输出。</p><h4 id="1-3-8-Split"><a href="#1-3-8-Split" class="headerlink" title="1.3.8 Split"></a>1.3.8 Split</h4><p><strong>Split：</strong>分片，一个分片就是一个大的数据集中的一个小的子集，位于分布式query plan中较低层次的stages从数据源获取splits，位于较高层次的中间stages则从其他stages获取数据。</p><h2 id="二、Presto-其它特性"><a href="#二、Presto-其它特性" class="headerlink" title="二、Presto 其它特性"></a>二、Presto 其它特性</h2><h3 id="2-1-presto-SQL执行步骤"><a href="#2-1-presto-SQL执行步骤" class="headerlink" title="2.1 presto SQL执行步骤"></a>2.1 presto SQL执行步骤</h3><p>1）客户端通过http发送一个查询语句给presto集群的coordinator；<br>2）coordinator接收到客户端的查询语句，对语句进行解析，生成查询执行计划，并根据生成的执行计划生成stage和task，并将task分发到需要处理数据的worker上进行分析；<br>3）worker执行task，task通过connector从数据源中读取需要的数据；<br>4）上游stage输出的结果给到下游stage作为输入，每个Stage的每个task在worker内存中进行计算和处理；<br>5）client从提交查询后，就一直监听coordinator中的查询结果，一有结果就立即输出，直到轮询所有的结果都返回，则本次查询结束。</p><h3 id="2-2-Presto-低延时查询原理"><a href="#2-2-Presto-低延时查询原理" class="headerlink" title="2.2 Presto 低延时查询原理"></a>2.2 Presto 低延时查询原理</h3><ol><li>得益于 YARN 调度的慢。YARN 的定位是一个通用的资源管理系统。但是无论是 Hive 采用 MR、TEZ 何种引擎，执行 SQL时，每个执行算子都在 Yarn Container 中运行，而 <strong>Yarn 拉起 Container 性能特别低（秒级）</strong>。这犹如应用程序在拉起进程和开启多线程一样。线程更轻量级，简单的运算开启线程的速度更快，加速更明显；而启用进程则要笨重的多，还容易受到操作系统限制。而 <strong>Presto 调度的确就是用了线程</strong>，而不是进程。</li><li>Presto 的Coordinator/Worker 架构更像 Spark Standalone 模式，只在两个进程和服务中完成。但是Spark更多侧重于 SparkRDD之间依赖关系，Stage失败线性恢复等功能导致有较大开销。Spark Input也直接依赖Hadoop InputFormat API，导致SparkSQL在运行时，并不能把 SQL 优化细节传导到 InputFormat。Presto 弃用 Hadoop InputFormat，但采用类似的数据分区技术，并且可以把 SQL 经过解析后，把Where 条件生成 TupleDomain 传递给 Connector。Connector 能根据字段元数据采用一定程度的索引下推，利用底层系统的索引能力，大大<strong>减少数据扫描区间和参与计算的数据量</strong>。</li><li>Presto 是完全基于内存的并行计算，他不像 <strong>Hive MR/TEZ 需要把中间数据写盘、Spark 需要把溢出的数据写盘</strong>，Presto 是完全假设数据能有效的放入内存。再者，得益于<strong>Presto流水线式的作业计算能力</strong>，在很多 SQL 执行时通过分析SQL的执行计划，能把立即展现的数据立即返回。这也是给用户一种很快的“假象”。但这种“假象”也是无可厚非的，我们即便是从一个结果中提取大量数据，也是遍历游标，等到我们遍历到那个位置，后续的结果数据已经源源不断的计算完成，并不影响我们获得结果</li></ol><h3 id="2-3-Presto-的优点与不足"><a href="#2-3-Presto-的优点与不足" class="headerlink" title="2.3 Presto 的优点与不足"></a>2.3 Presto 的优点与不足</h3><h4 id="2-3-1-presto-优点"><a href="#2-3-1-presto-优点" class="headerlink" title="2.3.1 presto 优点"></a>2.3.1 presto 优点</h4><p>1）<strong>多数据源：</strong>Presto支持MySQL、PostgreSQL、Cassandra、Hive、Kafka、JMX等，多数据源及多数据源之间混合计算；<br>2）<strong>支持 ANSI SQL：</strong>Presto支持<strong>ANSI SQL</strong>，并提供了一个SQL Shell给用户，用户可以直接使用ANSI SQL进行数据查询和计算；<br>3）<strong>扩展性：</strong>Presto有很好的扩展性，开发人员可以很容易地开发出适用于自己特定数据源的Connector，并且可以使用SQL语句查询和分析自定义Connector中的数据；<br>4）<strong>混合计算：</strong>在数据库中每种类型的数据源都对应于一种特定类型的Connector，用户可以根据业务需要在Presto中针对于一种类型的Connector配置一个或者多个Catalog并查询其中的数据，用户可以混合多个Catalog进行join查询和计算；<br>5）<strong>高性能：</strong>低延迟高并发的内存计算引擎，相对hive，无论是mr还是tez还是spark执行引擎，至少提升10倍以上 ；<br>6）<strong>流水线：</strong>presto是基于pipeline进行设计，在进行少量数据处理的过程中，用户无需等到所有数据计算完成才能看到结果，<strong>一旦开始计算就可产生一部分结果返回，后续的计算结果以多个page返回给终端用户（Driver）</strong>。</p><h4 id="2-3-2-Presto-不足"><a href="#2-3-2-Presto-不足" class="headerlink" title="2.3.2 Presto 不足"></a>2.3.2 Presto 不足</h4><p>通过 Presto 执行流程的架构，可以看出 Presto 在查询上也存在一些不足：</p><p>1）<strong>没有容错能力：</strong>当一个 query 分发到多个 Worker 去执行时，当有一个 Worker 因为各种原因查询失败，Master 感知到之后，整个 query 也会失败。<br>2）<strong>内存限制：</strong>由于 Presto 是纯内存计算，所以当内存不够时，Presto 并不会将结果 dump 到磁盘上，所以查询也就失败了。<br>3）<strong>并行查询：</strong>因为所有的 task 都是并行执行，如果其中一台 Worker 因为各种原因查询很慢，那么整个 query 就会变得很慢。<br>4）<strong>并发限制：</strong>因为全内存操作+内存限制，能同时处理的数据量有限，因而导致并发能力不足。</p><h3 id="2-4-Presto-应用场景"><a href="#2-4-Presto-应用场景" class="headerlink" title="2.4 Presto 应用场景"></a>2.4 Presto 应用场景</h3><p>1）<strong>实时计算：</strong>Presto 性能优越，实时查询工具上的重要选择；</p><p>2）<strong>Ad-Hoc查询：</strong>数据分析应用、Presto 根据特定条件的查询返回结果和生成报表；</p><p>3）<strong>ETL：</strong>因支持的数据源广泛、可用于不同数据库之间迁移，转换 和 完成 ETL 清洗的能力；</p><p>4）<strong>实时数据流分析：</strong>Presto-Kafka Connector 使用 SQL对Kafka的数据流进行清洗、分析；</p><p>5）<strong>作为MPP：</strong>Presto Connector 有非常好的扩展性，可进行扩展开发，可支持其他<strong>异构非SQL查询引擎转为SQL</strong>，支持索引下推。</p><blockquote><p>Ad-Hoc：即席查询（Ad Hoc）是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表。即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，而即席查询是由用户自定义查询条件的。<br>ETL：（Extract-Transform-Load ）用来描述将数据从来源端经过萃取（Extract）、转置（Transform）、加载（Load）至目的端的过程。<br>MPP：Massively Parallel Processor，翻译过来就是大规模并行处理。</p></blockquote><h3 id="2-5-presto-和hive-的对比"><a href="#2-5-presto-和hive-的对比" class="headerlink" title="2.5 presto 和hive 的对比"></a>2.5 presto 和hive 的对比</h3><p>hive和presto是针对不同使用场景的。presto虽然查询很快，但是也不是适用于所有的查询场景。</p><p>比如做多张大表的关联查询，由于presto是基于内存查询的。做大表关联查询时，数据要加载到内存中，假如使用presto查询超过了几分钟才会有返回。且严重影响集群的性能。这就违背了presto交互式查询的初衷，交互式就是要做到近实时查询与返回。所以，presto不适合做多张大表的join操作或者ETL操作。这种情况就该使用hive了。</p><p>虽然能够处理 PB 级别的海量数据分析，但不是代表 Presto 把 PB 级别都放在内存中计算的。而是根据场景，如 count，avg 等聚合运算，是边读数据边计算，再清内存，再读数据再计算，这种耗的内存并不高。但是连表查，就可能产生大量的临时数据，因此速度会变慢，反而 Hive此时会更擅长。</p><p>另外，hive只能做hdfs查询（es等需要插件支持），而presto支持了mysql，pg，kafka，redis等。<strong>presto是支持多数据源的查询利器</strong>。</p><h2 id="三、Presto的优化"><a href="#三、Presto的优化" class="headerlink" title="三、Presto的优化"></a>三、Presto的优化</h2><p>Presto 的优化是一个非常有水平的问题，大致总结下，分如下几个类别</p><h3 id="3-1数据存储"><a href="#3-1数据存储" class="headerlink" title="3.1数据存储"></a>3.1数据存储</h3><p>想要使用 Presto 更高效地查询数据，需要在数据存储方面利用一些优化手段。</p><h4 id="3-1-1-合理设置分区"><a href="#3-1-1-合理设置分区" class="headerlink" title="3.1.1 合理设置分区"></a>3.1.1 合理设置分区</h4><p>与 Hive 类似，Presto 会根据元数据信息读取分区数据，合理地设置分区能减少 Presto 数据读取量，提升查询性能。</p><h4 id="3-1-2-使用-ORC-格式存储"><a href="#3-1-2-使用-ORC-格式存储" class="headerlink" title="3.1.2 使用 ORC 格式存储"></a>3.1.2 使用 ORC 格式存储</h4><p>Presto 对 ORC文件 读取进行了特定优化，因此，在 Hive 中创建 Presto 使用的表时，建议采用 ORC 格式存储。相对于 Parquet 格式，Presto 对 ORC 格式支持得更好。</p><h4 id="3-1-3-使用压缩"><a href="#3-1-3-使用压缩" class="headerlink" title="3.1.3 使用压缩"></a>3.1.3 使用压缩</h4><p>数据压缩可以减少节点间数据传输对 IO 带宽的压力，对于即席查询需要快速解压，建议采用 Snappy压缩。</p><h4 id="3-1-4-预先排序"><a href="#3-1-4-预先排序" class="headerlink" title="3.1.4 预先排序"></a>3.1.4 预先排序</h4><p>对于已经排序的数据，在查询的数据过滤阶段，ORC格式支持跳过读取不必要的数据。比如对于经常需要过滤的字段可以预先排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">table</span> nation_orc <span class="keyword">partition</span>(p) <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> nation <span class="keyword">SORT</span> <span class="keyword">BY</span> n_name;</span><br></pre></td></tr></table></figure><p>如果需要过滤 n_name 字段，则性能将提升。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">count</span>(*) <span class="keyword">FROM</span> nation_orc <span class="keyword">WHERE</span> n_name=’AUSTRALIA’;</span><br></pre></td></tr></table></figure><h3 id="3-2-SQL查询"><a href="#3-2-SQL查询" class="headerlink" title="3.2 SQL查询"></a>3.2 SQL查询</h3><p>想要使用 Presto更高效地查询数据，需要在编写查询SQL语句方面利用一些优化手段。</p><h4 id="3-2-1-只选择需要的字段"><a href="#3-2-1-只选择需要的字段" class="headerlink" title="3.2.1 只选择需要的字段"></a>3.2.1 只选择需要的字段</h4><p>由于采用列式存储，所以只选择需要的字段可加快字段的读取速度，减少数据量。避免采用 * 读取所有字段。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[GOOD]: <span class="keyword">SELECT</span> <span class="built_in">time</span>,<span class="keyword">user</span>,host <span class="keyword">FROM</span> tbl[BAD]:  <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> tbl</span><br></pre></td></tr></table></figure><h4 id="3-2-2-过滤条件必须加上分区字段"><a href="#3-2-2-过滤条件必须加上分区字段" class="headerlink" title="3.2.2 过滤条件必须加上分区字段"></a>3.2.2 过滤条件必须加上分区字段</h4><p>对于有分区的表，where语句中优先使用分区字段进行过滤。acct_day 是分区字段，visit_time 是具体访问时间。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[GOOD]: <span class="keyword">SELECT</span> <span class="built_in">time</span>,<span class="keyword">user</span>,host <span class="keyword">FROM</span> tbl <span class="keyword">where</span> acct_day=<span class="number">20171101</span>[BAD]:  <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> tbl <span class="keyword">where</span> visit_time=<span class="number">20171101</span></span><br></pre></td></tr></table></figure><h4 id="3-2-3-Group-By语句优化"><a href="#3-2-3-Group-By语句优化" class="headerlink" title="3.2.3 Group By语句优化"></a>3.2.3 Group By语句优化</h4><p>合理安排 Group by语句中字段顺序对性能有一定提升。将 Group By 语句中字段按照每个字段 distinct 数据多少进行降序排列。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[GOOD]: <span class="keyword">SELECT</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> uid, gender</span><br><span class="line">[BAD]: <span class="keyword">SELECT</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> gender, uid</span><br></pre></td></tr></table></figure><h4 id="3-2-4-Order-by时使用Limit"><a href="#3-2-4-Order-by时使用Limit" class="headerlink" title="3.2.4 Order by时使用Limit"></a>3.2.4 Order by时使用Limit</h4><p>Order by 需要扫描数据到单个 worker 节点进行排序，导致单个worker需要大量内存。如果是查询 Top N 或者 Bottom N，使用 limit 可减少排序计算和内存压力。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[GOOD]: <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> tbl <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="built_in">time</span> <span class="keyword">LIMIT</span> <span class="number">100</span>[BAD]:  <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> tbl <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="built_in">time</span></span><br></pre></td></tr></table></figure><h4 id="3-2-5-使用近似聚合函数"><a href="#3-2-5-使用近似聚合函数" class="headerlink" title="3.2.5 使用近似聚合函数"></a>3.2.5 使用近似聚合函数</h4><p>Presto有一些近似聚合函数，对于允许有少量误差的查询场景，使用这些函数对查询性能有大幅提升。比如使用approx_distinct()函数比Count(distinct x)有大概2.3%的误差。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> approx_distinct(user_id) <span class="keyword">FROM</span> <span class="keyword">access</span></span><br></pre></td></tr></table></figure><h4 id="3-2-6-用regexp-like代替多个like语句"><a href="#3-2-6-用regexp-like代替多个like语句" class="headerlink" title="3.2.6 用regexp_like代替多个like语句"></a>3.2.6 用regexp_like代替多个like语句</h4><p>Presto查询优化器没有对多个 like 语句进行优化，使用regexp_like对性能有较大提升。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[GOOD]: <span class="keyword">SELECT</span> ...FROM accessWHERE <span class="keyword">regexp_like</span>(method, <span class="string">'GET|POST|PUT|DELETE'</span>)</span><br><span class="line">[BAD]: <span class="keyword">SELECT</span> ...FROM accessWHERE method <span class="keyword">LIKE</span> <span class="string">'%GET%'</span> <span class="keyword">OR</span> method <span class="keyword">LIKE</span> <span class="string">'%POST%'</span> <span class="keyword">OR</span> method <span class="keyword">LIKE</span> <span class="string">'%PUT%'</span> <span class="keyword">OR</span> method <span class="keyword">LIKE</span> <span class="string">'%DELETE%'</span></span><br></pre></td></tr></table></figure><h4 id="3-2-7-使用Join语句时将大表放在左边"><a href="#3-2-7-使用Join语句时将大表放在左边" class="headerlink" title="3.2.7 使用Join语句时将大表放在左边"></a>3.2.7 使用Join语句时将大表放在左边</h4><p>Presto中 join 的默认算法是broadcast join，即将 join 左边的表分割到多个 worker ，然后将join 右边的表数据整个复制一份发送到每个worker进行计算。如果右边的表数据量太大，则可能会报内存溢出错误。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[GOOD]: <span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> large_table l <span class="keyword">join</span> small_table s <span class="keyword">on</span> l.id = s.id</span><br><span class="line">[BAD]: <span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> small_table s <span class="keyword">join</span> large_table l <span class="keyword">on</span> l.id = s.id</span><br></pre></td></tr></table></figure><h4 id="3-2-8-使用Rank函数代替row-number函数来获取Top-N"><a href="#3-2-8-使用Rank函数代替row-number函数来获取Top-N" class="headerlink" title="3.2.8 使用Rank函数代替row_number函数来获取Top N"></a>3.2.8 使用Rank函数代替row_number函数来获取Top N</h4><p>在进行一些分组排序场景时，使用rank函数性能更好</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[GOOD]: <span class="keyword">SELECT</span> <span class="keyword">checksum</span>(rnk)<span class="keyword">FROM</span> (  <span class="keyword">SELECT</span> <span class="keyword">rank</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> l_orderkey, l_partkey <span class="keyword">ORDER</span> <span class="keyword">BY</span> l_shipdate <span class="keyword">DESC</span>) <span class="keyword">AS</span> rnk  <span class="keyword">FROM</span> lineitem) tWHERE rnk = <span class="number">1</span></span><br><span class="line">[BAD]: <span class="keyword">SELECT</span> <span class="keyword">checksum</span>(rnk)<span class="keyword">FROM</span> (  <span class="keyword">SELECT</span> row_number() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> l_orderkey, l_partkey <span class="keyword">ORDER</span> <span class="keyword">BY</span> l_shipdate <span class="keyword">DESC</span>) <span class="keyword">AS</span> rnk  <span class="keyword">FROM</span> lineitem) tWHERE rnk = <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="3-3-注意事项"><a href="#3-3-注意事项" class="headerlink" title="3.3 注意事项"></a>3.3 注意事项</h3><p>ORC和Parquet 都支持列式存储，但是ORC对Presto支持更好（Parquet对Impala支持更好）</p><p>对于列式存储而言，存储文件为二进制的，对于经常增删字段的表，建议不要使用列式存储（修改文件元数据代价大）。对比数据仓库，DWD层建议不要使用ORC，而DM层则建议使用。</p><h4 id="3-3-1-字段名引用"><a href="#3-3-1-字段名引用" class="headerlink" title="3.3.1 字段名引用"></a>3.3.1 字段名引用</h4><p>避免和关键字冲突：MySQL对字段加反引号`、Presto对字段加双引号分割；当然，如果字段名称不是关键字，可以不加这个双引号。</p><h4 id="3-3-2-时间函数"><a href="#3-3-2-时间函数" class="headerlink" title="3.3.2 时间函数"></a>3.3.2 时间函数</h4><p>对于Timestamp，需要进行比较的时候，需要添加Timestamp关键字，而MySQL中对Timestamp可以直接进行比较。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*MySQL的写法*/</span></span><br><span class="line"><span class="keyword">SELECT</span> t <span class="keyword">FROM</span> a <span class="keyword">WHERE</span> t &gt; <span class="string">'2017-01-01 00:00:00'</span>; </span><br><span class="line"></span><br><span class="line"><span class="comment">/*Presto中的写法*/</span></span><br><span class="line"><span class="keyword">SELECT</span> t <span class="keyword">FROM</span> a <span class="keyword">WHERE</span> t &gt; <span class="built_in">timestamp</span> <span class="string">'2017-01-01 00:00:00'</span>;</span><br></pre></td></tr></table></figure><h4 id="3-3-3-不支持INSERT-OVERWRITE语法"><a href="#3-3-3-不支持INSERT-OVERWRITE语法" class="headerlink" title="3.3.3 不支持INSERT OVERWRITE语法"></a>3.3.3 不支持INSERT OVERWRITE语法</h4><p>Presto中不支持 <code>insert overwrite</code> 语法，只能先 <code>delete</code>，然后 <code>insert into</code>。</p><h4 id="3-3-4-PARQUET格式"><a href="#3-3-4-PARQUET格式" class="headerlink" title="3.3.4 PARQUET格式"></a>3.3.4 PARQUET格式</h4><p>Presto目前支持Parquet格式，支持查询，但不支持insert。</p><h2 id="四、Presto-部署"><a href="#四、Presto-部署" class="headerlink" title="四、Presto 部署"></a>四、Presto 部署</h2><h3 id="4-1-Presto-单节点部署"><a href="#4-1-Presto-单节点部署" class="headerlink" title="4.1 Presto 单节点部署"></a>4.1 Presto 单节点部署</h3><h4 id="4-1-1-下载-presto"><a href="#4-1-1-下载-presto" class="headerlink" title="4.1.1 下载 presto"></a>4.1.1 下载 presto</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.265.1/presto-server-0.265.1.tar.gz</span><br></pre></td></tr></table></figure><h4 id="4-1-2-配置-presto"><a href="#4-1-2-配置-presto" class="headerlink" title="4.1.2 配置 presto"></a>4.1.2 配置 presto</h4><p><strong>配置 node.properties</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/presto-server-0.265.1.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ cd presto-server-0.265.1/</span><br><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ mkdir etc</span><br><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/node.properties</span><br><span class="line">node.environment=production</span><br><span class="line">node.id=ffffffff-ffff-ffff-ffff-ffffffffffff</span><br><span class="line">node.data-dir=/opt/data/presto</span><br></pre></td></tr></table></figure><blockquote><p><strong>node.environment：</strong>环境名称。群集中的所有Presto节点必须具有相同的环境名称；<br><strong>node.id：</strong>唯一的标识符 每个节点的node.id 需要不一样；<br><strong>node.data-dir：</strong>数据目录的位置。Presto将在此处存储日志和其他数据。</p></blockquote><p><strong>配置 jvm.config</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/jvm.config</span><br><span class="line">-server</span><br><span class="line">-Xmx8000M</span><br><span class="line">-XX:+UseG1GC</span><br><span class="line">-XX:G1HeapRegionSize=32M</span><br><span class="line">-XX:+UseGCOverheadLimit</span><br><span class="line">-XX:+ExplicitGCInvokesConcurrent</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX:+ExitOnOutOfMemoryError</span><br></pre></td></tr></table></figure><p><strong>配置 config.properties</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/config.properties</span><br><span class="line">coordinator=true</span><br><span class="line">node-scheduler.include-coordinator=true</span><br><span class="line">http-server.http.port=9527</span><br><span class="line">query.max-memory=5GB</span><br><span class="line">query.max-memory-per-node=6GB</span><br><span class="line">query.max-total-memory-per-node=6GB</span><br><span class="line">discovery-server.enabled=true</span><br><span class="line">discovery.uri=http://localhost:9527</span><br></pre></td></tr></table></figure><blockquote><p><strong>coordinator：</strong>允许这个 Presto 实例充当协调器（接受来自客户端的查询并管理查询执行）。<br><strong>node-scheduler.include-coordinator：</strong>允许在协调器上安排工作。对于较大的集群，协调器上的处理工作会影响查询性能，因为机器的资源无法用于调度、管理和监控查询执行的关键任务。<br><strong>http-server.http.port:</strong> 指定 HTTP 服务器的端口。Presto 使用 HTTP 进行所有内部和外部通信。<br><strong>query.max-memory：</strong>查询可以使用的最大分布式内存量。<br><strong>query.max-memory-per-node：</strong>查询可以在任何一台机器上使用的最大用户内存量。<br><strong>query.max-total-memory-per-node：</strong>查询在任何一台机器上可能使用的最大用户和系统内存量，其中系统内存是读取器、写入器和网络缓冲区等在执行过程中使用的内存。<br><strong>discovery-server.enabled：</strong>Presto 使用 Discovery 服务查找集群中的所有节点。每个 Presto 实例都会在启动时向 Discovery 服务注册自己。为了简化部署并避免运行额外的服务，Presto 协调器可以运行一个嵌入式版本的 Discovery 服务。它与 Presto 共享 HTTP 服务器，因此使用相同的端口。<br><strong>discovery.uri：</strong>发现服务器的 URI。因为我们在 Presto 协调器中启用了 Discovery 的嵌入式版本，所以这应该是 Presto 协调器的 URI。替换<code>example.net:8080</code>以匹配 Presto 协调器的主机和端口。此 URI 不得以斜杠结尾。</p></blockquote><p><strong>配置 log.properties</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/log.properties</span><br><span class="line">com.facebook.presto=INFO</span><br></pre></td></tr></table></figure><h4 id="4-1-3-运行-presto"><a href="#4-1-3-运行-presto" class="headerlink" title="4.1.3 运行 presto"></a>4.1.3 运行 presto</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ ./bin/launcher start</span><br></pre></td></tr></table></figure><blockquote><p>bin/launcher start 作为守护进程后台运行<br>bin/launcher run 则是在前台运行</p></blockquote><h4 id="4-1-4-访问-WEB-UI"><a href="#4-1-4-访问-WEB-UI" class="headerlink" title="4.1.4 访问 WEB UI"></a>4.1.4 访问 WEB UI</h4><p><img src="/2021/11/16/presto-fen-bu-shi-sql-cha-xun-yin-qing/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><h3 id="4-2-Presto-集群部署"><a href="#4-2-Presto-集群部署" class="headerlink" title="4.2 Presto 集群部署"></a>4.2 Presto 集群部署</h3><p><strong>服务规划</strong></p><table><thead><tr><th>主机名</th><th>Presto角色</th></tr></thead><tbody><tr><td>hadoop1</td><td>Coordinator</td></tr><tr><td>hadoop2</td><td>Worker</td></tr><tr><td>hadoop3</td><td>Worker</td></tr></tbody></table><h4 id="4-2-1-Coordinator-节点配置"><a href="#4-2-1-Coordinator-节点配置" class="headerlink" title="4.2.1 Coordinator 节点配置"></a>4.2.1 Coordinator 节点配置</h4><p><strong>下载 presto</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.265.1/presto-server-0.265.1.tar.gz</span><br></pre></td></tr></table></figure><p><strong>配置 presto</strong></p><p>配置 node.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/presto-server-0.265.1.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ cd presto-server-0.265.1/</span><br><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ mkdir etc</span><br><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/node.properties</span><br><span class="line">node.environment=production</span><br><span class="line">node.id=ffffffff-ffff-ffff-ffff-ffffffffffff</span><br><span class="line">node.data-dir=/opt/data/presto</span><br></pre></td></tr></table></figure><p>配置 jvm.config</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/jvm.config</span><br><span class="line">-server</span><br><span class="line">-Xmx8000M</span><br><span class="line">-XX:+UseG1GC</span><br><span class="line">-XX:G1HeapRegionSize=32M</span><br><span class="line">-XX:+UseGCOverheadLimit</span><br><span class="line">-XX:+ExplicitGCInvokesConcurrent</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX:+ExitOnOutOfMemoryError</span><br></pre></td></tr></table></figure><p>配置 config.properties</p><p>每台Presto服务器都可以充当协调器和工作器。但是专用于一台计算机仅执行协调工作的服务器可以在较大的群集上提供最佳性能。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/config.properties</span><br><span class="line">coordinator=true</span><br><span class="line">node-scheduler.include-coordinator=false</span><br><span class="line">http-server.http.port=9527</span><br><span class="line">query.max-memory=5GB</span><br><span class="line">query.max-memory-per-node=6GB</span><br><span class="line">query.max-total-memory-per-node=6GB</span><br><span class="line">discovery-server.enabled=true</span><br><span class="line">discovery.uri=http://localhost:9527</span><br></pre></td></tr></table></figure><p>配置 log.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/log.properties</span><br><span class="line">com.facebook.presto=INFO</span><br></pre></td></tr></table></figure><h4 id="4-2-2-Worker-节点配置"><a href="#4-2-2-Worker-节点配置" class="headerlink" title="4.2.2 Worker 节点配置"></a>4.2.2 Worker 节点配置</h4><p>将Coordinator 的presto分发到其它 Worker节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ rsync -av presto-server-0.265.1 hadoop@hadoop2:~/</span><br><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ rsync -av presto-server-0.265.1 hadoop@hadoop3:~/</span><br></pre></td></tr></table></figure><p>修改 node.properties 配置文件，中的 node.id 配置项，每个节点应该是唯一的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~/presto-server-0.265.1]$ vim etc/node.properties</span><br><span class="line">node.environment=production</span><br><span class="line">node.id=ffffffff-ffff-ffff-ffff-fffffffff02</span><br><span class="line">node.data-dir=/opt/data/presto</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop3 ~/presto-server-0.265.1]$ vim etc/node.properties</span><br><span class="line">node.environment=production</span><br><span class="line">node.id=ffffffff-ffff-ffff-ffff-fffffffff03</span><br><span class="line">node.data-dir=/opt/data/presto</span><br></pre></td></tr></table></figure><p>修改 config.properties 配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~/presto-server-0.265.1]$ vim etc/config.properties</span><br><span class="line">[hadoop@hadoop3 ~/presto-server-0.265.1]$ vim etc/config.properties</span><br><span class="line">coordinator=false #work节点需要填写false</span><br><span class="line">http-server.http.port=9527</span><br><span class="line">query.max-memory=5GB</span><br><span class="line">query.max-memory-per-node=6GB</span><br><span class="line">query.max-total-memory-per-node=6GB</span><br><span class="line">discovery.uri=http://hadoop1:9527</span><br></pre></td></tr></table></figure><h4 id="4-2-3-运行-presto"><a href="#4-2-3-运行-presto" class="headerlink" title="4.2.3 运行 presto"></a>4.2.3 运行 presto</h4><p>集群启动方式与单点的一样，每台都启动起来即可使用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ ./bin/launcher start</span><br><span class="line">[hadoop@hadoop2 ~/presto-server-0.265.1]$ ./bin/launcher start</span><br><span class="line">[hadoop@hadoop3 ~/presto-server-0.265.1]$ ./bin/launcher start</span><br></pre></td></tr></table></figure><h4 id="4-2-4-访问-WEB-UI"><a href="#4-2-4-访问-WEB-UI" class="headerlink" title="4.2.4 访问 WEB UI"></a>4.2.4 访问 WEB UI</h4><p><img src="/2021/11/16/presto-fen-bu-shi-sql-cha-xun-yin-qing/%E5%9B%BE%E7%89%873.png" alt="图片3"></p><h3 id="4-3-Prest-添加MySQL数据源"><a href="#4-3-Prest-添加MySQL数据源" class="headerlink" title="4.3 Prest 添加MySQL数据源"></a>4.3 Prest 添加MySQL数据源</h3><p>要配置MySQL连接器，请在<code>etc/catalog</code>命名中创建目录属性文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ mkdir etc/catalog</span><br></pre></td></tr></table></figure><blockquote><p><strong>配置 mysql Connector</strong>，在所有节点上配置</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/catalog/mysql.properties</span><br><span class="line">connector.name=mysql</span><br><span class="line">connection-url=jdbc:mysql://192.168.126.145:3306?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;autoReconnect=true&amp;failOverReadOnly=false&amp;serverTimezone=GMT%2B10</span><br><span class="line">connection-user=admin</span><br><span class="line">connection-password=abc!@#123ABC</span><br></pre></td></tr></table></figure><blockquote><p><strong>重启presto服务</strong>，重启所有节点的presto服务</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ ./bin/launcher stop</span><br><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ ./bin/launcher start</span><br></pre></td></tr></table></figure><p>客户端工具连接presto，使用dbeaver客户端连接presto</p><p><img src="/2021/11/16/presto-fen-bu-shi-sql-cha-xun-yin-qing/%E5%9B%BE%E7%89%874.png" alt="图片4"></p><p><img src="/2021/11/16/presto-fen-bu-shi-sql-cha-xun-yin-qing/%E5%9B%BE%E7%89%875.png" alt="图片5"></p><h3 id="4-4-Presto-添加-Hive数据源"><a href="#4-4-Presto-添加-Hive数据源" class="headerlink" title="4.4 Presto 添加 Hive数据源"></a>4.4 Presto 添加 Hive数据源</h3><p><strong>配置 hive Connector</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/catalog/hive.properties</span><br><span class="line">connector.name=hive-hadoop2</span><br><span class="line">hive.metastore.uri=thrift://hadoop1:9083</span><br><span class="line">hive.config.resources=etc/cluster/core-site.xml,etc/cluster/hdfs-site.xml</span><br></pre></td></tr></table></figure><p>对于基本设置，Presto 会自动配置 HDFS 客户端，不需要任何配置文件。在某些情况下，例如使用联合 HDFS 或 NameNode 高可用性时，需要指定额外的 HDFS 客户端选项才能访问您的 HDFS 集群。为此，添加<code>hive.config.resources</code>属性以引用您的 HDFS 配置文件。</p><p>如果presto运行在非hadoop节点上，则需要将hadoop的<code>core-site.xml</code> 和 <code>hdfs-site.xml</code>配置文件拷贝到所有presto节点上。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ cp /home/hadoop/hadoop-2.7.2/etc/hadoop/&#123;core,hdfs&#125;-site.xml etc/cluster/</span><br><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ rsync -sv etc/cluster/ hadoop@hadoop2:~/presto-server-0.265.1/etc/</span><br><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ rsync -sv etc/cluster/ hadoop@hadoop3:~/presto-server-0.265.1/etc/</span><br></pre></td></tr></table></figure><p>重启所有节点presto服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ ./bin/launcher stop</span><br><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ ./bin/launcher start</span><br></pre></td></tr></table></figure><h3 id="4-5-Presto-基于文件的用户管理配置（不完整，待补充）"><a href="#4-5-Presto-基于文件的用户管理配置（不完整，待补充）" class="headerlink" title="4.5 Presto 基于文件的用户管理配置（不完整，待补充）"></a>4.5 Presto 基于文件的用户管理配置（不完整，待补充）</h3><p><strong>启用密码文件身份认证</strong>，设置 <code>etc/config.properties</code> 中的<a href="https://trino.io/docs/current/security/authentication-types.html" target="_blank" rel="noopener">密码验证类型</a></p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">http-server.authentication.type</span>=<span class="string">PASSWORD</span></span><br></pre></td></tr></table></figure><p><strong>添加 <code>etc/password-authenticator.properties</code> 配置文件</strong>，使用<code>file</code>身份验证器名称在协调器上创建一个文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/password-authenticator.properties</span><br><span class="line">password-authenticator.name=file</span><br><span class="line">file.password-file=etc/password.db</span><br></pre></td></tr></table></figure><p>可用的配置属性有</p><blockquote><table><thead><tr><th><code>file.password-file</code></th><th>密码文件的路径。</th></tr></thead><tbody><tr><td><code>file.refresh-period</code></td><td>重新加载密码文件的频率。默认为<code>5s</code>.</td></tr><tr><td><code>file.auth-token-cache.max-size</code></td><td>缓存的已验证密码的最大数量。默认为<code>1000</code>.</td></tr></tbody></table></blockquote><p><strong>创建密码文件</strong></p><p>添加用于hive认证的用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">创建密码文件</span></span><br><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ touch etc/password.db</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">创建用户</span></span><br><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ htpasswd -B -C 10 etc/password.db admin</span><br></pre></td></tr></table></figure><p><strong>配置基于文件的系统访问控制</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/access-control.properties</span><br><span class="line">access-control.name=file</span><br><span class="line">security.config-file=etc/rules.json</span><br><span class="line">security.refresh-period=1s</span><br></pre></td></tr></table></figure><p><strong>配置hive用户访问权限规则</strong></p><p>例如，如果你想只允许用户<code>admin</code>访问 <code>mysql</code>和<code>hive</code>目录，允许所有用户访问<code>hive</code> 目录，允许用户<code>hive2</code>用户访问<code>mysql</code>目录，可以使用以下规则</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/presto-server-0.265.1]$ vim etc/rules.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"catalogs"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"user"</span>: <span class="string">"admin"</span>,</span><br><span class="line">      <span class="attr">"catalog"</span>: <span class="string">"(mysql|hive)"</span>,</span><br><span class="line">      <span class="attr">"allow"</span>: <span class="string">"all"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"catalog"</span>: <span class="string">"hive"</span>,</span><br><span class="line">      <span class="attr">"allow"</span>: <span class="string">"read-only"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"user"</span>: <span class="string">"hive2"</span>,</span><br><span class="line">      <span class="attr">"catalog"</span>: <span class="string">"mysql"</span>,</span><br><span class="line">      <span class="attr">"allow"</span>: <span class="string">"all"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上只能对访问的用户进行权限绑定，至于账户密码认证需要做一些开发上的修改，可以参考：<a href="https://blog.csdn.net/a80090023/article/details/119616321" target="_blank" rel="noopener">https://blog.csdn.net/a80090023/article/details/119616321</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a href=&quot;https://prestodb.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Presto&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
      <category term="OLAP" scheme="http://chenzhonzhou.github.io/tags/OLAP/"/>
    
      <category term="Presto" scheme="http://chenzhonzhou.github.io/tags/Presto/"/>
    
  </entry>
  
  <entry>
    <title>大数据主流流计算Flink及Storm、Spark比较</title>
    <link href="http://chenzhonzhou.github.io/2021/11/16/da-shu-ju-zhu-liu-liu-ji-suan-flink-ji-storm-spark-bi-jiao/"/>
    <id>http://chenzhonzhou.github.io/2021/11/16/da-shu-ju-zhu-liu-liu-ji-suan-flink-ji-storm-spark-bi-jiao/</id>
    <published>2021-11-16T08:30:13.000Z</published>
    <updated>2021-11-16T08:37:13.292Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --><h2 id="一、有限数据集和无限数据集"><a href="#一、有限数据集和无限数据集" class="headerlink" title="一、有限数据集和无限数据集"></a>一、有限数据集和无限数据集</h2><p><strong>有限数据集：</strong>数据大小有限（固定大小，比如固定的文件），用于批处理，这一类数据主要用于mr，hive，pig，spark等批计算引擎。</p><p><strong>无限数据集：</strong>数据持续增长（属于无限大小，比如kafka中的日志数据，总是有新数据进入，并且不知道什么时候结束或者是永远不结束），用于流式处理，这一类数据主要用于storm，spark streaming，flink等一些流式计算引擎。</p><h2 id="二、apache计算引擎的发展关系"><a href="#二、apache计算引擎的发展关系" class="headerlink" title="二、apache计算引擎的发展关系"></a>二、apache计算引擎的发展关系</h2><p>在apche中的三篇论文鉴定大数据的基础其中mr收到其中一篇论文的启发创造了mapreduce，同时随着时代的发展也出现了其他的技术技术。</p><h3 id="2-1-第一代计算引擎-mapreduce"><a href="#2-1-第一代计算引擎-mapreduce" class="headerlink" title="2.1 第一代计算引擎 mapreduce"></a>2.1 第一代计算引擎 mapreduce</h3><blockquote><p>mapreduce 作为第一个计算引擎，用于批处理，是计算引擎的先驱，内部支持机器学习但是现在机器学习库不在更新，并且mapreduce 编写十分的耗时，开发效率低，开发时间成本太大，所以很少有企业写mapreduce 来跑程序。</p></blockquote><h3 id="2-2-第二代计算引擎-pig-hive"><a href="#2-2-第二代计算引擎-pig-hive" class="headerlink" title="2.2 第二代计算引擎 pig/hive"></a>2.2 第二代计算引擎 pig/hive</h3><blockquote><ul><li>作为第二代引擎pig/hive 对hadoop（如果不知道hadoop的话，建议不要看了。。。。。）进行了嵌套，其存储基于hdfs，计算基于mr，hive/pig在处理任务时首先会把本身的代码解析为一个个m/r任务，这样就大大的降低了mr的编写编写成本；</li><li>pig 有自己的脚本语言属于，比hive更加的灵活；</li><li>hive 属于类sql语法，虽然没有pig灵活，但是对于现在程序员都会sql的世界来说大家更喜欢使用hive；</li><li>pig/hive 只支持批处理，且支持机器学习 （hivemall）。</li></ul></blockquote><h3 id="2-3-第三代计算引擎-spark-storm"><a href="#2-3-第三代计算引擎-spark-storm" class="headerlink" title="2.3 第三代计算引擎 spark/storm"></a>2.3 第三代计算引擎 spark/storm</h3><blockquote><p>随着时代的发展，企业对数据实时处理的需求愈来愈大，所以就出现了storm/spark</p><ul><li>这两者有着自己的计算模式；</li><li>storm属于真正的流式处理，低延迟（ms级延迟），高吞吐，且每条数据都会触发计算；</li><li>spark属于批处理转化为流处理即将流式数据根据时间切分成小批次进行计算，对比与storm而言延迟会高于0.5s（s级延迟），但是性能上的消耗低于storm。<code>流式计算是批次计算的特例（流式计算是拆分计算的结果）</code>。</li></ul></blockquote><h3 id="2-4-第四代计算引擎-flink"><a href="#2-4-第四代计算引擎-flink" class="headerlink" title="2.4 第四代计算引擎 flink"></a>2.4 第四代计算引擎 flink</h3><blockquote><ul><li>flink2015年出现在apache，后来又被阿里巴巴技术团队进行优化（这里我身为国人为之自豪）为blink，flink支持流式计算也支持的批次处理；</li><li>flink为流式计算而生属于每一条数据触发计算，在性能的消耗低于storm，吞吐量高于storm，延时低于storm，并且比storm更加易于编写。因为storm如果要实现窗口需要自己编写逻辑，但是flink中有窗口方法；</li><li>flink内部支持多种函数，其中包括窗口函数和各种算子（这一点和spark很像，但是在性能和实时上spark是没有办法比较的）；</li><li>flink支持仅一次语义保证数据不丢失；</li><li>flink支持通过envent time来控制窗口时间，支持乱序时间和时间处理（这点我觉得很厉害）；</li><li>对于批次处理flink的批处理可以理解为 “批次处理是流式处理的特例”（批次计算是流式计算的合并结果）。</li></ul></blockquote><h2 id="三、三种流式计算基本原理"><a href="#三、三种流式计算基本原理" class="headerlink" title="三、三种流式计算基本原理"></a>三、三种流式计算基本原理</h2><h3 id="3-1-Apache-Storm"><a href="#3-1-Apache-Storm" class="headerlink" title="3.1 Apache Storm"></a>3.1 Apache Storm</h3><p>在Storm中，需要先设计一个实时计算结构，我们称之为拓扑（topology）。之后，这个拓扑结构会被提交给集群，其中主节点（master node）负责给工作节点（worker node）分配代码，工作节点负责执行代码。在一个拓扑结构中，包含spout和bolt两种角色。数据在spouts之间传递，这些spouts将数据流以tuple元组的形式发送；而bolt则负责转换数据流。</p><p><img src="https://pic1.zhimg.com/80/v2-d0bfbd044b0f3f8952462596cbafd400_720w.jpg" alt="img"></p><h3 id="3-2-Apache-Spark"><a href="#3-2-Apache-Spark" class="headerlink" title="3.2 Apache Spark"></a>3.2 Apache Spark</h3><p>Spark Streaming，即核心Spark API的扩展，不像Storm那样一次处理一个数据流。相反，它在处理数据流之前，会按照时间间隔对数据流进行分段切分。Spark针对连续数据流的抽象，我们称为DStream（Discretized Stream）。 DStream是小批处理的RDD（弹性分布式数据集）， RDD则是分布式数据集，可以通过任意函数和滑动数据窗口（窗口计算）进行转换，实现并行操作。</p><p><img src="https://pic3.zhimg.com/80/v2-12a49e59c703ffdc6185b88796774e36_720w.jpg" alt="img"></p><h3 id="3-3-Apache-Flink"><a href="#3-3-Apache-Flink" class="headerlink" title="3.3 Apache Flink"></a>3.3 Apache Flink</h3><p>针对流数据+批数据的计算框架。把批数据看作流数据的一种特例，延迟性较低(毫秒级)，且能够保证消息传输不丢失不重复。</p><p><img src="https://pic1.zhimg.com/80/v2-1be738892ef8ab9933b1313f8ea128f8_720w.jpg" alt="img"></p><p>Flink创造性地统一了流处理和批处理，作为流处理看待时输入数据流是无界的，而批处理被作为一种特殊的流处理，只是它的输入数据流被定义为有界的。Flink程序由Stream和Transformation这两个基本构建块组成，其中Stream是一个中间结果数据，而Transformation是一个操作，它对一个或多个输入Stream进行计算处理，输出一个或多个结果Stream。</p><h2 id="四、区别对比"><a href="#四、区别对比" class="headerlink" title="四、区别对比"></a>四、区别对比</h2><table><thead><tr><th>对比项</th><th>Flink</th><th>Storm</th><th>Spark Streaming</th></tr></thead><tbody><tr><td>诞生时间</td><td>2014 年 12月</td><td>2014 年 9 月 17 日</td><td>2010</td></tr><tr><td>社区及生态</td><td>被阿里收购后，有了官方中文社区、很多资料也有中文版、在国内使用率最高</td><td>社区活跃，文档资料详细</td><td>社区活跃，文档资料详细</td></tr><tr><td>状态管理</td><td>基于操作的状态管理</td><td>无状态管理</td><td>基于 DStream的状态管理</td></tr><tr><td>消息投递保证次数</td><td>最多一次、至少一次、恰好一次</td><td>最多一次、至少一次</td><td>最多一次、至少一次、恰好一次</td></tr><tr><td>容错方式</td><td>checkpoint机制：通过分布式一致性快照机制，对数据流和算子状态进行保存</td><td>ACK 机制：对每个消息进行全链路跟踪，失败或超时进行重发</td><td>RDD CheckPoint（基于 RDD 做 CheckPoint）</td></tr><tr><td>吞吐量</td><td>高</td><td>低</td><td>高</td></tr><tr><td>延迟</td><td>低</td><td>低</td><td>中等</td></tr><tr><td>模型</td><td>Native（数据进入立即处理）</td><td>Native（数据进入立即处理）</td><td>Micro-Batching</td></tr><tr><td>API语言</td><td>scala、java、python、sql</td><td>scala、java、sql</td><td>scala、java、python、sql、R</td></tr><tr><td>运行时</td><td>单个jvm进程可以有多个应用多个任务</td><td>单个jvm进程单个应用多个任务，每个线程多任务</td><td>单个jvm进程单个应用多个任务，每个线程单个任务</td></tr></tbody></table><ul><li>相比于storm ，spark和flink两个都支持窗口和算子，减少了不少的编程时间；</li><li>flink相比于storm和spark，flink支持乱序和延迟时间（在实际场景中，这个功能很牛逼），个人觉得就这个功能就可以锤爆spark；</li><li>对于spark而言他的优势就是机器学习，如果我们的场景中对实时要求不高可以考虑spark，但是如果是要求很高就考虑使用flink，比如对用户异常消费进行监控，如果这个场景使用spark的话那么等到系统发现开始预警的时候（0.5s)，罪犯已经完成了交易，可想而知在某些场景下flink的实时有多重要。</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;一、有限数据集和无限数据集&quot;&gt;&lt;a href=&quot;#一、有限数据集和无限数据集&quot; class=&quot;headerlink&quot; title=&quot;一
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Flink 常用的几种模式部署</title>
    <link href="http://chenzhonzhou.github.io/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/"/>
    <id>http://chenzhonzhou.github.io/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/</id>
    <published>2021-11-16T07:51:25.000Z</published>
    <updated>2021-11-16T08:33:33.937Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><h3 id="一、Flink-local-模式"><a href="#一、Flink-local-模式" class="headerlink" title="一、Flink local 模式"></a>一、Flink local 模式</h3><h4 id="1-1-配置jdk环境"><a href="#1-1-配置jdk环境" class="headerlink" title="1.1 配置jdk环境"></a>1.1 配置jdk环境</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/jdk-8u301-linux-x64.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ vim .bash_profile</span><br><span class="line">export JAVA_HOME=/home/hadoop/jdk1.8.0_301</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib</span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure><h4 id="1-2-部署flink"><a href="#1-2-部署flink" class="headerlink" title="1.2 部署flink"></a>1.2 部署flink</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://dlcdn.apache.org/flink/flink-1.14.0/flink-1.14.0-bin-scala_2.11.tgz</span><br><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/flink-1.14.0-bin-scala_2.11.tgz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">启动flink</span></span><br><span class="line">[hadoop@hadoop1 ~]$ ./flink-1.14.0/bin/start-cluster.sh</span><br></pre></td></tr></table></figure><h4 id="1-3-访问-web-页面"><a href="#1-3-访问-web-页面" class="headerlink" title="1.3 访问 web 页面"></a>1.3 访问 web 页面</h4><p><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><p>默认端口8081，端口冲突可以修改 <code>conf/flink-conf.yaml</code> 配置项 <code>rest.port:</code> 进行修改</p><h4 id="1-4-提交作业（Job）"><a href="#1-4-提交作业（Job）" class="headerlink" title="1.4 提交作业（Job）"></a>1.4 提交作业（Job）</h4><p>Flink 的 Releases 附带了许多的示例作业。可以任意选择一个，快速部署到已运行的集群上。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ ./bin/flink run examples/streaming/WordCount.jar</span><br></pre></td></tr></table></figure><p>flink run命令参数</p><blockquote><p><strong>-c,–class &lt;classname&gt;：</strong> Flink应用程序的入口<br><strong>-C,–classpath &lt;url&gt;：</strong> 指定所有节点都可以访问到的url,可用于多个应用程序都需要的工具类加载<br><strong>-d,–detached：</strong> 是否使用分离模式，就是提交任务,cli是否退出,加了-d参数,cli会退出<br><strong>-n,–allowNonRestoredState:</strong> 允许跳过无法还原的savepoint。比如删除了代码中的部分operator<br><strong>-p,–parallelism &lt;parallelism&gt;：</strong> 执行并行度<br><strong>-s,–fromSavepoint：</strong> 从savepoint恢复任务<br><strong>-sae,–shutdownOnAttachedExit：</strong> 以attached模式提交，客户端退出的时候关闭集群<br><strong>flink yarn-cluster：</strong> 模式</p></blockquote><p>yarn-cluster模式</p><blockquote><p><strong>-d,–detached：</strong> 是否使用分离模式<br><strong>-m,–jobmanager &lt;arg&gt;：</strong> 指定提交的jobmanager<br><strong>-yat,–yarnapplicationType &lt;arg&gt;：</strong> 设置yarn应用的类型<br><strong>-yD &lt;property=value&gt;：</strong> 使用给定属性的值<br><strong>-yd,–yarndetached：</strong> 使用yarn分离模式<br><strong>-yh,–yarnhelp：</strong> yarn session的帮助<br><strong>-yid,–yarnapplicationId &lt;arg&gt;：</strong> 挂到正在运行的yarnsession上<br><strong>-yj,–yarnjar &lt;arg&gt;：</strong> Flink jar文件的路径<br><strong>-yjm,–yarnjobManagerMemory &lt;arg&gt;：</strong> jobmanager的内存(单位M)<br><strong>-ynl,–yarnnodeLabel &lt;arg&gt;：</strong> 指定 YARN 应用程序 YARN 节点标签<br><strong>-ynm,–yarnname &lt;arg&gt;：</strong> 自定义yarn应用名称<br><strong>-yq,–yarnquery：</strong> 显示yarn的可用资源<br><strong>-yqu,–yarnqueue &lt;arg&gt;：</strong> 指定yarn队列<br><strong>-ys,–yarnslots &lt;arg&gt;：</strong> 指定每个taskmanager的slots数<br><strong>-yt,–yarnship &lt;arg&gt;：</strong> 在指定目录中传输文件<br><strong>-ytm,–yarntaskManagerMemory &lt;arg&gt;：</strong> 每个taskmanager的内存<br><strong>-yz,–yarnzookeeperNamespace &lt;arg&gt;：</strong> 用来创建ha的zk子路径的命名空间<br><strong>-z,–zookeeperNamespace &lt;arg&gt;：</strong> 用来创建ha的zk子路径的命名空间</p></blockquote><p>通用CLI模式</p><blockquote><p><strong>-D &lt;property=value&gt;：</strong>允许指定多个常规配置选项。有关可用选项，请访问 <a href="https://nightlies.apache.org/flink/flink-docs-stable/ops/config.html" target="_blank" rel="noopener">https://nightlies.apache.org/flink/flink-docs-stable/ops/config.html</a><br><strong>-e,–executor &lt;arg&gt;：</strong>不推荐：请改用-t选项，该选项在”Application Mode”中也可用。用于执行给定作业的执行器的名称，相当于“execution.target”配置选项。当前可用的执行器有：”remote”, “local”, “kubernetes-session”, “yarn-per-job”, “yarn-session”。<br><strong>-t,–target &lt;arg&gt;：</strong>给定应用程序的部署目标，相当于”execution.target”配置选项。对于“run”操作，当前可用的目标有：”remote”, “local”, “kubernetes-session”,”yarn-per-job”, “yarn-session”。对于”run-application”操作，当前可用的有：”kubernetes-application”,”yarn-application”。</p></blockquote><p>通过web查看运行情况<br><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><p>查看运行日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ tail log/flink-hadoop-taskexecutor-0-hadoop1.out</span><br><span class="line">(nymph,1)</span><br><span class="line">(in,3)</span><br><span class="line">(thy,1)</span><br><span class="line">(orisons,1)</span><br><span class="line">(be,4)</span><br><span class="line">(all,2)</span><br><span class="line">(my,1)</span><br><span class="line">(sins,1)</span><br><span class="line">(remember,1)</span><br><span class="line">(d,4)</span><br></pre></td></tr></table></figure><h3 id="二、Flink-Standalone-集群"><a href="#二、Flink-Standalone-集群" class="headerlink" title="二、Flink Standalone 集群"></a>二、Flink Standalone 集群</h3><p><strong>服务规划</strong></p><table><thead><tr><th>主机名</th><th>Flink角色</th><th>其它依赖</th></tr></thead><tbody><tr><td>hadoop1</td><td>jobManager（Master）</td><td>JDK1.8</td></tr><tr><td>hadoop2</td><td>taskManager（Worker）</td><td>JDK1.8</td></tr><tr><td>hadoop3</td><td>taskManager（Worker）</td><td>JDK1.8</td></tr></tbody></table><h4 id="2-1-准备工作"><a href="#2-1-准备工作" class="headerlink" title="2.1 准备工作"></a>2.1 准备工作</h4><p>服务器免密登录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[dev@hadoop1 ~]# su - hadoop</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-keygen</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-copy-id hadoop1</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-copy-id hadoop2</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-copy-id hadoop3</span><br></pre></td></tr></table></figure><p>配置jdk 1.8环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/jdk-8u301-linux-x64.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ vim .bash_profile</span><br><span class="line">export JAVA_HOME=/home/hadoop/jdk1.8.0_301</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib</span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~]$ source .bash_profile</span><br></pre></td></tr></table></figure><h4 id="2-2-修改-flink-配置"><a href="#2-2-修改-flink-配置" class="headerlink" title="2.2 修改 flink 配置"></a>2.2 修改 flink 配置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">下载flink</span></span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://dlcdn.apache.org/flink/flink-1.14.0/flink-1.14.0-bin-scala_2.11.tgz</span><br><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/flink-1.14.0-bin-scala_2.11.tgz</span><br></pre></td></tr></table></figure><p>修改 flink-conf.yaml 配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ grep -Ev "^$|#" conf/flink-conf.yaml</span><br><span class="line">jobmanager.rpc.address: hadoop1</span><br><span class="line">jobmanager.rpc.port: 6123</span><br><span class="line">jobmanager.memory.process.size: 1600m</span><br><span class="line">taskmanager.memory.process.size: 1728m</span><br><span class="line">taskmanager.numberOfTaskSlots: 1</span><br><span class="line">parallelism.default: 1</span><br><span class="line">jobmanager.execution.failover-strategy: region</span><br><span class="line">rest.port: 8089</span><br></pre></td></tr></table></figure><p>修改 masters 配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ vim conf/masters</span><br><span class="line">hadoop1:8089</span><br></pre></td></tr></table></figure><p>修改 workers 配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ vim conf/workers</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure><h4 id="2-3-分发-flink-配置"><a href="#2-3-分发-flink-配置" class="headerlink" title="2.3 分发 flink  配置"></a>2.3 分发 flink 配置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ rsync -av flink-1.14.0 hadoop@hadoop2:~/</span><br><span class="line">[hadoop@hadoop1 ~]$ rsync -av flink-1.14.0 hadoop@hadoop3:~/</span><br></pre></td></tr></table></figure><h4 id="2-4-启动-flink-集群"><a href="#2-4-启动-flink-集群" class="headerlink" title="2.4 启动 flink 集群"></a>2.4 启动 flink 集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ ./bin/start-cluster.sh</span><br></pre></td></tr></table></figure><p>默认端口8081，端口冲突可以修改 <code>conf/flink-conf.yaml</code> 配置项 <code>rest.port:</code> 进行修改</p><p>访问web页面<br><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%873.png" alt="图片3"></p><h4 id="2-5-提交示例作业"><a href="#2-5-提交示例作业" class="headerlink" title="2.5 提交示例作业"></a>2.5 提交示例作业</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ ./bin/flink run examples/streaming/WindowJoin.jar</span><br></pre></td></tr></table></figure><p>在web查看任务<br><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%874.png" alt="图片4"></p><h3 id="三、Flink-Standalone-高可用"><a href="#三、Flink-Standalone-高可用" class="headerlink" title="三、Flink Standalone 高可用"></a>三、Flink Standalone 高可用</h3><p><strong>服务规划</strong></p><table><thead><tr><th>主机名</th><th>Flink角色</th><th>其它依赖</th></tr></thead><tbody><tr><td>hadoop1</td><td>jobManager</td><td>JDK1.8、zookeeper</td></tr><tr><td>hadoop2</td><td>jobManager、taskManager</td><td>JDK1.8、zookeeper</td></tr><tr><td>hadoop3</td><td>taskManager</td><td>JDK1.8、zookeeper</td></tr></tbody></table><p><strong>前置条件</strong></p><blockquote><p>Flink Standalone、zookeeper集群是可用的</p></blockquote><h4 id="3-1-修改-flink-配置"><a href="#3-1-修改-flink-配置" class="headerlink" title="3.1 修改 flink 配置"></a>3.1 修改 flink 配置</h4><p>修改 flink-conf.yaml 配置文件，增加以下配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ vim conf/flink-conf.yaml</span><br><span class="line"><span class="meta">#</span><span class="bash">高可用性配置</span></span><br><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.storageDir: hdfs:///flink/ha</span><br><span class="line">high-availability.zookeeper.quorum: hadoop1:2181, hadoop2:2181, hadoop3:2181</span><br><span class="line">high-availability.zookeeper.path.root: ./flink</span><br><span class="line">high-availability.cluster-id: /cluster_one</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">checkpoint配置</span></span><br><span class="line">state.backend: filesystem</span><br><span class="line">state.checkpoints.dir: hdfs:///flink-checkpoints</span><br><span class="line">state.savepoints.dir: hdfs:///flink-savepoints</span><br><span class="line">state.checkpoints.num-retained: 20</span><br></pre></td></tr></table></figure><p>修改 masters 配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ vim conf/masters</span><br><span class="line">hadoop1:8089</span><br><span class="line">hadoop2:8089</span><br></pre></td></tr></table></figure><h4 id="3-2-下载-Hadoop-附加组件"><a href="#3-2-下载-Hadoop-附加组件" class="headerlink" title="3.2 下载 Hadoop 附加组件"></a>3.2 下载 Hadoop 附加组件</h4><p>在Flink1.8版本后，Flink官方提供的安装包里没有整合HDFS的jar，需要额外下载jar包并放在Flink的lib目录下，使Flink能够支持对Hadoop的操作</p><p>前往官网 <a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">https://flink.apache.org/downloads.html</a> ，找到 <code>Additional Components</code> 选项，下载相应Hadoop版本的包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ wget https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.7.5-10.0/flink-shaded-hadoop-2-uber-2.7.5-10.0.jar</span><br><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ mv flink-shaded-hadoop-2-uber-2.7.5-10.0.jar lib/</span><br></pre></td></tr></table></figure><p>或者添加 <code>HADOOP_CLASSPATH</code> 环境变量，也是可以的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vim .bash_profile</span><br><span class="line">export HADOOP_HOME=/home/hadoop/hadoop-2.7.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure><h4 id="3-3-颁发-flink-配置"><a href="#3-3-颁发-flink-配置" class="headerlink" title="3.3 颁发 flink 配置"></a>3.3 颁发 flink 配置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ rsync -av flink-1.14.0 hadoop@hadoop2:~/</span><br><span class="line">[hadoop@hadoop1 ~]$ rsync -av flink-1.14.0 hadoop@hadoop3:~/</span><br></pre></td></tr></table></figure><p>除此之外，需要将master2上 <code>flink-conf.yaml</code> 配置中的监听地址修改为master2所在机器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~/flink-1.14.0]$ vim conf/flink-conf.yaml</span><br><span class="line">jobmanager.rpc.address: hadoop2</span><br></pre></td></tr></table></figure><h4 id="3-4-启动-flink-集群"><a href="#3-4-启动-flink-集群" class="headerlink" title="3.4 启动 flink 集群"></a>3.4 启动 flink 集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ ./bin/start-cluster.sh</span><br></pre></td></tr></table></figure><h4 id="3-5-验证高可用"><a href="#3-5-验证高可用" class="headerlink" title="3.5 验证高可用"></a>3.5 验证高可用</h4><p>通过 zookeeper 查看当前集群的<code>master</code>是那台节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/zookeeper-3.4.8]$ ./bin/zkCli.sh</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] get /flink/cluster_one/leader/rest_server/connection_info</span><br><span class="line">��whttp://hadoop1:8089srjava.util.UUID����m�/J</span><br></pre></td></tr></table></figure><p>关闭当前的master进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ jps |grep StandaloneSessionClusterEntrypoint</span><br><span class="line">10582 StandaloneSessionClusterEntrypoint</span><br><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ kill 10582</span><br></pre></td></tr></table></figure><p>提交示例作业</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~/flink-1.14.0]$ ./bin/flink run examples/streaming/WordCount.jar</span><br></pre></td></tr></table></figure><p>在另一台master web查看作业<br><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%875.png" alt="图片5"></p><h3 id="四、Flink-On-Yarn-模式"><a href="#四、Flink-On-Yarn-模式" class="headerlink" title="四、Flink On Yarn 模式"></a>四、Flink On Yarn 模式</h3><p>在YARN上使用Flink有3种模式：Per-Job模式、Session模式和Application模式。</p><p><strong>前置条件</strong></p><blockquote><p>Hadoop，Yarn集群是可用的<br>配置好 HADOOP_HOME 环境变量</p></blockquote><h4 id="4-1-Session-模式"><a href="#4-1-Session-模式" class="headerlink" title="4.1 Session 模式"></a>4.1 Session 模式</h4><p><strong>启动flink集群</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ ./bin/yarn-session.sh -n 3 -s 3 -nm yarn-session1</span><br></pre></td></tr></table></figure><p>yarn-session.sh 参数说明</p><blockquote><p>-n,–container &lt;arg&gt;： 表示分配容器的数量（也就是 TaskManager 的数量）。<br>-D &lt;arg&gt;： 动态属性。<br>-d,–detached： 在后台独立运行。<br>-jm,–jobManagerMemory &lt;arg&gt;：设置 JobManager 的内存，单位是 MB。<br>-nm,–name：在 YARN 上为一个自定义的应用设置一个名字。<br>-q,–query：显示 YARN 中可用的资源（内存、cpu 核数）。<br>-qu,–queue &lt;arg&gt;：指定 YARN 队列。<br>-s,–slots &lt;arg&gt;：每个 TaskManager 使用的 Slot 数量。<br>-tm,–taskManagerMemory &lt;arg&gt;：每个 TaskManager 的内存，单位是 MB。<br>-z,–zookeeperNamespace &lt;arg&gt;：针对 HA 模式在 ZooKeeper 上创建 NameSpace。<br>-id,–applicationId &lt;yarnAppId&gt;：指定 YARN 集群上的任务 ID，附着到一个后台独立运行的 yarn session 中</p></blockquote><p><strong>查看yarn UI界面</strong>，可以看到提交的application<br><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%876.png" alt="图片6"></p><p><strong>使用flink run提交示例作业</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ ./bin/flink run -t yarn-session \</span><br><span class="line">-Dyarn.application.id=application_1635158058849_0059 \</span><br><span class="line">examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure><p>之后可以通过yarn找到相应的 <code>application</code> 中的 <code>ApplicationMaster</code> 进入flink管理页面查看任务运行情况<br><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%877.png" alt="图片7"></p><p><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%878.png" alt="图片8"></p><p><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%879.png" alt="图片9"></p><p><strong>关闭yarn-session</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ yarn application -kill application_1635158058849_0059</span><br></pre></td></tr></table></figure><p>查看yarn UI中的application状态<br><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%8710.png" alt="图片10"></p><p>session模式将在 <code>/tmp/.yarn-properties-&lt;username&gt;</code> 中创建一个隐藏文件，在提交作业时，命令行界面将拾取该文件进行集群发现。</p><h4 id="4-2-Per-Job模式"><a href="#4-2-Per-Job模式" class="headerlink" title="4.2 Per-Job模式"></a>4.2 Per-Job模式</h4><p><strong>直接提交作业</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/flink-1.14.0]$ ./bin/flink run -t yarn-per-job \</span><br><span class="line">-yjm 4096 \</span><br><span class="line">-ytm 16384 \</span><br><span class="line">-ys 4 \</span><br><span class="line">examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure><p>在yarn UI中查看application执行状态<br><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%8711.png" alt="图片11"></p><p><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%8712.png" alt="图片12"></p><h4 id="4-3-Application-模式"><a href="#4-3-Application-模式" class="headerlink" title="4.3 Application 模式"></a>4.3 Application 模式</h4><p><strong>直接提交作业</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~/flink-1.14.0]$ ./bin/flink run-application -t yarn-application \</span><br><span class="line">-yjm 4096 \</span><br><span class="line">-ytm 16384 \</span><br><span class="line">-ys 4 \</span><br><span class="line">examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure><p>在yarn UI中查看application执行状态<br><img src="/2021/11/16/flink-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%8713.png" alt="图片13"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;一、Flink-local-模式&quot;&gt;&lt;a href=&quot;#一、Flink-local-模式&quot; class=&quot;headerlink&quot; ti
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
      <category term="Hadoop Flink" scheme="http://chenzhonzhou.github.io/tags/Hadoop-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 计算引擎Flink</title>
    <link href="http://chenzhonzhou.github.io/2021/11/12/hadoop-ji-suan-yin-qing-flink/"/>
    <id>http://chenzhonzhou.github.io/2021/11/12/hadoop-ji-suan-yin-qing-flink/</id>
    <published>2021-11-12T06:24:42.000Z</published>
    <updated>2021-12-21T09:59:32.393Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --><p><a href="https://flink.apache.org/" target="_blank" rel="noopener">Apache Flink</a> 是一个框架和分布式处理引擎，用于在 <strong>无边界</strong> 和 <strong>有边界</strong> 数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。</p><h2 id="一、Flink-特点"><a href="#一、Flink-特点" class="headerlink" title="一、Flink 特点"></a>一、Flink 特点</h2><h3 id="1-1-无界和有界数据"><a href="#1-1-无界和有界数据" class="headerlink" title="1.1 无界和有界数据"></a>1.1 无界和有界数据</h3><p>任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。</p><p>数据可以被作为 <strong>无界</strong> 或者 <strong>有界</strong> 流来处理。</p><ol><li><strong>无界流</strong> 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。</li><li><strong>有界流</strong> 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理。</li></ol><p><img src="https://flink.apache.org/img/bounded-unbounded.png" alt="img"></p><p><strong>Apache Flink 擅长处理无界和有界数据集</strong> 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。</p><h3 id="1-2-可靠的容错能力"><a href="#1-2-可靠的容错能力" class="headerlink" title="1.2 可靠的容错能力"></a>1.2 可靠的容错能力</h3><p>在分布式系统中，硬件故障、进程异常、应用异常、网络故障等多种多样的异常无处不在。像Flink这样的分布式计算引擎必须能够从故障中恢复到正常状态，以便实现全天候运行。这就要求引擎在故障发生后不仅可以重新启动应用程序，还要确保其内部状态保持一致，从最后一次正确的点重新执行，从用户的角度来说，最终的计算结果与未发生故障是一样的。</p><h4 id="1-3-集群级容错"><a href="#1-3-集群级容错" class="headerlink" title="1.3 集群级容错"></a>1.3 集群级容错</h4><p>（1）与集群管理器集成<br>Flink与集群管理器紧密集成，例如Hadoop YARN、Mesos或Kubernetes。当进程挂掉时，将自动启动一个新进程来接管它的工作。<br>（2）高可用性设置<br>Flink具有高可用性模式特性，可消除所有单点故障。HA模式基于Apache ZooKeeper，Zookeeper是一种经过验证的可靠的分布式协调服务。</p><h4 id="1-4-应用级容错"><a href="#1-4-应用级容错" class="headerlink" title="1.4 应用级容错"></a>1.4 应用级容错</h4><p>Flink使用轻量级分布式快照机制，设计了检查点（Checkpoint）来实现可靠的容错。其特性如下。<br>（1）一致性<br>Flink的恢复机制基于应用程序状态的一致性检查点。如果发生故障，将重新启动应用程序并从最新检查点加载其状态。结合可重放的流数据源，此特性可以保证精确、一次的状态一致性。<br>Flink、Spark、Storm等都支持引擎内的Exactly-Once语义，即确保数据仅处理一次，不会重复也不会丢失。但是在把结果写入外部存储的时候，可能会发生存储故障、网络中断、Flink应用异常恢复等多种情况，在这些情况下，部分数据可能已经写入外部存储，重复执行可能导致数据的重复写出，此时需要开发者为写出到外部存储的行为保证幂等性。<br>在Spark、Storm中需要开发者自行实现Sink，实现端到端的Exactly-Once行为。而Flink利用检查点特性，在框架层面提供了Exactly-Once的支持，内置了支持Exactly-Once语义的Sink，即使出现故障，也能保证数据只写出一次。<br>（2）轻量级<br>对于长期运行的Flink应用程序，其检查点的状态可能高达TB级，生成和保存检查应用程序的检查点成本非常高。所以Flink提供了检查点的执行异步和增量检查点，以便尽量降低生成和保存检查点带来的计算负荷，避免数据处理的延迟异常变大和吞吐量的短暂剧降。</p><h3 id="1-5-高吞吐、低延迟"><a href="#1-5-高吞吐、低延迟" class="headerlink" title="1.5 高吞吐、低延迟"></a>1.5 高吞吐、低延迟</h3><p>从Storm流计算引擎开始，大家似乎留下了这样一个印象，要实现低延迟，就要牺牲吞吐量，高吞吐、低延迟是流处理引擎的核心矛盾。以 <strong>Storm</strong> 为代表的第一代流计算引擎可以做到几十毫秒的处理延迟，但是吞吐量确实不高。后来的 <strong>Spark Streaming</strong> 基于mini-batch的流计算框架能够实现较高的吞吐量，但是数据处理的延迟不甚理想，一般可达到秒级。<br><strong>Flink</strong> 借助轻量级分布式快照机制，能够定时生成分布式快照，并将快照保存到外部存储中。检查点之间的数据处理被当作是原子的，如果失败，直接回到上一个检查点重新执行即可。在整个数据处理过程中不会产生阻塞，不必像mini-batch机制一样需要等待调度，可以持续处理数据，容错开销非常低。Flink在数据的计算、传输、序列化等方面也做了大量的优化，既能保持数据处理的低延迟，也能尽可能地提高吞吐量。</p><h3 id="1-6-大规模复杂计算"><a href="#1-6-大规模复杂计算" class="headerlink" title="1.6 大规模复杂计算"></a>1.6 大规模复杂计算</h3><p>Flink在设计之初就非常在意性能相关的任务状态和流控等关键技术的设计，这些都使得用Flink执行复杂的大规模任务时性能更胜一筹。<br>对于大规模复杂计算，尤其是长期运行的流计算应用而言，有状态计算是大数据计算引擎中一个比较大的需求点。所谓的有状态计算就是要结合历史信息进行的计算，例如对于反欺诈行为的识别，要根据用户在近几分钟之内的行为做出判断。一旦出现异常，就需要重新执行流计算任务，但重新处理所有的原始数据是不现实的，而Flink的<strong>容错机制</strong>和<strong>State</strong>能够使Flink的流计算作业恢复到近期的一个时间点，从这个时间点开始执行流计算任务，这无疑能够大大降低大规模任务失败恢复的成本。<br>Flink为了提供有状态计算的性能，针对本地状态访问进行了优化，任务状态始终驻留在内存中，如果状态大小超过可用内存，则保存在高效磁盘上的数据结构中。因此，任务通过访问本地（通常是内存中）状态来执行所有计算，从而达到特别低的处理延迟。Flink通过定期和异步检查点将本地状态进行持久存储来保证在出现故障时实现精确、一次的状态一致性。<br>Flink的轻量级容错机制也能够尽量降低大规模数据处理时的调度、管理成本，计算规模的增大不会显著增加容错，数据吞吐不会剧烈下降，数据延迟不会急剧增大。</p><h3 id="1-7-多平台部署"><a href="#1-7-多平台部署" class="headerlink" title="1.7 多平台部署"></a>1.7 多平台部署</h3><p>Flink是一个分布式计算系统，需要计算资源才能执行应用程序。Flink可以与所有常见的集群资源管理器（如Hadoop YARN、Apache Mesos和Kubernetes）集成，也可以在物理服务器上作为独立集群运行。<br>为了实现不同的部署模式，Flink设计了一套资源管理框架，针对上面提到的资源管理平台实现了对应的资源管理器（ResourceManager），能够与上面提到的资源管理平台无缝对接。<br>部署Flink应用程序时，Flink会根据应用程序配置的并行度自动识别所需资源，并向资源管理器申请资源。如果发生故障，Flink会通过请求新的资源来替换发生故障的资源。Flink提供了提交或控制应用程序的REST接口，方便与外部应用进行集成，管理Flink作业。</p><h2 id="二、Flink-架构"><a href="#二、Flink-架构" class="headerlink" title="二、Flink 架构"></a>二、Flink 架构</h2><h3 id="2-1-Flink-组件栈"><a href="#2-1-Flink-组件栈" class="headerlink" title="2.1 Flink 组件栈"></a>2.1 Flink 组件栈</h3><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><p>从上到下依次是</p><h4 id="2-1-1-Libraries层"><a href="#2-1-1-Libraries层" class="headerlink" title="2.1.1 Libraries层"></a>2.1.1 Libraries层</h4><p>该层也可以称为Flink应用框架层，是指根据API层的划分，在API层之上构建的满足特定应用场景的计算框架，总体上分为流计算和批处理两类应用框架。面向流计算的应用框架有流上SQL（Flink Table&amp;SQL）、CEP（复杂事件处理），面向批处理的应用框架有批上SQL（Flink Table&amp;SQL）、Flink ML（机器学习）、Gelly（图处理）。<br>（1）Table&amp;SQL<br>Table&amp;SQL是Flink中提供SQL语义支持的内置应用框架，其中Table API提供Scala和Java语言的SQL语义支持，允许开发者使用编码的方式实现SQL语义。SQL基于Apache Calcite，支持标准SQL，使用者可以在应用中直接使用SQL语句，同时也支持Table API和SQL的混合编码。</p><p>（2）CEP<br>CEP本质上是一种实时事件流上的模式匹配技术，是实时事件流上常见的用例。CEP通过分析事件间的关系，利用过滤、关联、聚合等技术，根据事件间的时序关系和聚合关系制定匹配规则，持续地从事件流中匹配出符合要求的事件序列，通过模式组合能够识别更加复杂的事件序列，主要用于反欺诈、风控、营销决策、网络安全分析等场景。<br>（3）Gelly<br>Gelly是一个可扩展的图形处理和分析库。Gelly是在DataSet API之上实现的，并与DataSet API集成在一起。因此，它受益于其可扩展且强大的操作符。Gelly具有内置算法，如label propagation（标签传播）、triangle （4）（4）ML<br>Flink ML是Flink的机器学习框架，定位类似于Spark MLLib，但是在目前阶段其实现的算法和成熟度距离Spark MLLib有较大差距，不具备生产环境的可用性，在Flink1.9之后的版本中会对其进行重构。</p><h4 id="2-1-2-API层"><a href="#2-1-2-API层" class="headerlink" title="2.1.2 API层"></a>2.1.2 API层</h4><p>API层是Flink对外提供能力的接口，实现了面向流计算的DataStream API和面向批处理的DataSet API。理论上来说，Flink的API应该像Apache Beam、Spark那样实现API层流批统一，但是目前却依然是两套系统，使用起来并不方便，所以社区也在以DataStream API为核心，推进批流API的统一。</p><h4 id="2-1-3-运行时层"><a href="#2-1-3-运行时层" class="headerlink" title="2.1.3 运行时层"></a>2.1.3 运行时层</h4><p>运行时层提供了支持Flink集群计算的核心，将开发的Flink应用分布式执行起来，包含如下内容。<br>1）DAG抽象：将分布式计算作业拆分成并行子任务，每个子任务表示数据处理的一个步骤，并且在上下游之间建立数据流的流通关系。<br>2）数据处理：包含了开发层面、运行层面的数据处理抽象，例如包含数据处理行为的封装、通用数据运算的实现（如Join、Filter、Map等）。<br>3）作业调度：调度批流作业的执行。<br>4）容错：提供了集群级、应用级容错处理机制，保障集群、作业的可靠运行。<br>5）内存管理、数据序列化：通过序列化，使用二进制方式在内存中存储数据，避免JVM的垃圾回收带来的停顿问题。<br>6）数据交换：数据在计算任务之间的本地、跨网络传递。<br>Flink运行时层并不是给一般的Flink应用开发者使用的。</p><h4 id="2-1-4-部署层"><a href="#2-1-4-部署层" class="headerlink" title="2.1.4 部署层"></a>2.1.4 部署层</h4><p>该层是Flink集群部署抽象层，Flink提供了灵活的部署模式，可以本地运行、与常见的资源管理集群集成，也支持云上的部署。<br>Flink支持多种部署模式：<br>1）Standalone模式：Flink安装在普通的Linux机器上，或者安装在K8s中，集群的资源由Flink自行管理。<br>2）Yarn、Mesos、K8s等资源管理集群模式：Flink向资源集群申请资源，创建Flink集群。<br>3）云上模式：Flink可以在Google、亚马逊云计算平台上轻松部署。</p><h3 id="2-2-Flink-集群架构"><a href="#2-2-Flink-集群架构" class="headerlink" title="2.2 Flink 集群架构"></a>2.2 Flink 集群架构</h3><p>Flink 运行时由两种类型的进程组成：一个 <em>JobManager</em> 和一个或者多个 <em>TaskManager</em>。</p><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%876.png" alt="图片2"></p><h4 id="2-2-1-组件介绍"><a href="#2-2-1-组件介绍" class="headerlink" title="2.2.1 组件介绍"></a>2.2.1 组件介绍</h4><p><strong>Flink客户端</strong><br>Flink客户端是Flink提供的CLI命令行工具，用来提交Flink作业到Flink集群，在客户端中负责Stream Graph（流图）和Job Graph（作业图）的构建。使用Table API和SQL编写的Flink应用，还会在客户端中负责SQL解析和优化。<br>Flink的Flip改进建议中提出了新的模式，SQL解析、优化，StreamGraph、JobGraph、ExecutionGraph构建转换等全部都会在JobManager中完成，这在Flink1.10后续版本中实现。</p><p><strong>JobManager</strong><br>JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：</p><blockquote><p><strong>Dispatcher：</strong>提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息；<br><strong>ResourceManager：</strong>负责 Flink 集群中的资源提供、回收、分配 - 它管理 <strong>task slots</strong>，这是 Flink 集群中资源调度的单位（请参考<a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/concepts/flink-architecture/#taskmanagers" target="_blank" rel="noopener">TaskManagers</a>）。Flink 为不同的环境和资源提供者（例如 YARN、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。</p><p><strong>JobManager：</strong>负责管理作业的执行，在一个 Flink 集群中可能有多个作业同时执行，每个作业都有自己的 JobManager 组件。</p></blockquote><p><strong>TaskManager</strong><br><em>TaskManager</em>（也称为 <em>worker</em>）执行作业流的 task，并且缓存和交换数据流。<br>必须始终至少有一个 TaskManager。在 TaskManager 中资源调度的最小单位是 task <em>slot</em>。TaskManager 中 task slot 的数量表示并发处理 task 的数量。请注意一个 task slot 中可以执行多个算子（请参考<a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/concepts/flink-architecture/#tasks-and-operator-chains" target="_blank" rel="noopener">Tasks 和算子链</a>）。</p><p>TaskManager接收JobManager分发的子任务，根据自身的资源情况，管理子任务的启动、停止、销毁、异常恢复等生命周期阶段。作业启动后开始从数据源消费数据、处理数据，并写入外部存储中。</p><h4 id="2-2-2-流程分析"><a href="#2-2-2-流程分析" class="headerlink" title="2.2.2 流程分析"></a>2.2.2 流程分析</h4><p>1）当用户提交作业的时候，提交脚本会首先启动一个 Client进程负责作业的编译与提交。它首先将用户编写的代码编译为一个 JobGraph，在这个过程，它还会进行一些检查或优化等工作，例如判断哪些 Operator 可以 Chain 到同一个 Task 中。然后，Client 将产生的 JobGraph 提交到集群中执行。此时有两种情况，一种是类似于 Standalone 这种 Session 模式，AM 会预先启动，此时 Client 直接与 Dispatcher 建立连接并提交作业即可。另一种是 Per-Job 模式，AM 不会预先启动，此时 Client 将首先向资源管理系统 （如Yarn、K8S）申请资源来启动 AM，然后再向 AM 中的 Dispatcher 提交作业。</p><p>2）当作业到 Dispatcher 后，Dispatcher 会首先启动一个 JobManager 组件，然后 JobManager 会向 ResourceManager 申请资源来启动作业中具体的任务。如果是Session模式，则TaskManager已经启动了，就可以直接分配资源。如果是per-Job模式，ResourceManager 也需要首先向外部资源管理系统申请资源来启动 TaskExecutor，然后等待 TaskExecutor 注册相应资源后再继续选择空闲资源进程分配，JobManager 收到 TaskExecutor 注册上来的 Slot 后，就可以实际提交 Task 了。</p><p>3）TaskExecutor 收到 JobManager 提交的 Task 之后，会启动一个新的线程来执行该 Task。Task 启动后就会开始进行预先指定的计算，并通过数据 Shuffle 模块互相交换数据。</p><h3 id="2-3-Flink-程序的核心概念"><a href="#2-3-Flink-程序的核心概念" class="headerlink" title="2.3 Flink 程序的核心概念"></a>2.3 Flink 程序的核心概念</h3><h4 id="2-3-1-flink-程序三个基本构建块"><a href="#2-3-1-flink-程序三个基本构建块" class="headerlink" title="2.3.1 flink 程序三个基本构建块"></a>2.3.1 flink 程序三个基本构建块</h4><ul><li><strong>source：</strong>数据源</li><li><strong>transformations：</strong>基于数据流的一组operate操作</li><li><strong>sink：</strong>数据处理结果的目的地</li></ul><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%877.png" alt="图片7"></p><h4 id="2-3-2-并行数据流"><a href="#2-3-2-并行数据流" class="headerlink" title="2.3.2 并行数据流"></a>2.3.2 并行数据流</h4><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%878.png" alt="图片8"></p><ul><li>在flink中，transformation是由一组operator组成，每一个operator被分割成operator subtask，同一个operator的多个 subtasks在不同的线程、不同的物理机或不同的容器中彼此互不依赖得并行执行。</li><li>Stream在operator有两种形式（One-to-one：类似于spark中的窄依赖，Redistributing类似于spark中的宽依赖）</li></ul><h4 id="2-3-3-operator-chains"><a href="#2-3-3-operator-chains" class="headerlink" title="2.3.3 operator chains"></a>2.3.3 operator chains</h4><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%879.png" alt="图片9"></p><p>出于分布式程序效率考虑，Flink将前后有依赖关系的一组operator的subtask链接在一起形成operator chains。operator chain在一个线程中执行，它能减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。链接的行为可以在编程API中进行指定。</p><h4 id="2-3-4-窗口"><a href="#2-3-4-窗口" class="headerlink" title="2.3.4 窗口"></a>2.3.4 窗口</h4><blockquote><p>flink可以基于窗口对在流上对数据进行聚合操作。flink支持的窗口有：</p></blockquote><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%8710.png" alt="图片10"></p><ul><li>时间窗口（tumbing windows(不重叠)，sliding windows（有重叠，session windows(有空隙的活动)）</li><li>数据窗口（tumbing windows(不重叠)，sliding windows（有重叠，session windows(有空隙的活动)）</li><li>事件窗口</li></ul><h4 id="2-3-5-时间"><a href="#2-3-5-时间" class="headerlink" title="2.3.5 时间"></a>2.3.5 时间</h4><blockquote><p>Stream中的记录时，记录中通常会包含各种典型的时间字段，Flink支持多种时间的处理：</p></blockquote><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%8711.png" alt="图片11"></p><ul><li>event Time：表示事件创建时间；</li><li>Ingestion Time：表示事件进入到Flink Dataflow的时间；</li><li>Processing Time：表示某个Operator对事件进行处理时的本地系统时间（是在TaskManager节点上）。</li></ul><h2 id="三、Flink-的运行模式"><a href="#三、Flink-的运行模式" class="headerlink" title="三、Flink 的运行模式"></a>三、Flink 的运行模式</h2><h3 id="3-1-Flink-Local-模式"><a href="#3-1-Flink-Local-模式" class="headerlink" title="3.1 Flink Local 模式"></a>3.1 Flink Local 模式</h3><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%8712.png" alt="图片12"></p><blockquote><p>1.Flink程序由JobClient进行提交；<br>2.JobClient将作业提交给JobManager；<br>3.JobManager负责协调资源分配和作业执行。资源分配完成后，任务将提交给相应的TaskManager；<br>4.TaskManager启动一个线程以开始执行。TaskManager会向JobManager报告状态更改,如开始执行，正在进行或已完成；<br>5.作业执行完成后，结果将发送回客户端(JobClient)。</p></blockquote><h3 id="3-2-Flink-Standalone-模式"><a href="#3-2-Flink-Standalone-模式" class="headerlink" title="3.2 Flink Standalone 模式"></a>3.2 Flink Standalone 模式</h3><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%8713.png" alt="图片13"></p><blockquote><p>1.client客户端提交任务给JobManager；<br>2.JobManager负责申请任务运行所需要的资源并管理任务和资源；<br>3.JobManager分发任务给TaskManager执行；<br>4.TaskManager定期向JobManager汇报状态。</p></blockquote><p>Standalone模式需要先启动Jobmanager和TaskManager进程，每一个作业都是自己的JobManager。 Client：任务提交，生成JobGraph</p><blockquote><p><strong>JobManager：</strong>调度Job，协调Task，通信，申请资源；<br><strong>TaskManager：</strong>具体任务执行，请求资源。</p></blockquote><h3 id="3-3-Flink-On-Yarn-模式"><a href="#3-3-Flink-On-Yarn-模式" class="headerlink" title="3.3 Flink On Yarn 模式"></a>3.3 Flink On Yarn 模式</h3><p>Flink如何和Yarn进行交互的</p><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%8714.png" alt="图片14"></p><blockquote><p>1.Client上传jar包和配置文件到HDFS集群上；<br>2.Client向Yarn ResourceManager提交任务并申请资源；<br>3.ResourceManager分配Container资源并启动ApplicationMaster,然后AppMaster加载Flink的Jar包和配置构建环境，启动JobManager；</p><ul><li>JobManager和ApplicationMaster运行在同一个container上；</li><li>一旦他们被成功启动，AppMaster就知道JobManager的地址(AM它自己所在的机器)；</li><li>它就会为TaskManager生成一个新的Flink配置文件(他们就可以连接到JobManager)；</li><li>这个配置文件也被上传到HDFS上；</li><li>此外，AppMaster容器也提供了Flink的web服务接口；</li><li>YARN所分配的所有端口都是临时端口，这允许用户并行执行多个Flink；</li></ul><p>4.ApplicationMaster向ResourceManager申请工作资源，NodeManager加载Flink的Jar包和配置构建环境并启动TaskManager；<br>5.TaskManager启动后向JobManager发送心跳包，并等待JobManager向其分配任务。</p></blockquote><p>长久以来，在YARN集群中部署Flink作业有两种模式，即Session Mode和Per-Job Mode，而在Flink 1.11版本中，又引入了第三种全新的模式：Application Mode。</p><h4 id="3-3-1-Session-模式"><a href="#3-3-1-Session-模式" class="headerlink" title="3.3.1 Session 模式"></a>3.3.1 Session 模式</h4><p>Session模式是预分配资源的，也就是提前根据指定的资源参数初始化一个Flink集群，并常驻在YARN系统中，拥有固定数量的JobManager和TaskManager（注意JobManager只有一个）。提交到这个集群的作业可以直接运行，免去每次分配资源的overhead。但是Session的资源总量有限，多个作业之间又不是隔离的，故可能会造成资源的争用；如果有一个TaskManager宕机，它上面承载着的所有作业也都会失败。另外，启动的作业越多，JobManager的负载也就越大。所以，<strong>Session模式一般用来部署那些对延迟非常敏感但运行时长较短的作业</strong>。</p><p>Session 模式特点：<br>1）共享Dispatcher与ResourceManager；<br>2）共享资源；<br>3）适合小规模，执行时间较短的作业。</p><h4 id="3-3-2-Per-Job-模式"><a href="#3-3-2-Per-Job-模式" class="headerlink" title="3.3.2 Per-Job 模式"></a>3.3.2 Per-Job 模式</h4><p>顾名思义，在Per-Job模式下，每个提交到YARN上的作业会各自形成单独的Flink集群，拥有专属的JobManager和TaskManager。可见，以Per-Job模式提交作业的启动延迟可能会较高，但是作业之间的资源完全隔离，一个作业的TaskManager失败不会影响其他作业的运行，JobManager的负载也是分散开来的，不存在单点问题。当作业运行完成，与它关联的集群也就被销毁，资源被释放。所以，<strong>Per-Job模式一般用来部署那些长时间运行的作业</strong>。</p><p>Per-Job 模式特点：<br>1）独享Dispatcher与ResourceManager ；<br>2）按需申请资源(TaskExecutor) ；<br>3）适合执行时间较长的大作业 。</p><p><strong>存在的问题</strong></p><p>上述<strong>Session</strong>模式和<strong>Per-Job</strong>模式可以用如下的简图表示，其中红色、蓝色和绿色的图形代表不同的作业。</p><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%8715.png" alt="图片15"></p><p>Deployer代表向YARN集群发起部署请求的节点，一般来讲在生产环境中，也总有这样一个节点作为所有作业的提交入口（即客户端）。在main()方法开始执行直到env.execute()方法之前，客户端也需要做一些工作，即：</p><p>1）获取作业所需的依赖项；<br>2）通过执行环境分析并取得逻辑计划，即StreamGraph→JobGraph；<br>3）将依赖项和JobGraph上传到集群中。<br>只有在这些都完成之后，才会通过env.execute()方法触发Flink运行时真正地开始执行作业。试想，如果所有用户都在Deployer上提交作业，较大的依赖会消耗更多的带宽，而较复杂的作业逻辑翻译成JobGraph也需要吃掉更多的CPU和内存，客户端的资源反而会成为瓶颈——不管Session还是Per-Job模式都存在此问题。为了解决它，社区在传统部署模式的基础上实现了Application模式。</p><h4 id="3-3-3-Application-模式"><a href="#3-3-3-Application-模式" class="headerlink" title="3.3.3 Application 模式"></a>3.3.3 Application 模式</h4><p><img src="/2021/11/12/hadoop-ji-suan-yin-qing-flink/%E5%9B%BE%E7%89%8716.png" alt="图片16"></p><p>可见，原本需要客户端做的三件事被转移到了JobManager里，也就是说main()方法在集群中执行（入口点位于ApplicationClusterEntryPoint），Deployer只需要负责发起部署请求了。另外，如果一个main()方法中有多个env.execute()/executeAsync()调用，在Application模式下，这些作业会被视为属于同一个应用，在同一个集群中执行（如果在Per-Job模式下，就会启动多个集群）。可见，<strong>Application模式本质上是Session和Per-Job模式的折衷</strong>。</p><p>用Application模式提交作业的示例命令如下。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run-application -t yarn-application \</span><br><span class="line">-Djobmanager.memory.process.size=<span class="number">2048</span>m \</span><br><span class="line">-Dtaskmanager.memory.process.size=<span class="number">4096</span>m \</span><br><span class="line">-Dtaskmanager.numberOfTaskSlots=<span class="number">2</span> \</span><br><span class="line">-Dparallelism.<span class="keyword">default</span>=<span class="number">10</span> \</span><br><span class="line">-Dyarn.application.name=<span class="string">"MyFlinkApp"</span> \</span><br><span class="line">/path/to/my/flink-app/MyFlinkApp.jar</span><br></pre></td></tr></table></figure><p><code>-t</code>参数用来指定部署目标，目前支持YARN（<code>yarn-application</code>）和K8S（<code>kubernetes-application</code>）。<code>-D</code>参数则用来指定与作业相关的各项参数，具体可参见<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fci.apache.org%2Fprojects%2Fflink%2Fflink-docs-stable%2Fops%2Fconfig.html" target="_blank" rel="noopener">官方文档</a>。</p><p>那么如何解决传输依赖项造成的带宽占用问题呢？Flink作业必须的依赖是发行包flink-dist.jar，还有扩展库（位于$FLINK_HOME/lib）和插件库（位于$FLINK_HOME/plugin），我们将它们预先上传到像HDFS这样的共享存储，再通过<code>yarn.provided.lib.dirs</code>参数指定存储的路径即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Dyarn.provided.lib.dirs=<span class="string">"hdfs://myhdfs/flink-common-deps/lib;hdfs://myhdfs/flink-common-deps/plugins"</span></span><br></pre></td></tr></table></figure><p>这样所有作业就不必各自上传依赖，可以直接从HDFS拉取，并且YARN NodeManager也会缓存这些依赖，进一步加快作业的提交过程。同理，包含Flink作业的用户JAR包也可以上传到HDFS，并指定远程路径进行提交。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:01 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a href=&quot;https://flink.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apach
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
      <category term="Hadoop Flink" scheme="http://chenzhonzhou.github.io/tags/Hadoop-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Spark 常用的几种模式部署</title>
    <link href="http://chenzhonzhou.github.io/2021/11/11/spark-chang-yong-de-ji-chong-mo-shi-bu-shu/"/>
    <id>http://chenzhonzhou.github.io/2021/11/11/spark-chang-yong-de-ji-chong-mo-shi-bu-shu/</id>
    <published>2021-11-11T02:44:18.000Z</published>
    <updated>2021-11-11T07:40:24.084Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --><p>Spark 支持本地运行模式（Local 模式）、独立运行模式（Standalone 模式）、Mesos、YARN（Yet Another Resource Negotiator）、Kubernetes 模式等。</p><h2 id="一、Spark-Local-模式"><a href="#一、Spark-Local-模式" class="headerlink" title="一、Spark Local 模式"></a>一、Spark Local 模式</h2><h3 id="1-1-下载解压-Spark"><a href="#1-1-下载解压-Spark" class="headerlink" title="1.1 下载解压 Spark"></a>1.1 下载解压 Spark</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz</span><br><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/spark-3.1.2-bin-hadoop2.7.tgz</span><br><span class="line"></span><br><span class="line"><span class="comment">#配置环境变量</span></span><br><span class="line">[hadoop@hadoop1 ~]$ vim .bash_profile</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/home/hadoop/spark-3.1.2-bin-hadoop2.7</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$PATH</span>:<span class="variable">$STORM_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~]$ <span class="built_in">source</span> .bash_profile</span><br></pre></td></tr></table></figure><p>解压目录说明</p><blockquote><p><strong>bin：</strong>可执行脚本<br><strong>conf：</strong>配置文件<br><strong>data：</strong>示例程序使用数据<br><strong>examples：</strong>示例程序<br><strong>jars：</strong>依赖 jar 包<br><strong>python：</strong>pythonAPI<br><strong>R：</strong>R 语言 API<br><strong>sbin：</strong>集群管理命令<br><strong>yarn：</strong>整合yarn需要的东西</p></blockquote><h3 id="1-2-启动-spark-shell"><a href="#1-2-启动-spark-shell" class="headerlink" title="1.2 启动 spark-shell"></a>1.2 启动 spark-shell</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ./spark-3.1.2-bin-hadoop2.7/bin/spark-shell --master <span class="built_in">local</span>[1]</span><br><span class="line">21/11/09 10:39:08 WARN NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Using Spark<span class="string">'s default log4j profile: org/apache/spark/log4j-defaults.properties</span></span><br><span class="line"><span class="string">Setting default log level to "WARN".</span></span><br><span class="line"><span class="string">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span></span><br><span class="line"><span class="string">Spark context Web UI available at http://hadoop1:4040</span></span><br><span class="line"><span class="string">Spark context available as '</span>sc<span class="string">' (master = local[1], app id = local-1636425555582).</span></span><br><span class="line"><span class="string">Spark session available as '</span>spark<span class="string">'.</span></span><br><span class="line"><span class="string">Welcome to</span></span><br><span class="line"><span class="string">      ____              __</span></span><br><span class="line"><span class="string">     / __/__  ___ _____/ /__</span></span><br><span class="line"><span class="string">    _\ \/ _ \/ _ `/ __/  '</span>_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 3.1.2</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_301)</span><br><span class="line">Type <span class="keyword">in</span> expressions to have them evaluated.</span><br><span class="line">Type :<span class="built_in">help</span> <span class="keyword">for</span> more information.</span><br></pre></td></tr></table></figure><blockquote><p><strong>spark-shell –master local[N]</strong> 表示在本地模拟N个线程来运行当前任务<br><strong>spark-shell –master local[*]</strong> 表示使用当前机器上所有可用的资源</p></blockquote><h3 id="1-3-计算圆周率案例"><a href="#1-3-计算圆周率案例" class="headerlink" title="1.3 计算圆周率案例"></a>1.3 计算圆周率案例</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ./spark-3.1.2-bin-hadoop2.7/bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">spark-3.1.2-bin-hadoop2.7/examples/jars/spark-examples_2.12-3.1.2.jar \</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line">…………</span><br><span class="line">21/11/09 11:01:43 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 30 ms on hadoop1 (executor driver) (100/100)</span><br><span class="line">21/11/09 11:01:43 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 2.820 s</span><br><span class="line">21/11/09 11:01:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job</span><br><span class="line">21/11/09 11:01:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool</span><br><span class="line">21/11/09 11:01:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished</span><br><span class="line">21/11/09 11:01:43 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 2.956005 s</span><br><span class="line">Pi is roughly 3.1415763141576316</span><br><span class="line">21/11/09 11:01:43 INFO SparkUI: Stopped Spark web UI at http://hadoop1:4041</span><br><span class="line">21/11/09 11:01:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">21/11/09 11:01:44 INFO MemoryStore: MemoryStore cleared</span><br><span class="line">21/11/09 11:01:44 INFO BlockManager: BlockManager stopped</span><br><span class="line">21/11/09 11:01:44 INFO BlockManagerMaster: BlockManagerMaster stopped</span><br><span class="line">21/11/09 11:01:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class="line">21/11/09 11:01:44 INFO SparkContext: Successfully stopped SparkContext</span><br><span class="line">21/11/09 11:01:44 INFO ShutdownHookManager: Shutdown hook called</span><br><span class="line">21/11/09 11:01:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-19b0ca6d-7ff5-4a75-b8d3-df59185049fc</span><br><span class="line">21/11/09 11:01:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-6400e584-4c21-427a-a445-644793b60378</span><br></pre></td></tr></table></figure><h2 id="二、Spark-Standalone-模式"><a href="#二、Spark-Standalone-模式" class="headerlink" title="二、Spark Standalone 模式"></a>二、Spark Standalone 模式</h2><p><strong>服务规划</strong></p><table><thead><tr><th>服务器</th><th>IP地址</th><th>spark角色</th><th>其它依赖</th></tr></thead><tbody><tr><td>hadoop1</td><td>10.10.8.11</td><td>Master、Slave</td><td>JDK1.8</td></tr><tr><td>hadoop2</td><td>10.10.8.12</td><td>Slave</td><td>JDK1.8</td></tr><tr><td>hadoop3</td><td>10.10.8.13</td><td>Slave</td><td>JDK1.8</td></tr></tbody></table><h3 id="2-1-准备工作"><a href="#2-1-准备工作" class="headerlink" title="2.1 准备工作"></a>2.1 准备工作</h3><p>服务器免密登录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[dev@hadoop1 ~]# su - hadoop</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-keygen</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-copy-id hadoop1</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-copy-id hadoop2</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-copy-id hadoop3</span><br></pre></td></tr></table></figure><p>配置jdk 1.8环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/jdk-8u301-linux-x64.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ vim .bash_profile</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/home/hadoop/jdk1.8.0_301</span><br><span class="line"><span class="built_in">export</span> JRE_HOME=<span class="variable">$&#123;JAVA_HOME&#125;</span>/jre</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$&#123;JAVA_HOME&#125;</span>/lib:<span class="variable">$&#123;JRE_HOME&#125;</span>/lib</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~]$ <span class="built_in">source</span> .bash_profile</span><br></pre></td></tr></table></figure><h3 id="2-2-修改-spark-配置"><a href="#2-2-修改-spark-配置" class="headerlink" title="2.2 修改 spark 配置"></a>2.2 修改 spark 配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下载spark</span></span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ wget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz</span><br><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/spark-3.1.2-bin-hadoop2.7.tgz</span><br><span class="line">[hadoop@hadoop1 ~]$ <span class="built_in">cd</span> spark-3.1.2-bin-hadoop2.7/conf/</span><br></pre></td></tr></table></figure><p>修改 spark-env.sh 配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7/conf]$ cp spark-env.sh.template spark-env.sh</span><br><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7/conf]$ vim spark-env.sh</span><br><span class="line"><span class="comment"># java环境变量</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/home/hadoop/jdk1.8.0_301</span><br><span class="line"><span class="comment"># spark home</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/home/hadoop/spark-3.1.2-bin-hadoop2.7</span><br><span class="line"><span class="comment"># spark集群master进程主机host</span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=hadoop1</span><br><span class="line"><span class="comment"># spark运行产生临时数据目录</span></span><br><span class="line"><span class="built_in">export</span> SPARK_LOCAL_DIRS=<span class="variable">$SPARK_HOME</span>/datas</span><br></pre></td></tr></table></figure><p>修改 slaves 配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7/conf]$ vim slaves</span><br><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure><h3 id="2-3-分发-spark-配置"><a href="#2-3-分发-spark-配置" class="headerlink" title="2.3 分发 spark 配置"></a>2.3 分发 spark 配置</h3><p>将spark配置拷贝到其它服务器上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ rsync -av spark-3.1.2-bin-hadoop2.7 hadoop@hadoop2:~/</span><br><span class="line">[hadoop@hadoop1 ~]$ rsync -av spark-3.1.2-bin-hadoop2.7 hadoop@hadoop3:~/</span><br></pre></td></tr></table></figure><h3 id="2-4-启动-spark-集群"><a href="#2-4-启动-spark-集群" class="headerlink" title="2.4 启动 spark 集群"></a>2.4 启动 spark 集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">启动master</span></span><br><span class="line">[hadoop@hadoop1 ~]$ ./spark-3.1.2-bin-hadoop2.7/sbin/start-slave.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">启动slaves</span></span><br><span class="line">[hadoop@hadoop1 ~]$ ./spark-3.1.2-bin-hadoop2.7/sbin/start-slaves.sh</span><br></pre></td></tr></table></figure><h3 id="2-5-计算圆周率案例"><a href="#2-5-计算圆周率案例" class="headerlink" title="2.5 计算圆周率案例"></a>2.5 计算圆周率案例</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Standalone cluster模式运行</span></span><br><span class="line">[hadoop@hadoop3 ~/spark-3.1.2-bin-hadoop2.7]$ ./bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop1:7077 \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">examples/jars/spark-examples_2.12-3.1.2.jar \</span><br><span class="line">1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Standalone client模式运行</span></span><br><span class="line">[hadoop@hadoop3 ~/spark-3.1.2-bin-hadoop2.7]$ .bin/spark-submit  \</span><br><span class="line">--master spark://hadoop1:7077 \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">examples/jars/spark-examples_2.12-3.1.2.jar \</span><br><span class="line">1000</span><br></pre></td></tr></table></figure><h2 id="三、Spark-Standalone-高可用"><a href="#三、Spark-Standalone-高可用" class="headerlink" title="三、Spark Standalone 高可用"></a>三、Spark Standalone 高可用</h2><p><strong>服务规划</strong></p><table><thead><tr><th>服务器</th><th>IP地址</th><th>spark角色</th><th>其它依赖</th></tr></thead><tbody><tr><td>hadoop1</td><td>10.10.8.11</td><td>Master、Slave</td><td>JDK1.8、zookeeper</td></tr><tr><td>hadoop2</td><td>10.10.8.12</td><td>Master、Slave</td><td>JDK1.8、zookeeper</td></tr><tr><td>hadoop3</td><td>10.10.8.13</td><td>Slave</td><td>JDK1.8、zookeeper</td></tr></tbody></table><h3 id="3-1-修改-spark-配置"><a href="#3-1-修改-spark-配置" class="headerlink" title="3.1 修改 spark 配置"></a>3.1 修改 spark 配置</h3><p>在<strong><code>Spark Standalone 模式</code></strong>的基础上修改spark-env.sh配置文件，增加zk配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vim spark-3.1.2-bin-hadoop2.7/conf/spark-env.sh</span><br><span class="line"><span class="comment"># java环境变量</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/home/hadoop/jdk1.8.0_301</span><br><span class="line"><span class="comment"># spark home</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/home/hadoop/spark-3.1.2-bin-hadoop2.7</span><br><span class="line"><span class="comment"># spark集群master进程主机host</span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=hadoop1</span><br><span class="line"><span class="comment"># spark运行产生临时数据目录</span></span><br><span class="line"><span class="built_in">export</span> SPARK_LOCAL_DIRS=<span class="variable">$SPARK_HOME</span>/datas</span><br><span class="line"><span class="comment"># 配置zk 此处能够独立配置zk list,逗号分隔</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">"</span></span><br><span class="line"><span class="string">-Dspark.deploy.recoveryMode=ZOOKEEPER</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.url=hadoop1,hadoop2,hadoop3</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.dir=/spark"</span></span><br></pre></td></tr></table></figure><h3 id="3-2-分发-spark-配置"><a href="#3-2-分发-spark-配置" class="headerlink" title="3.2 分发 spark 配置"></a>3.2 分发 spark 配置</h3><p>将spark配置拷贝到其它服务器上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ rsync -av spark-3.1.2-bin-hadoop2.7 hadoop@hadoop2:~/</span><br><span class="line">[hadoop@hadoop1 ~]$ rsync -av spark-3.1.2-bin-hadoop2.7 hadoop@hadoop3:~/</span><br></pre></td></tr></table></figure><h3 id="3-3-修改-master2-配置"><a href="#3-3-修改-master2-配置" class="headerlink" title="3.3 修改 master2 配置"></a>3.3 修改 master2 配置</h3><p>修改master2机器上的spark-env.sh配置文件 <code>SPARK_MASTER_HOST</code> 参数，将master地址指向master2服务器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~]$ vim spark-3.1.2-bin-hadoop2.7/conf/spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=hadoop2</span><br></pre></td></tr></table></figure><h3 id="3-4-启动-spark-集群"><a href="#3-4-启动-spark-集群" class="headerlink" title="3.4 启动 spark 集群"></a>3.4 启动 spark 集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">启动master1</span></span><br><span class="line">[hadoop@hadoop1 ~]$ ./spark-3.1.2-bin-hadoop2.7/sbin/start-master.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">启动master2</span></span><br><span class="line">[hadoop@hadoop2 ~]$ ./spark-3.1.2-bin-hadoop2.7/sbin/start-master.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">启动slaves</span></span><br><span class="line">[hadoop@hadoop1 ~]$ ./spark-3.1.2-bin-hadoop2.7/sbin/start-slaves.sh</span><br></pre></td></tr></table></figure><blockquote><p>如果和其它服务端口冲突可以修改 <strong>start-master.sh</strong> 中的 <strong><code>SPARK_MASTER_WEBUI_PORT=8080</code></strong> 更改为其它端口</p></blockquote><p>查看 spark web页面</p><p><img src="/2021/11/11/spark-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%871.png" alt="图片1"></p><p><img src="/2021/11/11/spark-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%872.png" alt="图片2"></p><h3 id="3-4-验证-高可用"><a href="#3-4-验证-高可用" class="headerlink" title="3.4 验证 高可用"></a>3.4 验证 高可用</h3><p>停止 master 节点服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ./spark-3.1.2-bin-hadoop2.7/sbin/stop-master.sh</span><br></pre></td></tr></table></figure><p>访问master1节点</p><p><img src="/2021/11/11/spark-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%873.png" alt="图片3"></p><p>访问master2节点</p><p><img src="/2021/11/11/spark-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%874.png" alt="图片4"></p><h2 id="四、Spark-on-YARN模式"><a href="#四、Spark-on-YARN模式" class="headerlink" title="四、Spark on YARN模式"></a>四、Spark on YARN模式</h2><p><strong>服务规划</strong></p><table><thead><tr><th>主机名</th><th>IP地址</th><th>spark角色</th><th>依赖服务</th></tr></thead><tbody><tr><td>hadoop1</td><td>10.10.8.11</td><td>Master、Slave</td><td>JDK1.8、YARN</td></tr><tr><td>hadoop2</td><td>10.10.8.12</td><td>Slave</td><td>JDK1.8、YARN</td></tr><tr><td>hadoop3</td><td>10.10.8.13</td><td>Slave</td><td>JDK1.8、YARN</td></tr></tbody></table><p><strong>前置条件</strong></p><blockquote><p>hadoop yarn集群是可用的</p></blockquote><h3 id="4-1-准备工作"><a href="#4-1-准备工作" class="headerlink" title="4.1 准备工作"></a>4.1 准备工作</h3><p>服务器免密登录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[dev@hadoop1 ~]# su - hadoop</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-keygen</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-copy-id hadoop1</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-copy-id hadoop2</span><br><span class="line">[hadoop@hadoop1 ~]$ ssh-copy-id hadoop3</span><br></pre></td></tr></table></figure><p>配置jdk 1.8环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar xf downloads/jdk-8u301-linux-x64.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~]$ vim .bash_profile</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/home/hadoop/jdk1.8.0_301</span><br><span class="line"><span class="built_in">export</span> JRE_HOME=<span class="variable">$&#123;JAVA_HOME&#125;</span>/jre</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$&#123;JAVA_HOME&#125;</span>/lib:<span class="variable">$&#123;JRE_HOME&#125;</span>/lib</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~]$ <span class="built_in">source</span> .bash_profile</span><br></pre></td></tr></table></figure><h3 id="4-2-修改-spark-配置"><a href="#4-2-修改-spark-配置" class="headerlink" title="4.2 修改 spark 配置"></a>4.2 修改 spark 配置</h3><p>修改master2机器上的spark-env.sh配置，增加hadoop YARN配置文件目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7/conf]$ cp spark-env.sh.template spark-env.sh</span><br><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7/conf]$ vim spark-env.sh</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/home/hadoop/jdk1.8.0_301</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/home/hadoop/spark-3.1.2-bin-hadoop2.7</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=hadoop1</span><br><span class="line"><span class="built_in">export</span> SPARK_LOCAL_DIRS=<span class="variable">$SPARK_HOME</span>/datas</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/hadoop/hadoop-2.7.2</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br></pre></td></tr></table></figure><p>修改 slaves 配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7/conf]$ vim slaves</span><br><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure><h3 id="4-3-分发-spark-配置"><a href="#4-3-分发-spark-配置" class="headerlink" title="4.3 分发 spark 配置"></a>4.3 分发 spark 配置</h3><p>将spark配置拷贝到其它服务器上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ rsync -av spark-3.1.2-bin-hadoop2.7 hadoop@hadoop2:~/</span><br><span class="line">[hadoop@hadoop1 ~]$ rsync -av spark-3.1.2-bin-hadoop2.7 hadoop@hadoop3:~/</span><br></pre></td></tr></table></figure><h3 id="4-4-启动-spark-集群"><a href="#4-4-启动-spark-集群" class="headerlink" title="4.4 启动 spark 集群"></a>4.4 启动 spark 集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">启动master</span></span><br><span class="line">[hadoop@hadoop1 ~]$ ./spark-3.1.2-bin-hadoop2.7/sbin/start-master.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">启动slaves</span></span><br><span class="line">[hadoop@hadoop1 ~]$ ./spark-3.1.2-bin-hadoop2.7/sbin/start-slaves.sh</span><br></pre></td></tr></table></figure><blockquote><p>如果和其它服务端口冲突可以修改 <strong>start-master.sh</strong> 中的 <strong><code>SPARK_MASTER_WEBUI_PORT=8080</code></strong> 更改为其它端口</p></blockquote><h3 id="4-5-运行示例"><a href="#4-5-运行示例" class="headerlink" title="4.5 运行示例"></a>4.5 运行示例</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cluster模式运行</span></span><br><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7]$ ./bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">examples/jars/spark-examples_2.12-3.1.2.jar \</span><br><span class="line">10</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> client模式运行</span></span><br><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7]$ ./bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">examples/jars/spark-examples_2.12-3.1.2.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure><blockquote><p>client模式如果运行报错：<code>ERROR spark.SparkContext: Error initializing SparkContext.</code> 修改 <code>spark-env.sh</code> 添加 <code>export SPARK_LOCAL_IP=&lt;IP_Address&gt;</code> 配置</p></blockquote><h3 id="4-6-在-YRAN-上查看任务"><a href="#4-6-在-YRAN-上查看任务" class="headerlink" title="4.6 在 YRAN 上查看任务"></a>4.6 在 YRAN 上查看任务</h3><p><img src="/2021/11/11/spark-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%875.png" alt="图片5"></p><h2 id="五、Spark-on-Mesos-模式"><a href="#五、Spark-on-Mesos-模式" class="headerlink" title="五、Spark on Mesos 模式"></a>五、Spark on Mesos 模式</h2><p><strong>服务规划</strong></p><table><thead><tr><th>主机名</th><th>IP地址</th><th>mesos角色</th></tr></thead><tbody><tr><td>hadoop1</td><td>10.10.8.11</td><td>Master、Agent</td></tr></tbody></table><p><strong>前置条件</strong></p><blockquote><p>spark 是可用的</p></blockquote><h3 id="5-1-部署mesos"><a href="#5-1-部署mesos" class="headerlink" title="5.1 部署mesos"></a>5.1 部署mesos</h3><p>编译安装 mesos</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/downloads]$ wget http://archive.apache.org/dist/mesos/1.9.0/mesos-1.9.0.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ tar xf mesos-1.9.0.tar.gz</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ sudo wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ subversion install below.</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ sudo yum install -y epel-release</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ sudo bash -c 'cat &gt; /etc/yum.repos.d/wandisco-svn.repo &lt;&lt;EOF</span><br><span class="line">[WANdiscoSVN]</span><br><span class="line">name=WANdisco SVN Repo 1.9</span><br><span class="line">enabled=1</span><br><span class="line">baseurl=http://opensource.wandisco.com/centos/7/svn-1.9/RPMS/\$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco</span><br><span class="line">EOF'</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ sudo yum groupinstall -y "Development Tools"</span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ sudo yum install -y apache-maven python-devel python-six python-virtualenv zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~/downloads]$ cd mesos-1.9.0</span><br><span class="line">[hadoop@hadoop1 ~/downloads/mesos-1.9.0]$ ./bootstrap</span><br><span class="line">[hadoop@hadoop1 ~/downloads/mesos-1.9.0]$ mkdir build</span><br><span class="line">[hadoop@hadoop1 ~/downloads/mesos-1.9.0]$ cd build</span><br><span class="line">[hadoop@hadoop1 ~/downloads/mesos-1.9.0/build]$ ../configure</span><br><span class="line">[hadoop@hadoop1 ~/downloads/mesos-1.9.0/build]$ make</span><br><span class="line">[hadoop@hadoop1 ~/downloads/mesos-1.9.0/build]$ make check</span><br><span class="line">[hadoop@hadoop1 ~/downloads/mesos-1.9.0/build]$ sudo make install</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~]$ ln -s downloads/mesos-1.9.0/build/bin mesos</span><br><span class="line">[hadoop@hadoop1 ~]$ cd mesos</span><br></pre></td></tr></table></figure><p>启动 mesos</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动 mesos master</span></span><br><span class="line">[hadoop@hadoop1 ~/mesos]$ ./bin/mesos-master.sh --ip=10.10.8.11 --work_dir=/home/hadoop/mesos/mesos_master</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 mesos agent</span></span><br><span class="line">[hadoop@hadoop1 ~/mesos]$ ./bin/mesos-agent.sh --master=hadoop1:5050 --work_dir=/home/hadoop/mesos/mesos_agent</span><br></pre></td></tr></table></figure><p>访问 mesos WEB页面</p><p><img src="/2021/11/11/spark-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%876.png" alt="图片6"></p><h3 id="5-2-spark-配置"><a href="#5-2-spark-配置" class="headerlink" title="5.2 spark 配置"></a>5.2 spark 配置</h3><p>将当前使用的spark二进制包上传到hdfs</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hdfs dfs -put ~/downloads/spark-3.1.2-bin-hadoop2.7.tgz spark-3.1.2-bin-hadoop2.7.tgz</span><br></pre></td></tr></table></figure><p>修改 spark-env.sh 配置文件，增加mesos相关配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7]$ vim conf/spark-env.sh</span><br><span class="line">export MESOS_NATIVE_JAVA_LIBRARY=/usr/local/lib/libmesos.so</span><br><span class="line">export SPARK_EXECUTOR_URI=hdfs://hadoop2:8020/user/hadoop/spark-3.1.2-bin-hadoop2.7.tgz</span><br></pre></td></tr></table></figure><h3 id="5-3-运行示例"><a href="#5-3-运行示例" class="headerlink" title="5.3 运行示例"></a>5.3 运行示例</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7]$ ./bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master mesos://hadoop1:5050 \</span><br><span class="line">examples/jars/spark-examples_2.12-3.1.2.jar \</span><br><span class="line">1000</span><br></pre></td></tr></table></figure><p>或使用spark-shell</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 上传示例文件到hdfs</span></span><br><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7]$ hdfs dfs -put README.md</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 统计文件行数</span></span><br><span class="line">[hadoop@hadoop1 ~/spark-3.1.2-bin-hadoop2.7]$ ./bin/spark-shell --master mesos://hadoop1:5050</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val lines  = sc.textFile(<span class="string">"README.md"</span>)</span></span><br><span class="line">lines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> lines.count()</span></span><br><span class="line">res0: Long = 108</span><br></pre></td></tr></table></figure><p>同时前往mesos WEB查看进行中的任务</p><p><img src="/2021/11/11/spark-chang-yong-de-ji-chong-mo-shi-bu-shu/%E5%9B%BE%E7%89%877.png" alt="图片7"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Mon Aug 22 2022 10:59:02 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Spark 支持本地运行模式（Local 模式）、独立运行模式（Standalone 模式）、Mesos、YARN（Yet Another Re
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://chenzhonzhou.github.io/tags/Hadoop/"/>
    
      <category term="Hadoop Spark" scheme="http://chenzhonzhou.github.io/tags/Hadoop-Spark/"/>
    
  </entry>
  
</feed>
